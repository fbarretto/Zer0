
RESUMO
      Esta pesquisa teórico-prática de doutorado adota uma visão transdisciplinar,
aprofundando a pesquisa realizada no âmbito do mestrado (BARRETTO, 2011) e
propõe um novo conceito descrito como Artelligent. A palavra Artelligent tem a sua
origem na junção das palavras latinas ars/artis e intelligere. A palavra ars,
correspondente ao termo grego tékne, descreve basicamente uma habilidade prática
para produzir artefatos e ao mesmo denota um conhecimento, ciência, metodologia
relacionado à alguma teoria arte-científica. Para este caso, consideramos que trata-se
de um conceito envolvendo a expressão ou habilidade imaginativa representadas em
suas mais amplas vertentes como pintura, musica e literatura e que tem associado
algum potencial emocional ou estético. Já a palavra intelligere, também oriunda do
latim, está na raiz da palavra inteligente e é uma composição da palavras inter, isto é,
entre, e legere, que significa escolher, coletar, colher, reunir. Juntas, inter e legere
significam perceber, reconhecer, compreender, entender e realizar.
      Um sistema Artelligent pode ser definido como um sistema autopoiético que
através da utilização de técnicas especificas de inteligência artificial, representa o
conhecimento de forma extensível e, considerando os princípios que regem o
processo criativo humano,  é capaz de exibir resultados de caráter reconhecidamente
emergentes em um determinado ambiente. É apresentada nesta pesquisa uma
estrutura, a fim de definir claramente quais as técnicas, conceitos e princípios devem
ser aplicados a fim de criar tais sistemas Artelligentes, sobretudo considerando os
conceitos de autopoiese, cunhado por Maturana e Varela (1970) e emergência como
uma heurística para criatividade. A fim de explorar esses conceitos, apresentamos
Zer0, uma jogo que convida o interagente à realizar uma deriva em um universo
governado por formas geométricas. Através de interações com outras formas, o
jogador é capaz de evoluir a partir de uma única forma de linha para formas mais
complexas. Zer0 é um sistema multiagentes capaz de compor música emergente em
tempo real.
Palavras-chave: Artelligent, Arte Computacional, Inteligência Artificial,
Criatividade, Emergência, Autopoiese.


INTRODUÇÃO
      Esta pesquisa recorre à metodologia inter e transdisciplinar pois visa o
trabalho construtivo, que se desenvolve a partir de várias áreas de conhecimento como
principalmente Arte, Ciências da Computação, Biologia e Psicologia. A
transdisciplinaridade neste projeto potencializa a ideia de caminhar, de ultrapassar as
fronteiras das disciplinas e de ousar transitar por elas. A transdisciplinaridade, como
método, vai ao encontro da proposta de Basarab Nicolescu (NICOLESCU, 2006), no
movimento que se estabelece "entre", "através" e para "além" das disciplinas cuja
dinâmica consolida-se na "coerência", na "legitimidade" e na "articulação" de saberes
que desdobram-se de seu difícil exercício e complexidade. O método de trabalho em
laboratório, neste caso os espaços do Midialab  e do LATE! , para construir software
artístico tenta ultrapassar o pensamento clássico a fim de abarcar os vários níveis de
realidade tomando por base a lógica da complexidade, que envolve o trabalho coletivo
e colaborativo, como sustentáculo da sua metodologia de pesquisa. Assim, o trabalho
resultante deste esforço encontra-se em um lugar que ultrapassa o limite das
disciplinas que embasam a pesquisa no sentido de que cria um novo conceito que
transborda as disciplinas originais não podendo nelas limitar-se.
      Sob o ponto de vista artístico, a comunidade há algum tempo discorre sobre os
panoramas de utilização de Inteligências Artificiais (IA), ou agentes , poeticamente
representados no cinema pelo computador Hal, em 2001: Uma Odisseia no Espaço,
os replicantes de Blade Runner, baseado no romance de Philip K. Dick Do Androids
Dream of Electronic Sheeps, ou pelo próprio Wintermute criado por William Gibson
no romance intitulado Neuromancer (KUBRICK e CLARCKE, 1968; DICK, 1982;
GIBSON, 2003). No entanto, há uma enorme diferença entre os agentes imaginários
retratados nestas obras e os agentes inteligentes com os quais convivemos no mundo
“real”, como aqueles que agendam passagens ou triam os nossos e-mails.
      O conceito computacional geral de agente remonta aos anos 50 quando John
McCarthy, considerado por muitos como um dos pais da IA ao lado de Marvin Misky
e vários outros, desenvolveu o software Advice Talker. No entanto, o termo “agente”
só caiu em uso nas discussões high tech por volta de 1989, quando a Apple lançou um
vídeo intitulado The Knowledge Navigator onde exibia um mordomo trajando um
belo smoking que obedecia prontamente aos comandos de um interagente frente ao
computador (JOHNSON, 1997).
      Nas primeiras discussões científicas acerca da definição conceitual de IA
abordou-se, entre outros, o ponto de vista da busca pela compreensão do
funcionamento da inteligência humana que, por sua vez, está ligada diretamente à
capacidade que temos de compreender o mundo através das nossas habilidades
cognitivas. Dentre estas habilidades, pode-se destacar a relevância da concepção
artística no contexto da expressão desta inteligência através do uso do nosso sistema
conotativo (LUGER, 2004).
      A representação do conhecimento acerca das relações que dominam a
expressão criativa do artista fica mais evidente ao se trabalhar com música, posto que
ela possui gramática e regras próprias capazes de serem descritas em forma de
algoritmo (ROADS, 1996). A composição algorítmica tem sido desenvolvida há
décadas como forma de prover uma composição assistida pelo computador ou,
inclusive, a realização de uma obra composta apenas pelo computador.
      Sendo a inteligência artificial uma espécie de automação de comportamento
inteligente, segundo a definição de Luger e Stubblefield (LUGER, 2004), e a Arte
uma expressão de comportamento inteligente inerente ao nosso sistema cognitivo e
conotativo (FERNANDES, 2008), pode-se estudar a relação do comportamento de
expressão criativa através da aplicação de técnicas de IA sob dois paradigmas
composicionais: composição assistida pelo computador e composição pelo
computador (ROADS, 1996; POHLMANN, 2002).
      No primeiro paradigma, a composição artística é realizada por um usuário
através do auxílio do computador. Neste caso, o computador pode facilmente auxiliar
usuários inexperientes no processo composicional. A segunda abordagem diz respeito
à composição totalmente realizada por um computador, sendo bem mais promissor
para o estudo do ponto de vista da inteligência artificial (ROADS, 1996).
      Os pesquisadores da área de IA buscam, através do desenvolvimento de
técnicas e modelos específicos, soluções para alguns problemas da área: aprendizado
de máquina, sistemas artificiais, visão computacional, criatividade computacional, etc.
Alguns dos modelos apresentados neste trabalho, como as redes neurais artificiais e os
algoritmos genéticos, possibilitam ao artista diversas aplicações que permeiam os
conceitos de emergência e autopoiese. O primeiro, definido, segundo Peter Cariani
(2009) como o surgimento de novas entidades que, em um sentido ou em outro, não
poderiam ter sido previstas com base naquilo que as precedeu, enquanto a autopoiesis,
ou autopoiese, (do grego auto "próprio", poiesis "criação") descreve os sistemas
autônomos, capazes de autoproduzir e autorregular, mantendo interações com o meio.
Por sua vez, o meio pode desencadear, apenas de forma indireta, mudanças nos
processos ou estrutura internas do sistema autopoiético (MATURANA e VARELA,
1997).
      A utilização de computadores e dispositivos digitais, especificamente no
processo de composição ou de produção artística, tem criado um vínculo único entre
os domínios científico e artístico. Esta relação se estabelece na música, por exemplo,
à medida em que todo o processo de produção musical vem se tornando cada vez mais
dependente dos meios tecnológicos (ROADS, 1996). No entanto, esta dependência é
recíproca, posto que alguns problemas de cunho primariamente artísticos podem vir a
propor novos desafios para a comunidade científica. Vários exemplos desta
dependência recíproca podem ser percebidas nos trabalhos aqui apresentados e nos
trabalhos desenvolvidos no Midialab e LATE. Todas essas características definem
uma relação estreita entre os domínios da ciência e da arte. Esta forte interação é
capaz de gerar outros campos de estudo onde se torna difícil definir os limites de cada
área, como na arte eletrônica, criatividade computacional, sistemas emergentes e
autopoiéticos.
      A importância da tecnologia no processo composicional e mais
especificamente na produção artística vem aumentando de forma diretamente
proporcional ao desenvolvimento das tecnologias ligadas à computação e à eletrônica.
Este aperfeiçoamento dos computadores e suas linguagens de programação têm
permitido o desenvolvimento de ferramentas para várias áreas, inclusive para a arte. A
programação de alto nível, analogamente à composição musical, exige um processo
mental de extrema atenção aos detalhes. Portanto, não é surpresa que os músicos
tenham sido, segundo Curtis Roads (1996), alguns dos primeiros artistas a se
utilizarem do uso massivo dos computadores.
      A habilidade de programar em qualquer linguagem, por sua vez, dá ao artista a
possibilidade de entender melhor o funcionamento do computador, permitindo que ele
possa ser capaz de manipulá-lo com mais precisão, desenvolvendo e moldando seus
próprios algoritmos, imprimindo, assim, aspectos importantes de sua criatividade a
fim de atingir o resultado desejado. Segundo John Chowning, a linguagem de
programação não é simplesmente uma ferramenta para se atingir um objetivo ou
realizar uma tarefa, mas uma base estruturada extensa com a qual a imaginação pode
interagir (apud. ROADS, 1996). Neste sentido, o artista passa a assumir também o
papel de designer (projetista), devendo considerar aspectos técnicos como os que são
abordados nesta pesquisa.
      Muitos estudos podem ser desenvolvidos na confluência das áreas citadas
envolvendo o conceito de criatividade, consciência, emergência e autopoiese no
sentido de criar, ou simular, através da IA, estes atributos inerentes à existência
humana. Esta simulação pode se dar através do desenvolvimento de sistemas
artísticos inteligentes que sejam capazes de se expressar de forma autônoma em
termos artísticos, através de música ou imagem. Assim, alguns trabalhos, resultados
de pesquisas e obras que utilizam diferentes técnicas de IA, serão aqui apresentados a
fim de demonstrar o delineamento das intersecções entre os diferentes conceitos
apresentados.
      De um ponto de vista computacional, como podemos definir sistemas que
exibem um comportamento inteligente e criativo ao produzir resultados artísticos
emergentes? Quais princípios, características, funções, um sistema tem que
demonstrar para ser considerado como tal?  A fim de responder tais questões fez-se
necessária a criação do termo Artelligent, que tem sua origem na junção das palavras
em latim ars/artis e intelligere. A palavra ars, correspondente ao termo grego tékne,
descreve basicamente uma habilidade prática para produzir artefatos e ao mesmo
denota um conhecimento, ciência, método relacionado à alguma teoria arte-científica.
Para este caso, consideramos que trata-se de um conceito envolvendo a expressão ou
habilidade imaginativa representadas em suas mais amplas vertentes como pintura,
musica e literatura e que tem associado algum potencial emocional ou estético. Já a
palavra intelligere, também oriunda do latim, está na raiz da palavra inteligente e é
uma composição da palavras inter, isto é, entre, e legere, que significa escolher,
coletar, colher, reunir. Juntas, inter e legere significam perceber, reconhecer,
compreender, entender e realizar.
      Durante a pesquisa desenvolvida no âmbito do mestrado, foi investigada a
possibilidade de desenvolvimento de sistemas inteligentes capazes de gerar resultados
emergentes (BARRETTO, 2011). Essa investigação anterior visou a exploração da
transição determinista-emergente em máquinas computacionais, sob o ponto de vista
de que mesmo sendo o computador um aparato primariamente determinista seria
possível desenvolver sistemas inteligentes que, não por mero uso da aleatoriedade,
pudessem demonstrar esse comportamento emergente. A idéia de Artelligent emerge
então da pesquisa de mestrado, onde pode-se já perceber algumas claras intersecções
interdisciplinares envolvendo e entrelaçando o referencial teórico levantado e
aprofundado nesta tese aplicando-o exatamente no contexto da arte, inteligência
artificial e criatividade computacional.
      Além disso, estes conceitos abordados permeiam praticamente toda a
produção artística do autor, que os utiliza para ilustrar e demonstrar as características
elencada. Pode-se perceber, durante o processo de pesquisa e desenvolvimento, que as
diferentes teorias, de diferentes áreas, convergem em pontos específicos ainda que
tenham sido desenvolvidas em campos distintos do saber. Propomos, portanto, este
novo termo para designar estas relações e de forma mais profunda caracterizar toda
uma gama de trabalhos, obras, instalações que demonstram estas características
comuns. Sobre este objeto, apresenta-se uma hipótese que versa sobre a possibilidade
de delineamento de um framework teórico-técnico que servirá como um norte para o
desenvolvimento de sistemas capazes de exibir um comportamento criativo,
denominados Artelligent.
      Um sistema Artelligent pode ser definido como um sistema autopoiético que
através da utilização de técnicas especificas de inteligência artificial, representa o
conhecimento de forma extensível e, considerando os princípios que regem o
processo criativo humano,  é capaz de exibir resultados de caráter reconhecidamente
emergentes em um determinado ambiente. No entanto, é preciso esclarecer cada uma
das partes que compõem esta definição posto que, certamente, ela não englobará
qualquer sistema artístico que envolva arte e inteligência artificial apenas.
      De uma forma mais objetiva, pode-se dizer que um sistema Artelligent deve
atender aos seguintes requisitos, que foram definidos ao longo desta pesquisa:
a.	Utilizar um agente inteligente ou um conjunto/sistema de agentes
inteligentes e seu ambiente de tarefa (item 1.3);
b.	Utilizar alguma técnica de IA, para descrever e implementar o item
acima, que facilite o surgimento de um comportamento emergente
(item 1.4);
c.	Represente o conhecimento de tal forma que ele seja extensível ou
emergente (item 1.5);
d.	Utilize os princípios para design de agentes, minimizando o papel do
designer na construção do conhecimento agente ao passo em que se
maximiza o papel da aprendizagem e/ou adaptação (item 2.1);
e.	Considere pelo menos dois Ps: Person (Pessoa), Process (Processo),
Product (Produto) e Press/Place (Ambiente/Local) (item 2.2.1);
f.	Seja capaz de gerar Produto(s) considerados no mínimo mini-C (item
2.2.1);
g.	Considere pelo menos uma das abordagens para criatividade:
desenvolvimentista, processual, evolutiva ou sistêmica (itens 2.2.1.1 a
2.2.1.4);
h.	Demonstre um comportamento autopoiético no que diz respeito ao
gerenciamento das suas estruturas internas de conhecimento e/ou
cognição (item 2.3);
i.	Seja capaz de exibir comportamento emergente (combinatória ou
epistêmica) ou demonstre algum tipo de co-emergência dinâmica com
o ambiente (item 2.4).
      Na primeira seção aqui apresentada, será abordada a relação entre a área da IA
e da arte computacional, apresentando um breve histórico da área a fim de introduzir
ao leitor os conceitos fundamentais da IA e a forma como eles foram construídos ao
longo do tempo. Estes conceitos apresentados são fundamentais para a compreensão
dos itens a, b e c da descrição de um sistema Artelligent. Serão privilegiados os
aspectos relacionados à cognição em sistemas inteligentes englobando a representação
de conhecimento, aprendizagem e expressão com enfoque nas técnicas de Algoritmos
Genéticos, Redes Neurais e Sistemas Multiagentes.
      Para qualquer estudo que permeie a IA, a identificação e codificação dos
dados de forma a constituir e representar conhecimento de modo que a “máquina”
seja capaz de compreendê-lo, processá-lo e, então, agir é fundamental para a
estruturação cognitiva de um sistema inteligente. Uma vez estruturado o
conhecimento, dada a possibilidade de compreensão do mesmo, é possível que o
sistema aprenda através da avaliação heurística dos resultados obtidos através da
aplicação de um método de “raciocínio”. Ao ser capaz de inferir, baseado em uma
heurística, a “distância” entre o resultado esperado e o resultado obtido é possível
guiar as ações ou “decisões” do sistema na direção de um determinado objetivo que se
quer atingir. Este aspecto é apresentado sob o prisma de três abordagens: simbólica,
conexionista e evolutiva.
      Na segunda seção, é apresentado um delineamento das intersecções entre IA,
criatividade, autopoiese e emergência. São apresentados os princípios para design de
agentes, que segundo os estudos a IA corporificada, se concentram em princípios de
design e princípios de  design de agentes. Além disso aborda-se de forma mais
profunda a questão da criatividade, seus modelos, abordagens e representações a fim
de tornar mais claros os princípios que influenciam diretamente na criação de um
sistema artístico inteligente. Posteriormente são discutidos os conceitos de autopoiese
e emergência dentro do contexto da inteligência artificial e criatividade
computacional.
      A criatividade computacional pode ser simulada através de uma série de
algoritmos sendo alguns deles aplicáveis ou possivelmente encontrados nas técnicas
de IA. Uma outra abordagem possível pode ser a identificação da criatividade em
sistemas inteligentes cujo objetivo não era, necessariamente, a simulação da
criatividade. A identificação dos sistemas criativos pode ser detectada através da
obtenção de resultados emergentes posto que uma das características da criatividade,
assim como na emergência, é o surgimento de novas informações ou criação de novas
formas que não existiam antes. Estes sistemas inteligentes, que demonstram
características emergentes, podem também ser identificados como autopoiéticos, ou
seja, se mantém e se autorregulam, como no caso das Redes Neurais.
      Por fim, na terceira seção é apresentado e analisado o jogo Zer0 sob o prisma
do conceito de Artelligent. Zer0 é um jogo que convida o interagente à realizar uma
deriva em um universo dominado por formas geométricas. Cada forma geométrica é a
representação visual de um agente inteligente capaz de decidir quais ações pretende
realizar. Além disso, a construção cognitiva do agente é desenvolvida através da sua
interação com o meio. O jogo demonstra ainda um comportamento emergente no que
diz respeito ao arranjo visual das formas na tela, assim com a composição musical que
integra a sua trilha sonora.
      Primeiramente é abordado o caráter estético e poético da obra, assim como a
motivação para o desenvolvimento do sistema. São abordadas ainda as características
listadas como primordiais para que um sistema seja considerado como Artelligent.
Além disso, descreve-se de um ponto de vista computacional a modelagem dos
agentes, do ambiente, representação do conhecimento, interação social e resultados
emergentes no que diz respeito à composição musical que resulta da interação entre os
indivíduos que compõem a população. Posteriormente são descritos os aspectos de
implementação, incluindo os agentes e o ambiente, assim como os resultados que
emergem desta interação.



SEÇÃO I |Inteligência Artificial na Arte Computacional
      Considerando os princípios e requisitos que compõem um sistema considerado
Artelligent, sobretudo os que dizem respeito à (a) utilização de um agente ou conjunto
de agentes inteligentes e seu ambiente de tarefa, (b) a utilização de alguma técnica de
IA para descrever o agente e (c) a sua respectiva representação de conhecimento,
necessitamos em um primeiro momento compreender o escopo de estudo do campo
da IA e suas possíveis aplicações.
      Aqui serão descritos estes três itens listados acima observando sob a ótica da
capacidade de facilitar o surgimento de resultados emergentes. Portanto, além da
abordagem histórica que facilita a contextualização do problema, as técnicas e
abordagens aqui descritas possuem grande afinidade com o conceito de Artelligent,
sempre ressaltando a questão da capacidade de adaptação, autonomia e emergência.
1.1.	Do silogismo aos sistemas lógicos
      Várias áreas contribuíram para a formação do conceito de inteligência
artificial, sendo a primeira delas a filosofia. Desde meados do século IV A.C. já havia
um questionamento acerca da possibilidade da formação de regras para obtenção de
conclusões lógica válidas. Aristóteles (384-322 a.C) propôs um sistema, formado por
um conjunto de leis que governariam a parte racional da mente. Este sistema informal
de silogismos baseava-se na premissa de que seria possível gerar conclusões
mecanicamente através de um conjunto de premissas iniciais.
      Para Luger (2004), esta tentativa de formular regras que pudessem simular a
manipulação de premissas lógicas de forma a gerar novos fatos, que pudessem ser
automaticamente considerados como verdadeiros simboliza o início da tentativa de
desenvolver métodos que pudessem reproduzir raciocínio humano. Considerar um
conjunto de regras como sendo capazes de descrever a parte formal e racional da
mente levou ao surgimento de um paradigma que considera a mente como um sistema
físico e, portanto, regido pelo mesmo sistema de leis da física.
      Neste sentido, René Descartes (1596-1650) aponta alguns problemas que
surgem desse paradigma, relacionado com a concepção puramente física da mente. O
principal destes problemas é que uma vez que a mente é governada inteiramente pelas
leis da física, então, parece não haver espaço suficiente para a expressão do livre-
arbítrio. Além disso, pode-se supor que os animais são simples máquinas de agir
arbitrariamente (RUSSEL e NORVIG, 2003). A solução proposta pelo dualismo para
justificar o livre-arbítrio parece convergir no que diz respeito à existência de parte
intangível e imaterial inerente à mente humana que estaria isenta às leis da física.
      Por outro lado, o materialismo se opõe ao dualismo no sentido de que
considera que as operações físicas do cérebro constituem a mente. Portanto, o livre-
arbítrio seria apenas uma questão de como percebemos as escolhas. É importante
ressaltar que o fato de que tanto o dualismo quanto o materialismo consideram a
mente como uma máquina capaz de realizar operações. Esta premissa tem, portanto,
implicações no desenvolvimento de disciplinas como o positivismo lógico e a teoria
da confirmação, culminando em 1928 na publicação de um livro intitulado “The
Logical Structure of The World” de Rudolf Carnap (KNEALE, 1968).
      Toda a abordagem lógica que descende do silogismo aristotélico baseia-se
necessariamente no fato de que a mente, através de processos físicos ou não, manipula
uma série de informações que estão representadas através de símbolos. Portanto, se
faz necessário compreender esta conexão entre conhecimento e ação posto que esta
relação é vital paro campo da Inteligência Artificial (IA). A partir desta relação
desenvolve-se a necessidade de compreender o raciocínio como uma ferramenta para
alcançar um determinado objetivo através da justificativa de uma conexão lógica entre
o objetivo à ser alcançado e o conhecimento do resultado das ações (RUSSEL e
NORVIG, 2003). É inegável que grande parte do desenvolvimento das pesquisas
relacionadas à área da IA tem suas bases nas teorias da abordagem lógica.
      As primeiras discussões acerca da definição conceitual de Inteligência
Artificial abordam, entre outros, o ponto de vista da busca pela compreensão do
funcionamento da inteligência humana que, por sua vez, está ligada diretamente a
nossa capacidade de compreender o mundo através das nossas habilidades cognitivas
(LUGER, 2004). Dentre estas habilidades cognitivas pode-se destacar, por exemplo, a
relevância da concepção artística no contexto da expressão desta inteligência através
do uso do nosso sistema conotativo.
      O termo Inteligência Artificial, cunhado por McCarthy e outros pesquisadores
em 1956 (MCCARTHY, MINSKY, et al., 2006), baseava-se na premissa de que todo
aspecto relacionado à aprendizagem ou inteligência pode ser matematicamente
descrito e, portanto, computacionalmente simulado. Segundo Russel e Norvig (2003),
neste sentido há várias vertentes que abordam a modelagem matemática da
Inteligência Artificial sob diferentes aspectos, como os que tentam simular o
raciocínio humano, pensamento racional, comportamento humano ou comportamento
racional.
      Ainda por volta de 1956 o objetivo era tentar simular a inteligência humana,
que requeria uma modelagem matemática muito perfeita do funcionamento do
cérebro humano, ou como pensam os humanos, a fim de obter resultados realmente
satisfatórios. Dada a complexidade do cérebro humano e a subjetividade de alguns
conceitos como "bom senso", por exemplo, tornou-se praticamente inviável a
simulação do pensamento humano.
      A evolução das pesquisas nesta área se deu no sentido de tentar simular não
mais o raciocínio humano, mas o comportamento inteligente em problemas menos
complexos cujo domínio poderia ser mais facilmente modelado matematicamente.  O
campo de estudo das pesquisas em IA, oriundo dos desdobramentos citados acima,
que desenvolve a agenda racional/lógica e segue o paradigma “tradicional” do
raciocínio lógico, deu origem ao que se convencionou chamar de GOFAI  (Good Old
Fasioned Artificial Intelligence) ou “boa e velha IA”, cujas fases estão sumarizadas na
tabela 1 abaixo (BITTENCOURT, 2006).

ÉPOCA
OBJETIVOS
MÉTODOS
FRACASSO
Clássica
(1956-1970)
Simular a inteligência
humana.
Solucionadores gerais de
problemas e uso da
lógica matemática.
Subestimação da
complexidade de
problemas.
Romântica
(1970-1980)
Simular a inteligência
humana em situações pré-
determinadas.
Formalismos de
representação de
conhecimento
adaptados ao tipo de
problema.
Subestimação da
quantidade de
conhecimento necessária
para tratar mesmo o mais
simples problema de bom
senso.
Moderna
(1980-1990)
Simular o comportamento de
um especialista ao resolver
problemas em um domínio
especifico.
Sistemas de regras.
Representação da
incerteza.
Conexionismo.
Subestimação da
complexidade do
problema de aquisição de
conhecimento
Pós-
Moderna
(1991-Hoje)
Entender o comportamento
inteligente. Construir
sistemas que apresentem
esse comportamento
inteligente contextualizado.
Integração de raciocínio,
aprendizagem e planeja-
mento. Inteligência
distribuída. Sistemas
híbridos.

Tabela 1: Fases da GOFAI, adaptado de Bittencourt (2006).
1.2.	Da GOFAI aos Sistemas Evolutivos
      Mesmo para a GOFAI, a definição do conceito de IA continua pouco concisa,
pois é difícil encontrar uma definição unificada não-ambígua acerca do sujeito
(LUGER, 2004). Segundo Russel e Norvig (2003), as diversas definições de IA
podem ser divididas em quatro grandes categorias conforme figura 1 abaixo.

Figura 1: Algumas definições de IA agrupadas em quatro grandes categorias (RUSSEL e
NORVIG, 2003)
      Cada uma destas categorias define IA sob um paradigma diferente. Os
sistemas que pensam humanamente enfatizam a abordagem a partir do modelo
cognitivo, ou seja, baseiam-se na forma como a mente humana funciona. Para que
isso seja possível, se faz necessário entender como os processos cognitivos são
desencadeados na mente humana e, segundo Russel, isto só é possível através de
basicamente duas formas: introspecção (através da análise dos nossos pensamentos na
medida em que eles acontecem) ou experimentos psicológicos. O campo
interdisciplinar das ciências cognitivas congrega modelos computacionais de IA e
técnicas experimentais de psicologia a fim de construir teorias precisas a respeito do
funcionamento da mente humana (RUSSEL e NORVIG, 2003).
      Os sistemas que pensam racionalmente tiveram com Aristóteles (384-322
A.C.) as primeiras tentativas de realização de um sistema informal de silogismos
(abordagem lógica) a definir um conjunto preciso de regras que representariam um
processo racional irrefutável da mente. Este conjunto de regras permitia que algumas
conclusões fossem geradas de forma mecânica baseadas em premissas iniciais
(RUSSEL e NORVIG, 2003).
      Pensava-se que essas leis que regiam o pensamento deveriam, também,
governar a operação da mente, culminando assim no surgimento do campo da lógica.
Na visão lógica, qualquer problema é solucionável uma vez expresso através de uma
relação lógica, em uma notação específica. 	Nesta abordagem residem dois
problemas principais: a dificuldade de expressar conhecimento informal em termos
formais e o fato de que qualquer problema com algumas dezenas de sentenças pode
sobrecarregar a capacidade de processamento do computador, caso não haja um guia
sobre qual possibilidade tentar primeiro.
      O teste formulado por Alan Turing, descrito em 1950 no artigo Computing
Machinery and Intelligence (TURING, 1950), nos dá uma noção operacional de
inteligência, ou seja, um comportamento seria inteligente dado um contexto singular
de questões medindo o desempenho de uma máquina “inteligente” através do
comparativo com o desempenho humano (LUGER, 2004). Uma versão deste teste
consiste em colocar em salas separadas uma máquina e um correspondente humano,
interrogador, que deverá distinguir através da realização de uma série de perguntas e
respostas qual o interlocutor humano e qual é a máquina. Esta abordagem é baseada
no comportamento humano e, portanto, aplicável para sistemas que agem como
humanos. Contudo, este tipo de comportamento descrito no teste de Turing requisita
do computador algumas habilidades como, por exemplo, a capacidade de processar
linguagem natural e formalizar representações de conhecimento e aprendizagem de
máquina limitando-se ao desenvolvimento de sistemas que atuam como humanos.
(RUSSEL e NORVIG, 2003).
      Os sistemas que agem racionalmente têm como objetivo o desenvolvimento de
agentes racionais capazes de atingir o melhor resultado possível ou, quando incapazes
de distinguir, atingir o melhor resultado esperado. Neste caso, as decisões são
baseadas em inferências que devem ser corretas, pois, através destas inferências,
pode-se julgar uma determinada ação como forma de atingir o objetivo. Entretanto,
existem situações onde ainda não há um objetivo concreto ou um resultado esperado
e, ainda assim, as decisões devem ser tomadas, portanto, a ação racional não deve se
basear apenas nas inferências realizadas, mas, também, segundo Peter Norvig (2003),
na adaptabilidade do sistema a diversos contextos.
      Apesar de ser permeada por diversas áreas como matemática, economia,
neurociência, psicologia, cibernética e linguística, por exemplo, o desenvolvimento da
GOFAI apresenta algumas limitações quanto à sua abordagem lógica e racional, ao
tentar interagir com domínios incertos ou em transformação.
      Ainda em 1943, antes mesmo da GOFAI, o primeiro trabalho largamente
reconhecido como IA desenvolvido por Warren McCulloch e Walter Pitts
(MCCULLOCH e PITTS, 1943) propunha um modelo artificial de neurônios onde
cada neurônio poderia ser caracterizado como “ligado” ou “desligado”. A mudança
para “ligado” ocorreria ao neurônio quando sofresse estímulo suficiente através de
seus vizinhos, propondo uma ênfase menor na lógica e mais acentuada no
funcionamento do cérebro físico (RUSSEL e NORVIG, 2003).
      Esta abordagem conexionista, (descrita no item 1.5.2) apoia-se na modelagem
biológica da arquitetura neuronal. Os modelos neuronais da inteligência enfatizam
uma outra habilidade do cérebro: a de se adaptar ao mundo no qual está inserido,
através da modificação ou recombinação dos relacionamentos entre os neurônios
individuais (LUGER, 2004).  Nesta abordagem, ao invés de representar o
conhecimento através de sentenças lógicas explicitas, o sistema é capaz de capturar o
conhecimento de forma implícita como um padrão de relacionamentos.
      Um outro modelo de inteligência, alternativo à GOFAI, busca inspiração nos
processos evolutivos. Os trabalhos envolvendo vida artificial e algoritmos genéticos
(descrito no item 1.4.1) aplicam, computacionalmente, os princípios da evolução
biológica. Ao invés de raciocinar logicamente, esta abordagem propõe que ao gerar
sucessivamente diversas populações de potenciais soluções que competem entre si,
pode-se forçar uma evolução no sentido de atingir um determinado objetivo
quantificável através de uma função de adaptação.
      O estudo da inteligência artificial é, portanto, um dos campos da tecnologia
cujos limites de pesquisa vão bem além da análise técnica. A princípio ela pode servir
também para investigar a natureza do ser humano e da inteligência humana, os limites
da máquina e nossos próprios limites como construtores de artefatos “inteligentes”
(WILSON, 1995).
      Não é surpresa que os artistas passassem, portanto, a explorar o vasto campo
da IA como forma de produzir trabalhos interativos mais sofisticados que vão além
das simples janelas e menus que tanto caracterizam a multimídia. Alguns artistas
acreditam que os campos da vida artificial e da inteligência artificial oferecem várias
abordagens para o desenvolvimento destes trabalhos: sistemas especialistas (ES),
processamento de linguagem natural, algoritmos genéticos (AG), redes neurais
artificiais (RNA), entre outras (WILSON, 2002).
      A concepção e desenvolvimento de obras de arte, sejam elas instalações,
performances ou sistemas, utilizando inteligência artificial tem sido estudada por
alguns artistas como, por exemplo na figura 2, Stephen Wilson em Is There Anyone
There (1992), quando através de uma cabine telefônica o interagente pode estabelecer
contato com personagens artificiais, e Nicolas Reeves que utiliza em Mutations of The
White Doe (1997) autômatos celulares, algoritmos genéticos e redes neurais para
simulação de sistemas de vida artificial.
 (a)   (b)
Figura 2: (a) Is There Anyone There, Stephen Wilson (1992) e (b) Mutations of The White Doe,
Nicolas Reeves (1997).
      Para Stephen Wilson, em seu livro Information Arts (2002), o
desenvolvimento de algoritmos e heurísticas capazes de permitir que os computadores
realizem análises sofisticadas ou demonstrem um comportamento complexo, como
produzir arte, constam entre os grandes desafios da pesquisa científica
contemporânea. Este desafio deriva não apenas do desenvolvimento de novas
tecnologias capazes de suportar a demanda computacional de tais algoritmos, mas
também, da necessidade de se entender o fenômeno da inteligência através de novas
perspectivas e abordagens capazes de levantar novos questionamentos filosóficos
acerca do assunto. Silvia Laurentiz aponta que um destes questionamentos levantados
pelo estudo dos sistemas e agentes inteligentes no computador é exatamente sobre os
próprios termos utilizados na área, como a palavra inteligência, por exemplo
(LAURENTIZ, 2007).
      Alguns artistas buscaram criar e desenvolver trabalhos artísticos sobre a forma
de verdadeiros programas de agentes de forma mais metafórica. O projeto
Impermanence Agent (1998-2003), figura 3, desenvolvido para Internet por Wardrip-
Fruin, Chapman, Moss e Whitehurst (MOSS e WARDRIP-FRUIN, 2002), estende as
funcionalidades do browser do internauta e constrói uma representação de acordo
com o histórico dos sítios visitados. Uma vez instalado o programa, a obra toma a
forma de uma janela do navegador que permanece aberta onde o agente, com o passar
do tempo, se integra aos hábitos do internauta e se utiliza do caráter fugaz da Web
para construir uma narrativa pessoal nesta janela, utilizando fragmentos dos sítios
visitados enquanto, para Christiane Paul (2002), questiona a natureza da hipermídia.

Figura 3: Impermanence Agent (Wardrip-Fruin e Moss, 2002).
1.3.	Agentes Inteligentes
      Segundo Negroponte (1995), o conceito de “agente” é incorporado ao fato de
ajudarmo-nos uns aos outros, frequentemente. É um conceito onde a especialização
encontra-se mesclada ao conhecimento do agente sobre o interagente humano. Além
disso, Minsky, já em 1987, apontava que o desenvolvimento de agentes de interface é
normalmente encarado como uma máquina central e onisciente, muito embora o mais
provável é que o agente se componha por uma série de programas de computador e
aplicativos móveis, sendo executados distribuidamente em computadores ou
dispositivos móveis (como celulares, por exemplo), onde cada programa é especialista
em alguma coisa e eficiente em matéria de intercomunicação (apud. NEGROPONTE,
1995).
      Segundo Russel e Norvig (2003), um agente pode ser entendido como aquele
capaz de perceber o ambiente em que está inserido através de sensores e interagir com
o mesmo através de atuadores, conforme ilustrado na figura 4.

Figura 4: Representação de um agente capaz de interagir com o ambiente através de sensores e
atuadores (RUSSEL e NORVIG, 2003).
      Um agente humano possui olhos, ouvidos entre outros órgãos capazes de
sentir o mundo, além de mãos, pernas entre outra partes capazes de atuar sobre o
meio. De mesmo, um agente deve ter formas de sentir e interagir com o meio físico
ou virtual para que possa desenvolver sua cognição. Esta capacidade cognitiva do
agente, por sua vez, pode ser representada através de agentes inteligentes
expressando-se de diversas formas distintas como a música, a pintura e a poesia, por
exemplo.
      Geoffrey Jefferson (1949) define a necessidade de dotar a máquina de
capacidade cognitiva a fim de produzir artefatos como sonetos e concertos para que
pudessem ser comparadas à mente humana. Para Jefferson, enquanto a máquina não
for capaz de compor um concerto, motivada pelos pensamentos e “emoções” que
sente e não pelo mero acaso, não poderemos dizer que a máquina se equipara ao
cérebro. Isto significa dizer que a máquina não só deve ser capaz de produzir tais
artefatos como também ter consciência de que o fez.
      Segundo Robert Rowe, a área da cognição musical, mais especificamente,
ocupa uma posição intermediária entre a teoria musical, as ciências cognitivas e a
inteligência artificial. Neste sentido, a inteligência artificial se aplica à simulação do
raciocínio humano em sistemas computacionais enquanto as ciências cognitivas se
interessam em estudar a viabilidade dos modelos de processos mentais propostos
através da implementação dos mesmos (ROWE, 2001).
      O termo percepção é utilizado para referir-se aos inputs perceptuais do agente
em um dado momento. Em geral, pode-se dizer que a ação a ser executada, escolhida
pelo agente em um dado momento deve depender da avaliação de todo o histórico de
“percepções” do agente até o momento presente. Matematicamente falando, o
comportamento do agente pode ser definido como uma  função capaz de mapear
qualquer sequência de percepções em ações (RUSSEL e NORVIG, 2003).
      Há diversas abordagens conhecidas e bastante exploradas em Russel e Norvig
(2003) para desenvolvimento da função que guiará o comportamento do agente. O
sistema baseado em regras, figura 5, é a abordagem mais simplificada pois, neste
caso, considera-se apenas os inputs do momento atual, em detrimento do histórico de
entradas. Normalmente estas funções são definidas através de regras do tipo
condição-ação (if-then-else). Pela estrutura simples utilizada para tecer o raciocínio do
agente ele é capaz de exercer tarefas de limitada inteligência, normalmente incapaz de
adaptar-se à mudanças no ambiente. Uma outra abordagem possível é a do sistema
baseado em objetivos onde o agente toma as decisões buscando atingir um objetivo
predeterminado.

Figura 5: Modelo genérico de um agente reativo simples capaz de interagir com o meio (Russel e
Norvig, 2003).
      A formalização de conhecimento permite ao agente rever as ações tomadas
através de um processo de aprendizagem. A ideia por trás do aprendizado é que a
percepção do agente deve ser usada não apenas para guiar a tomada de decisão mas,
também, para melhorar a performance do agente em ações futuras. A aprendizagem se
dá quando o agente analisa o resultado das suas ações e o seu processo de decisão,
resguardando a capacidade de alterá-lo.
      Admitir que um sistema seja capaz de aprender analisando as suas decisões ou
o processo de raciocínio desenvolvido implica, em alguns casos, em alterações na
resposta do sistema aos mesmos inputs, sobretudo se o domínio de conhecimento sob
o qual age o sistema é incerto.
1.3.1.	Agentes Baseados em Objetivos e o Raciocínio Prático
      Dentre os diferentes modelos de agentes, os baseados em objetivo destacam-se
pela capacidade de ir além do simples conhecimento sobre o estado atual do
ambiente. Esta categoria de agentes baseia-se em informações referentes à objetivos,
que descrevem situações desejáveis.
      O programa do agente combina estas informações a fim de traçar planos de
ação que o aproximem do objetivo. Por vezes, esta seleção é direta, quando a
satisfação do objetivo resulta da execução direta de uma única ação. Em outros casos,
longas sequências de ações são necessárias para atingir o objetivo. Portanto, busca e
planejamento são dois subcampos envolvidos neste caso.
      Um dos métodos de raciocínio utilizados nestes casos, quando o agente deve
tomar uma decisão sobre qual objetivo ou planos seguir é o prático. Este tipo de
raciocínio baseia-se nos processos cognitivos cujo foco são as ações que devem ser
executadas, considerando um determinado objetivo. Este processo de raciocínio pode
ser dividido em duas grandes áreas: deliberação e raciocínio meios-fim. Na primeira
parte, decide-se qual o objetivo enquanto na segunda define-se quais os passos
necessários para atingi-lo (WOOLDRIDGE, 2009).
      O modelo BDI (Belief-Desire-Intention) é bastante utilizado para implementar
este tipo de raciocínio pois baseia-se nas crenças, planos e objetivos. As crenças
(beliefs) constituem a base de conhecimento do agente, onde temos representados
através de algum tipo de formalismo os fatos sobre o mundo que o agente tem ciência.
Os objetivos (desire) representam os estados ou condições que o agente almeja
alcançar. Os planos  de ação (intentions) são seqüências lógicas de ações necessárias
para executar ou alcançar os objetivos e podem possuir algum tipo de pré-condição,
prioridade e gatilhos (GEORGEFF, PELL, et al., 1998).
1.3.2.	Ambientes de Tarefa
      Tão importante quanto descrever um agente é descrever, especificar ou
escolher corretamente o seu ambiente de tarefa. Um ambiente de tarefa pode ser
definido como o contexto onde um agente será capaz de interagir. Sem dúvida a
quantidade de ambientes que podem existir é enorme, porém podemos categorizar os
ambientes de acordo com as suas características. Estas categorias estão divididas em
algumas dimensões, que por sua vez influenciarão na decisão do projetista (designer)
ao formular o agente. É imprescindível ressaltar que nesta categorização, proposta por
Russel e Norvig (2003), assumimos a descrição do ambiente sob o ponto de vista do
agente.
      A primeira dimensão analisada diz respeito à capacidade do agente de
observar o ambiente de forma total ou parcial. Classificamos o ambiente como
completamente observável quando o agente é capaz de recuperar todos os aspectos
relevantes do ambiente antes de tomar uma decisão. Um ambiente completamente
observável pode ainda conter informações que permanecerão desconhecidas ou
inacessíveis ao agente contanto que estas informações sejam irrelevantes para a
tomada de decisão. Por sua vez, a relevância do agente depende da sua medida de
desempenho que descreve quais são seus objetivos ou qual é a função que analisa o
desempenho do agente. No entanto, dificilmente um ambiente será completamente
observável para um agente a menos que o agente seja onisciente. Para os casos em
que alguma informação relevante esteja indisponível ou inacessível, classificamos o
ambiente como parcialmente observável. Na total ausência de informação relevante, o
ambiente torna inobservável.
      A distinção dos ambientes quanto ao número de agentes que atuam
simultaneamente sobre eles fornece uma outra dimensão importante. Um ambiente
onde um agente resolve um caça palavras pode ser considerado um ambiente de
agente único, enquanto outro agente engajado em um jogo de xadrez contra um
agente humano ou com outro agente computacional está em um contexto multiagente.
Ainda no que diz respeito aos ambientes multiagentes, podemos classifica-los como
competitivos ou cooperativos de acordo como os agentes interagem uns com os
outros. Se temos dois agentes, como no jogo da velha, que tentam mutuamente
minimizar a medida de desempenho do oponente caracterizamos como competitivo.
Por outro lado, se há dois agentes que tomam decisões com o intuito de maximizar a
medida de desempenho de todo um conjunto de agentes, o ambiente define-se
cooperativo.
      As mudanças que acontecem em um ambiente podem ou não ser previsíveis
para um agente. Em alguns contextos, um agente é capaz de deduzir qual o próximo
estado do ambiente ou quais são as consequências das suas ações pois a mudança no
ambiente depende apenas da decisão tomada, caracterizando um ambiente
determinista. Por outro lado, se um ambiente é parcialmente observável ele pode
parecer estocástico para o agente. Podemos afirmar que um ambiente é incerto ou
estocástico se ele não é completamente observável ou determinístico.
      Alguns ambientes são tratados pela experiência do agente através da
representação em episódios atômicos. Em um jogo de damas, por exemplo, cada
configuração do tabuleiro representa um episódio atômico e bem definido no sentido
de que a próxima ação não dependerá das ações anteriores, apenas do estado atual do
ambiente. Para este tipo de ambiente episódico, não há a necessidade de uma
representação ou narrativa temporal que realize algum tipo de interligação entre os
estados de mundo. Por outro lado, um agente que necessite deslocar-se de um lugar à
outro dentro de um labirinto necessitará armazenar um histórico das ações realizadas
anteriormente a fim de facilitar o deslocamento. Este tipo de ambiente sequencial são
mais complexos pois não apenas se faz necessário armazenar um histórico de ações
(que pode ser limitado) como também pode forçar o agente a planejar ações futuras.
      Após adquirir informação sobre o ambiente, um agente passa para um estado
de deliberação onde, através de algum mecanismo de inferência, vai analisar as
informações adquiridas a fim de determinar qual ação será executada. Neste intervalo
de tempo em que o agente está deliberando, o ambiente pode sofrer alterações
caracterizando um ambiente dinâmico. Caso o ambiente não seja alterado enquanto o
agente raciocina, o ambiente é declarado como estático.
      Os agentes que trabalham com tarefas temporizadas caracterizam os ambientes
de acordo como a dimensão temporal é tratada. Se temos um ambiente, cuja dimensão
temporal é corrente e ininterrupta declaramos o ambiente como contínuo. Por
exemplo, ao se deslocar fisicamente um robô considera o mundo como estados e
tempo contínuos posto que as informações variam de forma continua no decorrer do
tempo, enquanto o agente que joga damas pode considerar o tempo como sendo
discreto posto que as mudanças acontecem de forma discreta, divididas em amostras
temporais.
      A base de conhecimentos de um agente contém informações relevantes para o
agente no que tange o ambiente em que será inserido. Em alguns casos é possível
inserir todas as informações existentes sobre uma determinada tarefa, como jogar
damas ou xadrez, pois o ambiente é conhecido. Por outro lado, dada uma limitação
pelo tamanho do escopo de conhecimento ou pela falta de conhecimento do projetista
pode-se construir uma base de conhecimento que não detenha todas as informações
necessárias, determinando um ambiente desconhecido. O ambiente desconhecido
fornece desafios adicionais ao agente pois este deverá aprender como interagir no
ambiente em que será inserido. Um ambiente pode também ser desconhecido
intencionalmente pelo agente, se assim o desejar o seu designer. Para estes casos o
processo torna-se interessante pois o comportamento do agente deve emergir da
interação com o ambiente.
      Uma vez determinadas as características do ambiente, de acordo com as
dimensões citadas anteriormente, podemos descrever de forma genérica o agente
através do seu PEAS (Performance, Environment, Actuators, Sensors) ou DAAS
(Desempenho, Ambiente, Atuadores, Sensores). A medida de desempenho, ou
performance,  descreve o que desejamos que o nosso agente tenha como principais
objetivos ou premissas que deve manter válidas. O ambiente é descrito utilizando as
dimensões de disponibilidade de informações, quantidade de agentes, previsibilidade,
representação episódica, dinâmica, conhecimento disponível. Os atuadores descrevem
quais formas o agente tem para se expressar e modificar o ambiente enquanto os
sensores descrevem como o agente é capaz de perceber e coletar informações
disponíveis (e observáveis) do ambiente.
1.4.	Sistemas E-volutivos, Conexionistas e Sociais
1.4.1.	Algoritmos Genéticos
      A computação evolutiva, utiliza a metáfora baseada na teoria da evolução
natural proposta por John Holland (1975) e considera que não é imperativo ter um
conhecimento prévio de uma formalização (algoritmo ou representação de mundo)
explícita para se encontrar uma solução para um problema.
      Com os algoritmos genéticos e a vida artificial as soluções para o problema
são evoluídas através de múltiplas gerações, melhorando as soluções anteriores a cada
nova interação. Tal qual na evolução natural, os operadores genéticos como a
recombinação e a mutação permitem que a cada nova geração de indivíduos sejam
geradas soluções potenciais cada vez melhores para o problema. Esta evolução no
sentido da busca pela “melhor” solução se dá através de uma função de avaliação
(fitness) que calcula o grau de adaptação do indivíduo e seus vizinhos, inferindo quais
os mais aptos à reprodução e , por conseguinte, quais variações estão mais aptas à
extinção.
      Na aplicação desta abordagem a um problema qualquer, a representação do
conhecimento se dá na definição da estrutura do cromossomo. Geralmente esta
codificação é dada na forma de uma sequência de dígitos com um tamanho fixo,
facilitando a implementação dos operadores genéticos. A estrutura deve permitir
mapear o cromossomo no espaço das possíveis soluções para o sistema, portanto cada
variável ou característica relevante do sistema deve ser mapeada no cromossomo
como um dígito ou um conjunto de dígitos. Esta sequência de dígitos é chamada
genótipo e representa o cromossomo do indivíduo, que por sua vez, contém vários
alelos. Segundo Fernandes (2002), este mapeamento das informações que são
relevantes para o domínio e, portanto, codificadas no cromossomo são fundamentais
para a determinação da função de avaliação.
      A função de avaliação deve ser específica para o problema e, em geral, associa
um número real ao cromossomo tal que valores maiores indicam maior adequação do
indivíduo. Durante a fase evolutiva são selecionados alguns indivíduos da população
para o cruzamento com a finalidade de gerar descendentes que constituirão a geração
seguinte a ser analisada pela função de avaliação.
      A seleção dos pais e a aplicação dos operadores genéticos de recombinação e
mutação obedecem a parâmetros randômicos. Na seleção dos pais, é atribuída à cada
indivíduo uma probabilidade de ser escolhido proporcional ao grau de adaptabilidade
inferido pela função de avaliação. Ao serem escolhidos os pais, aplica-se o operador
de recombinação. Este operador divide os cromossomos pais em uma posição
aleatória, produzindo assim dois pedaços em cada pai. Posteriormente, os pedaços
obtidos são intercalados produzindo-se novos cromossomos completos, ambos
descendentes de cada um dos pais.
      O operador de mutação desenvolve um papel importantíssimo na geração de
novos cromossomos e, naturalmente, facilitando o processo de surgimento de novas
possibilidades não previstas anteriormente pelo sistema. Este operador é aplicado
individualmente à cada filho resultante do processo de recombinação e consiste em
uma alteração aleatória em cada um dos alelos. Normalmente, é associada uma
probabilidade muito pequena de mutação para cada alelo, o suficiente apenas para
garantir que nenhum espaço de busca tenha probabilidade zero de ser examinada.
      Por fim, apesar da existência de diversas técnicas adaptativas que se baseiam
nos modelos biológicos existentes na natureza, como os algoritmos genéticos e as
redes neurais artificiais, Tom Froese e Tom Ziemke apontam que estas abordagens
focam no seu próprio estabelecimento e implementação como uma alternativa viável
aos paradigmas tradicionais da computação (FROESE e ZIEMKE, 2009). No entanto,
há para eles um esforço insuficiente no que tange o entrelaçamento entre teorias de
diferentes campos fora da IA, como as teorias biológicas e a fenomenologia, como
possíveis forma de solucionar os problemas ligados à autonomia natural e a
corporificação dos sistemas viventes.
      Entre os projetos que trabalham o tema da evolução das espécies estão as
obras do Karl Sims: Genetic Images (1993), Evolved Creatures (1994) e Galápagos
(1997) (apud. PAUL, 2008). Ambos permitem que o interagente influencie em uma
evolução simulada onde os organismos/indivíduos da população são escolhidos
através do parâmetro estético. Em Galápagos, figura 6, são exibidos organismos
abstratos em várias telas dispostas em círculo onde o interagente pode escolher as que
considera mais sedutoras. Os organismos escolhidos reagem mutando ou
reproduzindo-se a fim de substituir os indivíduos menos preteridos e, portanto, menos
adaptados. Esta evolução simulada é fruto de uma interação entre o homem e a
máquina onde o interagente exerce um controle criativo direto ao tomar uma decisão
de ordem estética.

Figura 6: Formas emergentes da obra Galápagos, Karl Sims (1997)
      Para Laurentiz (2007), a obra Evolved Creatures (1994) de Karl Sims explora
as consequências de se trabalhar de forma conjunta com diferentes conceitos como os
algoritmos simples que controlam a morfologia do indivíduo, as redes neurais e
sensores responsáveis pelo controle das reações dos indivíduos no ambiente e
aprendizado, além de outras funções como, por exemplo, a que é implementada de
forma a estimular a competição entre os indivíduos orientados por um propósito
comum.
1.4.2.	Redes Neurais Artificiais
      A linha conexionista, visa modelar a inteligência humana através da simulação
da estrutura e funcionamento do cérebro, em especial dos neurônios e suas ligações
sinápticas. O processamento de informação no cérebro é realizado pelo neurônio, com
sinais elétricos propagando-se entre os neurônios através das sinapses.
      Uma representação simplificada de um neurônio, figura 7, consiste de um
corpo celular que tem várias protuberâncias ramificadas chamadas dendritos e um
único ramo denominado axônio. Quando os impulsos recebidos dos outros neurônios
através dos dendritos excedem um certo limiar, o neurônio dispara um impulso que é
propagado ao longo do axônio. As terminações do axônio formam, então, sinapses –
do grego “syn” (juntos) and “haptein” (prender) – com os dendritos de outros
neurônios. Estas sinapses podem ser inibitórias ou excitatórias, dependendo se elas
contribuem, respectivamente, para atenuar o sinal global ou para aumentá-lo.

Figura 7: Representação simplificada da estrutura morfológica de um neurônio (do autor).

      Esta descrição extremamente simplificada do funcionamento do neurônio é
capaz de capturar as características que são relevantes para a criação dos modelos
neurais de computação (Luger, 2004). As redes neurais artificiais (RNA) são modelos
que tem grande apelo para o desenvolvimento de sistemas inteligentes pois, ao
capturar o conhecimento através de uma rede distribuída de neurônios simulados, tem
maior potencial para reconhecer dados ruidosos e incompletos, assim como a mente
humana, capaz de distinguir e interpretar sinais ruidosos  como reconhecer um rosto
parcialmente encoberto.
      Este modelo inicialmente proposto por McCulloch e Pitts (1943), considera
que a capacidade de aprendizagem do cérebro está relacionada com a plasticidade,
isto é, a capacidade de gerar alterações nas ligações sinápticas entre neurônios. A
plasticidade pode ser atribuída a dois mecanismos: a criação de novas conexões
sinápticas entre neurônios e a modificação das sinapses existentes. Na representação
artificial conexionista, cada sinapse tem um peso associado positivo se a sinapse for
excitatória e negativo se for inibitória. O neurônio artificial é composto por um vetor
de valores de entrada (0 ou 1), um vetor de pesos sinápticos (um peso para cada
conexão), um somador e um limiar (do inglês threshold) e um valor de saída (0 ou 1).
      O neurônio opera numa escala de tempo discreta, ou seja, realiza os cálculos
em intervalos constantes de tempo (passos). A cada passo é calculada a soma dos
valores das entradas, multiplicados pelos respectivos pesos sinápticos. Se o valor
resultante for maior do que o limiar, o neurônio “ativa” a saída, representada pelo
valor 1. Caso contrário o neurônio segue “desativado” (saída assume o valor 0). Este
modelo pode ser estendido para trabalhar com valores contínuos em suas entradas, ao
invés de 0 ou 1 passam a aceitar valores entre 0 e 1.
      Do ponto de vista topológico uma rede neural pode ser vista como um grafo
orientado, onde os nós representam neurônios artificiais e o arcos representam as
sinapses entre eles. A orientação dos arcos indica o sentido do fluxo de informação
entre os neurônios de diferentes camadas. A figura 7 ilustra uma RNA cuja primeira
camada é a de entrada, responsável por distribuir os valores recebidos para os
neurônios da camada seguinte. A última camada é a de saída, podendo existir, como
neste caso, uma ou mais camadas intermediárias denominadas camadas escondidas.
Caso existam camadas escondidas a rede neural é denominada multicamadas. Ainda,
se existem arcos de todo neurônio de uma camada para todos os neurônios da camada
seguinte, e rede é denominada totalmente conectada. Caso contrário, a rede é
considerada como parcialmente conectada.
      No que diz respeito ao fluxo de informações dentro da RNA diz-se que uma
rede é alimentada à frente (feedforward) quando os arcos são unidirecionais e o grafo
é acíclico. Por outro lado, denomina-se uma rede como recorrente quando há algum
tipo de retro-propagação através de arcos bidirecionais, por exemplo.

Figura 8: Topologia de uma Rede Neural Artificial totalmente conectada alimentada à frente (do
autor).
      Embora autores como Marvin Minsky (1988) apontem como desvantagem das
RNAs o fato de serem sistemas fechados ou “caixas pretas”, limitando a compreensão
do funcionamento interno e minimizando as interferências exteriores no ajuste dos
pesos e aprendizagem da rede, a IA conexionista possui um vasto campo de aplicação
tendo obtido sucesso em vários domínios de difícil abordagem através de outras
técnicas, sobretudo quando o domínio é mutável, como no reconhecimento de
imagens faciais de pessoas em função do envelhecimento (MINKSY, 1988). No
entanto esta “desvantagem” pode ser entendida como uma característica autopoiética
deste sistema, dotando-o de uma identidade (MATURANA e VARELA, 1997).
      Um bom exemplo de trabalho artístico é a instalação La Funambule Virtuelle,
figura 9, de Marie-Hélène Tramus e Michel Bret (2000-2007) onde uma equilibrista
virtual evolui para manter-se sobre uma corda bamba, reagindo aos movimentos do
interagente. A personagem tenta reproduzir a postura do participante ao passo que
tenta manter-se sobre a corda. Nesta instalação, através de uma RNA, a equilibrista é
capaz de aprender a manter-se sobre a corda durante a interação. A partir deste gestual
aprendido, um novo comportamento emerge através de movimentos que não foram
ensinados, dotando a personagem do que a artista chama de capacidade de
improvisação (TRAMUS e CHEN, 2005).

Figura 9: La Funambule Virtuelle, de Marie-Hélène Tramus e Michel Bret (2000-2007)
      Aplicações musicais de RNA incluem ainda a análise rítmica, percepção do
pitch (tom), planejamento de performance, simulações de tonalidade e polifonia.
Mark Dolson sugeriu que a análise de timbre e de síntese também seria aplicável
nesta abordagem (DOLSON, 1989). James Todd, também em 1989, propôs uma
abordagem conexionista para a composição melódica na qual uma rede sequencial
com resposta foi treinada por um rol de melodias e a interpolação/extrapolação
melódica pode ser gerada inserindo-se na rede estruturas melódicas que são diferentes
das do treinamento (TODD, 1989).
      Outra característica importante nos modelos de RNA é o seu caráter
multicamada. Um modelo típico pode ter uma camada de entrada (input layer), uma
camada oculta (hidden layer) e uma camada de saída (output layer) (ROADS, 1996).
Um simples exemplo de sequência de treinamento para composição melódica seria
um rol de melodias desejáveis e indesejáveis associado ao julgamento de um
“treinador” que as caracterizaria com 0 ou 1.
1.4.3.	Sistemas Multiagentes (SMA)
      Os sistemas multiagentes (SMA) constituem uma subárea relativamente nova
da IA cujos esforços se encontram no estudo de agentes autônomos em um ambiente
multiagentes. No entanto, enquanto a IA se concentra no estudo dos componentes da
inteligência, como visão, cognição, aprendizagem, planejamento e raciocínio, a SMA
se debruça sobre as entidades que integram estes componentes e como estas entidades
interagem socialmente. Para a SMA, a autonomia desempenha uma papel
importantíssimo pois dota o agente da capacidade de definir por si só quais ações
deve executar além de comunicar-se outros agentes a fim de realizar ações de
cooperação, competição, coordenação ou negociação (REIS, 2003).
      Um dos pontos essenciais para permitir a criação de uma sociedade de agentes
está na capacidade de coordenação de uma forma geral. Uma vez que os agentes são
autônomos, a coordenação passa a ser realiza de forma distribuída entre os próprios
agentes que tornam-se responsáveis por mediar suas próprias interações. Se
considerarmos que um SMA pode conter um conjunto heterogêneo de agentes
podemos supor que torna-se necessária a criação de algum tipo de protocolo de
comunicação e negociação para mediar as interações. Neste sentido, toda uma
infraestrutura multiagentes é necessária para que um agente possa efetivamente operar
como sendo parte do sistema, figura 10.
      Em um SMA, cada agente tem sua própria capacidade de percepção e ação no
ambiente, mantendo assim uma esfera de influencia própria, independente do modelo
de agente inteligente implementado. Ou seja, o agente torna-se capaz de influenciar
em uma determinada parte do seu ambiente. Estas esferas de influencia podem
convergir de acordo com as relações estabelecidas entre os agentes, fazendo emergir
um relacionamento de dependência entre os agentes (WOOLDRIDGE, 2009).

Figura 10: Estrutura típica de um sistema multiagentes (WOOLDRIDGE, 2009).
      	Agentes são também podem ser conectados por vários tipos de
relacionamentos como de poder, onde um atua como chefe do outro. A fim de
estabelecer relações entre os agentes se faz necessário um determinado que um
protocolo de comunicação seja explicitamente determinado a fim de normatizar as
requisições entre os agentes. A ajuda a regular a organização do sistema, negociação
de tarefas e recursos entre agentes além da coordenação de esforços (WOOLDRIDGE
e JENNINGS, 1995).
      A comunicação entre os agentes pode ser realizada de diferentes maneiras,
utilizando diferentes protocolos. Quando os agentes se comunicam diretamente uns
com os outros diz-se que a comunicação é direta. Um agente utiliza algum protocolo
de comunicação para fazer enviar uma mensagem à outro. Neste caso o agente deve
ser capaz não apenas de formular suas próprias mensagens como interpretar e
deliberar a respeito das mensagens recebidas. Knowledge Sharing Effort (KSE) foi
formado, com o intuito de discutir protocolos em desenvolvimento para a troca de
conhecimento representado entre sistemas de informação autônomos.
      O KSE gerou dois principais pacotes, sendo o primeiro a KQML (Knowledge
Query and Manipulation Language) (FININ, FRITZSON, et al., 1994). KQML é uma
linguagem para comunicação entre agentes e define um formato de comum para
mensagens. Cada mensagem contém uma performativa (que pode ser vista como o
tipo da mensagem ou o tipo de requisição) e um determinado numero de parâmetros
associados à valores (que podem ser interpretados como o conteúdo da mensagem).
      O conjunto de performativas associadas ao KQML podem ser agrupadas em
três grupos: (a) as performativas de discurso são utilizadas no contexto de troca de
informação e conhecimento entre dois agentes, por exemplo tell, ask-if, ask-one, ask-
all, etc.; (b) intervenção e mecânica de conversação são utilizadas para intervir em
durante o curso normal de uma conversação, por exemplo,  error, sorry, discard, etc.;
(c) performativas de rede e facilidades permitem que os agentes encontrem outros
agentes e possam ajuda-los a lidar com suas próprias questões, por exemplo, register,
unregister, forward, etc.
   Em 1995, a Foundation for Intelligent Physical Agents (FIPA) começou seu
trabalho em padrões em desenvolvimento para sistemas de agente. O centro de mesa
desta iniciativa era o desenvolvimento de uma ACL (Agent Communication
Language) ou Linguagem de Comunicação de Agentes. A ACL é superficialmente
semelhante a KQML pois define um formato para mensagens, através de
performativas e parâmetros (WOOLDRIDGE, 2009).
   No entanto, o uso de um padrão de comunicação entre agentes não é pré-requisito
para um sistema multiagentes. Os esforços neste sentido são para que no futuro seja
possível interligar diferentes sistemas multiagentes formando um sistema mais
complexo com uma maior rede de interação.
   Um ótimo exemplo de sistema multiagentes que utilizam um sistema de
comunicação próprio é a obra Morfogênese, figura 11, desenvolvida pelo Tiago
Barros (SILVA, 2013). Neste sistema são utilizados agentes evolutivos geométricos
são capazes de interagir, demonstrando uma infraestrutura semelhante ao descrito por
Wooldridge (2009).

Figura 11: Representação visual em preto e branco dos agentes que compõem o sistema
multiagentes (SILVA, 2013).
      No sistema multiagentes Morfogênese o autor destaca que os agentes são
autônomos, possuindo comportamentos programados que determinam seus arranjos
emergentes através de representações visuais, cinéticas e sonoras. Nesta obra, a
interação entre os indivíduos desempenha um papel fundamental para a concepção
poética. A comunicação entre os indivíduos se dá através da detecção de colisão, onde
cada individuo é capaz de perceber um outro além de possivelmente estabelecer um
relacionamento de competição, predatismo, parasitismo ou colaboração. Desta forma,
emerge da interação como um todo um comportamento inteligente de clusterização
baseada na afinidade entre os agentes.
      Outro trabalho artístico que aborda claramente a relação multiagentes é a obra
#OpenEnvironment , onde uma comunidade virtual de indivíduos, baseado no
comportamento natural biológico, interagem entre si. Estes indivíduos precisam de
energia para sobreviver, mas a energia deles é a informação. O fluxo de informação
gerado cotidianamente pelas pessoas pode ajudar a mantê-los equilibrados. Em outras
palavras, OpenEnvironment é uma obra de arte que aponta algumas questões
emergentes relacionadas à forma como o fluxo de informação nas cidades é capaz de
promover grandes mudanças socioambientais, transformando e redefinindo as
relações existentes.

Figura 12: Interface de interação local do publico com #openEnvironment (Barretto, 2013).
      A interação com o público se deu através das mensagens do Twitter com a
hashtag #OpenEnvironment, figura 12. As mensagens capturadas pelo sistema eram
então exibidas em um painel de LEDs, localizado na sede do SESI na avenida Paulista
(figura 13). Os indivíduos representados visualmente por pixels, tinham como
objetivo alimentar-se as mensagens projetadas na tela. Neste sentido, cada individuo
era livre para tomar suas próprias decisões e organizar-se em grupos caso fosse do seu
interesse.
      Cada um dos indivíduos, ao ser criado, tinha associado a si uma expectativa de
vida, que podia ser estendida através da fagocitose das mensagens. Portanto cada um
dos agentes poderia escolher entre basicamente três ações: permanecer parado,
mover-se, alimentar-se. De uma forma geral, pode-se perceber algumas aglomerações
emergentes de agentes movendo-se de forma conjunta. Credita-se que este
comportamento ao fato de que cada um dos agentes, dentro da sua esfera de
influência, pode perceber outros agentes e sua movimentação e ser influenciado por
esta movimentação.

Figura 13: Projeção de mensagem enviada pelo twitter para interação com o ecossistema virtual.
      Outro fator importante da obra é que caso os indivíduos não se alimentassem,
por decisão própria ou por falta de informação disponível para ser fagocitada, eles
tenderiam a perecer. Ao remover um individuo cuja expectativa de vida foi alcançada
ou acelerada devido à inanição um novo individuo aleatório era inserido em seu lugar.
Portanto, não havia a utilização de operadores genéticos. Mesmo assim, foi possível
visualizar um determinado comportamento emergente a partir da interação entre os
agentes, que se comunicavam de forma indireta e subjetiva através da visualização da
sua esfera de influência.
1.5.	Representação de Conhecimento, Cognição e Aprendizagem
      O comportamento inteligente do agente pressupõe aquisição, armazenamento
e processamento de conhecimento. Para que isto seja possível, é fundamental que este
conhecimento seja representado. Segundo Elaine Rich e Kevin Knight (1993), este
conhecimento deve ser representado de tal forma que: (a) capture generalizações,
identificando e agrupando as propriedades relevantes; (b) seja compreensível para as
pessoas que o fornecem; (c) seja facilmente modificável para permitir a correção de
erros, refletir as mudanças do mundo e da visão do mundo que o usuário ou agente
possuem; (d) possa ser utilizado em diversas situações mesmo sendo incompleto ou
impreciso; (e) ajude a superar o seu próprio volume de dados, auxiliando a limitar as
diversas possibilidades que devem ser consideradas (RICH e KNIGHT, 1993).
      Os estudos realizados nesta área baseiam-se em parte na hipótese de
representação de conhecimento de Brian Smith (1985), onde qualquer processo
inteligente a ser realizado por uma máquina deve conter uma formalização que
permita uma descrição proporcional do conhecimento exibido pelo processo e que
desempenhe um papel formal, causal e essencial na geração do comportamento que
manifesta tal conhecimento, independente de uma semântica. Podemos perceber isto
claramente quando tratamos da estruturação do conhecimento para Redes Neurais e
Algoritmos Genéticos, que não utilizam uma estruturação semântica muito embora
dependam de uma representação de mundo.
1.5.1.	Abordagem Simbólica.
      Para a abordagem lógica, pregada pela GOFAI, a inteligência requer
conhecimento declarativo e alguma forma de mecanismo de raciocínio, denominado
cognição, além de permitir que o conhecimento e o raciocínio possam ser estudados
abstraindo os detalhes de percepção e controle motor. A evolução dos estados da
cognição e linguagem permitem descrever a evolução destes, criados durante o
processo de cognição, através do uso de vocabulário similar à linguagem natural ou
linguagem lógico-matemática. Esta evolução dos estados da cognição permite que se
desenvolva uma forma de aprendizagem independente dos estados cognitivos,
configurando aprendizagem.
      Baseado nesses preceitos, o comportamento inteligente pode ser descrito em
termos do conhecimento que demonstram ter ao executarem uma certa tarefa que
demanda inteligência. Logo, os programas inteligentes devem ser capazes de modelar
conhecimento e manipulá-lo. Neste sentido, a representação de conhecimento, que
será manipulada através de um mecanismo de raciocínio, diz respeito à um
formalismo escolhido para descrever um conhecimento do mundo real.
      No entanto o conhecimento humano é volumoso, difícil de caracterizar,
altamente mutável e normalmente se organiza dependendo da forma como será
utilizado. As técnicas de IA exploram o conhecimento de forma que ele seja capaz de
capturar generalizações através da valorização das propriedades importantes,
permitindo modificação, correção e compreensão além de tentar limitar as várias
possibilidades que, geralmente, seriam analisadas.
      Charniak e McDermott (CHARNIAK e MCDERMOTT, 1985) corroboram a
afirmação de Barr e Feigenbaum (BARR e FEIGENBAUM, 1981), no que tange a
produção de comportamento inteligente, ao afirmar que Inteligência Artificial é o
ramo da Ciência da Computação que trata do estudo das faculdades mentais através
da modelagem computacional. Estes modelos apresentam características associadas à
inteligência no comportamento humano, como linguagem, raciocínio, etc.
      É assaz difícil encontrar um domínio real que possa ser estruturado de maneira
precisa em sua totalidade. Para realizar algum raciocínio sobre qualquer domínio é
necessário a construção de um modelo que passa a ser uma representação simplificada
do conhecimento e, certamente, deixará alguns fatos desconhecidos seja por
ignorância teórica ou prática. Por isto, normalmente, os sistemas inteligentes devem
lidar com a falta de informação em algum momento e associar medidas de certeza aos
fatos (crenças), gerando conhecimento incerto.
      O conhecimento incerto pode ser parcial ou aproximado. O conhecimento é
parcial quando algumas respostas a questões relevantes não são conhecidas enquanto
o conhecimento é aproximado quando as respostas fornecidas não são precisas ou
exatas em decorrência da baixa confiabilidade da fonte ou sofrem de imprecisão
inerente à linguagem representativa.
      A representação simbólica de conhecimento assume uma relação semântica e
sintática, onde várias unidades (átomos) interconectados são, coletivamente,
responsáveis por representar vários conceitos. Um conceito, representado num senso
distribuído, é indicado por um envolvimento em atividades sobre esta coleção de
unidades, podendo caracterizar um sistema fechado. Estes sistemas fechados,
conforme veremos adiante na seção 2, vão de encontro ao que Maturana e Varela
chamam de autopoiesis.
      Neste sentido, existem diversos paradigmas para representação de
conhecimento que emergem destas perspectivas. O conhecimento procedural, por
exemplo, representa o conhecimento na forma de funções e procedimentos. As redes
semânticas, por sua vez, caracterizam-se por grafos direcionados, cujos nós
representam conceitos e/ou entidades e as conexões entre nós (arcos) representam a
relação entre estes conceitos. O modelo lógico é estruturado de forma a declarar o
conhecimento na forma de proposições e pode considerar outras dimensões como o
tempo, espaço, crença, probabilidade e incerteza.
      Para a implementação completa de um agente inteligente, além da
representação do conhecimento, se faz necessária a presença de algum mecanismo de
inferências capaz de identificar o estado atual do agente através do mapeamento das
suas percepções para conhecimento representados. Este estado atual, por sua vez,
servirá como input para a tomada de decisão do agente sobre qual deverá ser a ação
executada. Este mecanismo cognitivo de raciocínio é expresso de acordo com a
representação do conhecimento e pode ser dividido basicamente em dois grandes
grupos: o raciocínio monotônico e o não-monotônico.
      O raciocínio monotônico baseia-se em conhecimento organizado através da
Lógica de Primeira Ordem (LPO) onde o domínio do problema é bem conhecido,
completo e imutável (mundo fechado). Todos os fatos que estão registrados na base
de conhecimento do sistema são verdadeiros, sem nenhuma exceção. Não há incerteza
associada para qualquer elemento do domínio e a adição de novos fatos não deve
invalidar nenhum fato antigo e, portanto, pode ser derivado diretamente dos fatos já
existentes. Assim sendo, uma vantagem deste tipo de raciocínio é que ao acrescentar
um novo fato, não há necessidade de validação ou verificação de inconsistências
considerando a base “antiga”. Além disso, ao acrescentar um novo fato à base não é
necessário armazenar a lista de fatos que corroboram o novo fato posto que a base
antiga se mantém e apenas acrescenta-se o novo fato.
      No entanto, em alguma situações do mundo real envolvendo parâmetros
subjetivos, por exemplo, torna-se muito difícil formalizar o conhecimento de forma
completa, consistente e imutável. Mesmo quando isto é possível, nem sempre os
novos fatos adicionados à base são não-conflitantes com o restante dos fatos. Um bom
exemplo de aplicação do raciocínio monotônico é o mundo do Wumpus (figura 14),
onde o domínio é todo conhecido e facilmente modelável para a IA (figura 15).


Figura 14: Implementação do mundo do Wumpus como jogo para atari (YOB, 1976).

Figura 15: Modelagem do jogo do Wumpus para agentes inteligentes (RUSSEL e NORVIG,
2003).
      O Mundo do Wumpus foi proposto em 1972 por Gregory Yob que cansado
dos jogos da época que eram baseados em grades cartesianas decidiu então criar um
novo tipo de jogo para computador (YOB, 1976). Este mundo é caracterizado por um
labirinto repleto de abismos, habitado por um terrível monstro, Wumpus. Além disso,
o mundo também esconde perigosas armadilhas. Manter-se vivo é a principal tarefa
para se concluir o objetivo. Porém, isso não é muito fácil, no interior da caverna,
deve-se ficar muito atento às indicações de perigo uma vez que o agente é dotado de
percepções, como por exemplo, ele é capaz de sentir a brisa que sai dos abismos
espalhados pela caverna Também é possível sentir o mal-cheiro exalado pelo terrível
Wumpus. Após essa perigosa busca, deve-se encontrar a o ouro para que jogo
termine.
      O raciocínio não-monotônico pode ser visto como uma extensão da LPO onde
a cada novo fato acrescentado à base é feita uma verificação de consistência pois
neste caso, trabalha-se com um domínio que pode ser mutável ou incompleto. Neste
tipo de raciocínio são admitidas inferências realizadas na ausência de informações
contrárias e que podem ser validadas ou invalidadas por novas informações através de
um mecanismo de revisão de crenças. Este tipo de raciocínio não-monotônico é
denominado raciocínio de exceções.
      Um exemplo clássico do raciocínio não-monotônico é que podemos afirmar
que os pássaros voam, se dissermos que tweety é um pássaro, o sistema, na ausência
de informação contrária infere que tweety também voa. Este fato é então acrescido à
base e é considerado como verdadeiro até que informemos ao sistema que tweety é
um pingüim (e pingüins não voam). Neste caso, então, é necessário rever a afirmação
de que tweety voa através do mecanismo de revisão de crenças, invalidando-o (ou
tornando falso).
      Há ainda o raciocínio minimalista, onde parte-se do princípio de que se algo é
verdadeiro e relevante, está na base de conhecimento, o raciocínio de abdução que
utiliza um mecanismo de inferência permitindo raciocinar a partir dos efeitos para
uma possível causa, entre outros.
      O raciocínio analógico baseia-se na determinação de semelhanças e relações
entre dois domínios objetivando aplicar o conhecimento adquirido num domínio
específico em um outro domínio semelhante. Este tipo de raciocínio pressupõe que
existe conhecimento prévio, fornecido por um especialista, acerca dos problemas e
respectivas soluções em domínios diversos. Uma utilização prática do raciocínio
analógico se dá com relativo sucesso no teste de prova automatizado de teoremas
baseando-se em outras demonstrações já conhecidas.
      Para aplicação de uma solução pré-existente à um novo caso, deve existir no
sistema uma forma de mensurar a eficácia/eficiência das soluções existentes,
classificando-as de acordo com o grau de similaridade entre o problema conhecido e o
problema a ser solucionado, possibilitando assim a escolha da melhor solução
possível. Esta abordagem não garante uma solução ótima do problema, o que pode ser
visto como uma desvantagem. Além disso, nem todo domínio de problemas e
soluções é facilmente modelável de forma à permitir este tipo de comparação
sobretudo se considerarmos que os parâmetros levados em consideração ao realizar a
comparação de similaridade entre dois problemas serão decisivos para o sucesso do
sistema, sendo a recíproca verdadeira.
      O raciocínio plausível utiliza uma abordagem numérica para tratamento de
incerteza. Nesta abordagem um determinado fato é caracterizado dentro de um
intervalo numérico que determina níveis de evidência e crença (ou crença e
plausibilidade) associados à uma incerteza resumida. Assim, ao invés de inferir um
nível pontual de certeza (verdadeiro ou falso, por exemplo), utiliza-se uma medida
numérica para cada proposição. A adição de novos fatos demanda uma combinação e
propagação da incerteza à ele associada como forma de rever os níveis de crença e
plausibilidade dos outros fatos contidos na base.
1.5.2.	Abordagem Conexionista
      Para Luger (2004), a maioria das técnicas encontradas na literatura representa
o conhecimento explicitamente através de abstrações e algoritmos de busca,
concebidos para implementar um comportamento inteligente. Estas técnicas são
normalmente associadas à GOFAI. No entanto, numa abordagem diferente pode-se
construir programas inteligentes utilizando modelos matemáticos que simulam
sistemas biológicos: neuronais ou evolutivos.
      Pode-se afirmar que a abordagem que raciocina baseando-se na formalização
de conhecimento declarativo não é a única capaz de produzir resultados inteligentes.
As abordagens alternativas à GOFAI são interessantes pois trazem outros modelos de
representação de conhecimento não-explícito, como as Redes Neurais Artificiais
(RNA) e Algoritmos Genéticos (AG), para a emergência do comportamento
inteligente, sendo portanto mais importantes neste estudo.
      Vale à pena ressaltar que muito embora o conhecimento não-explícito seja
utilizado, abstraindo a necessidade da estruturação lógica, sintática ou semântica do
conhecimento, ele também necessita ser estruturado. Nas RNAs, por exemplo, esta
estruturação se manifesta na topologia escolhida para a rede assim como na conexão
entre os neurônios.
      A modelagem de comportamento dos neurônios, por volta de 1997, surgiu
através de considerações neurovasculares, onde, por um longo período, foi estudada a
ação muscular e a condução de estímulos nervosos conjuntamente. Foram propostos
também vários modelos hidráulicos, mecânicos, ópticos, elétricos ou eletroquímicos,
até que em 1938, Rashevsky (RASHEVSKY, 1945) propôs o primeiro modelo
matemático que utilizava equações diferenciadas para descrever o comportamento
neuronal.
      Os sistemas conexionistas, como as Redes Neurais Artificiais ou RNA, se
baseiam no funcionamento neuronal onde o neurônio simulado passa a ser a unidade
computacional básica do sistema em questão. Segundo Lippman (1997),
(LIPPMANN, 1997) as RNAs são sistemas físicos que podem adquirir, armazenar e
utilizar conhecimentos experimentais, alcançando uma boa performance devido à sua
densa interconexão entre nós da rede.
      O conhecimento em uma rede neural é representado pelos pesos sinápticos
associados à cada conexão e o mútuo reforço ou inibição dos estímulos a um
determinado neurônio pelos outros neurônios. A não linearidade é essencial para dotar
a rede com “poder de decisão”, ou seja, mudanças quantitativas na entrada podem
assim produzir mudanças qualitativas na saída ao invés de simplesmente alterar a
saída proporcionalmente à entrada (ROADS, 1996).
      Neste caso, o conhecimento é representado através de strings de conexão entre
elementos e do reforço ou inibição mútua dos elementos através dos outros elementos.
Isso implica que não há a necessidade de um formalismo ou estruturação completa
acerca do domínio pois o sistema desenvolve um comportamento inteligente através
da experiência que adquire manipulando o domínio em questão.
      A aprendizagem com as RNAs consiste na adaptação dos pesos sinápticos sob
dois métodos: aprendizagem supervisionada (com um tutor) ou não supervisionada
(autônoma). Na aprendizagem supervisionada, um conjunto de casos de histórico,
juntamente com as respectivas respostas, é submetido à rede e a saída gerada é
comparada à saída esperada para cada caso. A regra de aprendizagem é utilizada para
ajustar os valores dos pesos sinápticos para que as respostas dadas se aproximem das
respostas corretas. Este método pressupõe um rol de casos e soluções conhecidas, o
que o torna menos interessante para o desenvolvimento de resultados inesperados
posto que a rede estará treinada para reagir aos estímulos de forma previamente
conhecida.
      Por sua vez, a aprendizagem não supervisionada permite que a rede ajuste os
seus pesos sinápticos sem a existência de casos e saídas pré-conhecidas. O objetivo,
aqui, é observar regularidades nos dados da entrada. Assim, criam-se as condições
para que se possa realizar uma inferência sobre a acuidade da representação do
conhecimento que a rede deve aprender, sendo os pesos ajustados para atender a esta
medida.
      O projeto Geopartitura, figura 16, projeto foi desenvolvido num processo
transdisciplinar, unindo sociologia, arquitetura, engenharia acústica, computação
ubíqua, design, inteligência artificial e arte musical (VENTURELLI, BARRETTO, et
al., 2003). Além disso, este projeto aponta para questões emergentes envolvendo a
música, a geografia e dispositivos móveis, como celulares, para permitir a criação
coletiva georreferenciada de um sistema multimídia em tempo real. A palavra
geopartitura tem origem na junção das palavras geografia e partitura. A Geografia é a
ciência que estuda o espaço, ou seja, busca o significado dos lugares, sendo assim
contribui significativamente com a sociedade, na reorganização de seus espaços e de
suas formas de interação com o ambiente. Partitura significa uma representação
escrita de música padronizada universalmente.
      O sistema geopartitura leva em consideração a filosofia da música
eletroacústica, no que concerne a ideia da paisagem sonora, já discutida
anteriormente, pois envolve na paisagem a ampliação do conceito pela inclusão de
outros sentidos como a visão, na construção coletiva e interativa do concerto. Este
concerto aplica tecnologias cibernéticas como instrumento de diálogo com as pessoas
que estão adensadas num espaço urbano por meio de seus celulares, de maneira
crítica, pois utiliza o dispositivo móvel para expressão estética.

Figura 16: Geopartitura em exposição durante o 10º Encontro Internacional de Arte e
Tecnologia, Brasília. (2011)
      O geoposicionamento de cada celular permitie ao sistema conectar cada
aparelho aos demais, dentro de um raio de "descoberta", criando para cada conexão
estabelecida uma corda virtual que vibra e soa de acordo com a distância entre os
pontos, criando assim uma rede de conexões entre os usuários que utiliza um modelo
semelhante ao das redes neurais artificiais, figura 17.
      Por meio do sistema, o interagente visualiza em forma de projeção no espaço
urbano uma cartografia que surge a partir das conexões de todos os indivíduos
detectados pelo sistema em tempo real. Estas cordas virtuais entre os interagentes,
geradas pelo sistema, produzem sons quando tocadas. Poeticamente, geopartitura está
relacionado com a existência de um ritmo no universo do conhecimento que conduz a
música e a imagem em suas diferentes formas de manifestação.
      Ao conectar os dispositivos/usuários/agentes geramos uma topologia onde
cada nó da rede age como um neurônio artificial. O estímulo gerado pelo interagente
ao tocar as cordas virtuais que aparecem na tela do dispositivo móvel é analisado por
uma função de ativação f(i) que possui um limiar pré-definido. Se o estímulo
ultrapassa o limiar estabelecido, uma mensagem é então enviada aos outros
dispositivos que estão conectados ao que foi estimulado, como em uma sinapse
neuronal biológica.


Figura 17: Topografia das conexões entre usuários no sistema Geopartitura (do autor).
      Esta rede de conexões e sinapses resulta em paisagens sonoras geradas como
concerto e são a base para a composição de uma cartografia subjetiva que mapeia por
meio de sons, arte e tecnologia o nomadismo humano físico, cultural, econômico e
social. São detectados os pontos-chave da cartografia sonora de cada indivíduo ou
transeunte, ou seja, forças de interferência na malha sonora projetada no contexto
urbano, resultado de uma deriva pela cidade, deixando o acaso e a interação
trabalharem.
      A aprendizagem no sistema geopartitura se faz através do ajuste dos pesos
sinápticos associados à cada conexão. Quanto mais estímulos uma determinada
conexão recebe, maior torna-se o seu limiar, ou seja, menos sensível ela será.
Podemos considerar, de um ponto de vista poético, que o conhecimento representado
no sistema relaciona imagem e som, no ritmo do universo do conhecimento que
conduz a dança de suas diferentes formas de manifestação. Alguns autores,
como  Fritjof Capra, em “O Tao da Física” (1991), e Gary Zukav, em “A Dança dos
Mestres Wu Li” (1979), já se dedicaram a observar a existência desse ritmo no
universo do conhecimento. Gary Zukav explora essa sintonia entre os novos caminhos
da física e o misticismo oriental. O caminho sugerido é duplo: a ciência busca teorias
tão amplas sobre o universo que acaba por tangenciar a sabedoria do oriente, onde
ciência, religião, arte e filosofia não são entidades que se pode distinguir com clareza.
      Percebe-se, então, que não há um consenso quanto à definição formal de
Inteligência Artificial. Pode-se concluir que, apesar de correta considerando as
abordagens da GOFAI, a afirmação de Barr e Feigenbaum, não se aplica,
necessariamente, às abordagens da IA “moderna” como as Redes Neurais, por
exemplo. Neste caso, seria mais prudente adotar uma definição mais ampla como a de
Inteligência Artificial como uma espécie de automação de comportamento inteligente,
segundo Luger e Stubblefield (LUGER, 2004).
1.5.3.	Abordagem Social e Emergente
      Assim como a abordagem conexionista, que teve sua inspiração inicial
apoiada no modelo neuronal biológico a fim de gerar um modelo computacional
equivalente, uma série de outras analogias biológicas também acabou influenciando a
concepção de novos modelos de representação, cognição e aprendizagem de máquina.
Estes novos modelos inspiram-se principalmente nos processos subjacentes à
evolução através da adequação de uma população de indivíduos através da
sobrevivência dos seus membros mais ajustados. O poder de seleção dos indivíduos
através deste processo se demonstra através da própria emergência de espécies na
evolução natural, assim como os processos sociais subjacentes às mudanças sociais
(LUGER, 2004).
      Os modelos emergentes de representação de conhecimento funcionam através
do processo de introdução de variações em sucessivas gerações ao passo que se
eliminam os indivíduos menos ajustados. Assim, emerge, uma população cuja
adaptação exibe uma crescente capacidade e diversidade. Evolução e emergência
acontecem em populações de indivíduos corporificados, cujos princípios são
discutidos no item 2.1. Estes indivíduos desempenham ações que afetam outros
indivíduos e, por sua vez, são afetados por outros indivíduos. Assim, as pressões
seletivas não são unicamente oriundas do ambiente no qual os indivíduos estão
inseridos mas também da interação com outros membros da população, configurando
assim uma ecossistema mais complexo.
      Conforme pudemos ver no item 1.5.2, a representação do conhecimento se dá
através do genótipo, formado por um conjunto de alelos. Este modelo, assim como na
evolução biológica, produz conhecimento operando sobre unidades de informação
que são transmitidas e adaptadas socialmente. A corporificação do individuo,
usualmente referida como seu fenótipo, descreve o seu comportamento e
características físicas do individuo baseadas na codificação do genótipo.
      Neste sentido, quando utilizamos um algoritmo genético para solucionar um
problema genérico temos três estágios distintos: primeiro, os indivíduos (potenciais
soluções) são representados de tal forma que sejam capazes de suportar variações
evolutivas através de operadores de seleção. No segundo estágio, são definidos alguns
mecanismos de acasalamento e mutação, análogos às atividades sexuais e mutantes de
formas biológicas. Estes mecanismos são capazes de produzir uma nova geração de
indivíduos através da recombinação de características de seus pais e da introdução de
uma variação genética. Finalmente, uma função de aptidão julga quais indivíduos
mais adaptados serão favorecidos através de uma maior probabilidade de propagação
de seus alelos.
	Os algoritmos genéticos abordam a aprendizagem como uma competição
numa população de possíveis soluções. Este aprendizado é guiado pela função de
avaliação que determina o grau de adaptação de cada individuo a fim de determinar
para cada um deles um índice (geralmente numérico). O procedimento pode ser
descrito no algoritmo adaptado de  Luger (2004) a seguir:
1.	Inicializar o tempo (t)
2.	Inicializar a população P(t);
3.	Avaliar a aptidão  de cada membro da população P(t);
4.	Selecionar probabilisticamente os membros da população P(t) com base na
aptidão;
5.	Produzir descendentes P’(t) utilizando operadores genéticos;
6.	Substitua probabilisticamente os membros de P(t) por P’(t) para formar
P(t+1);
7.	Ajuste o tempo t := t+1;
8.	Volte ao passo 3.
      Esta estrutura básica de algoritmo é capaz de descrever a aprendizagem
genética, porém são cabíveis diferentes implementações. Pode-se, por exemplo
especificar qual o percentual da população que deve permanecer para a próxima
geração além da frequência em que os operadores genéticos são aplicados.
      Abordagens mais sofisticadas podem ordenar uma população pela sua aptidão
e utilizar este fator para determinar uma medida de probabilidade de eliminação onde
esta seria uma função inversa da sua aptidão. Este tipo de abordagem torna-se
interessante pois, ao utilizar esta medida como um dos fatores para eliminar um
candidato, a probabilidade de eliminação de um individuo com alta avaliação de
aptidão é muito pequena mas não impossível. Assim, mesmo os indivíduos mais aptos
da sociedade podem ser removidos, fazendo com que alguns indivíduos cuja aptidão
global seja pobre sejam “salvos”. Neste sentido há uma enorme vantagem sobre a
abordagem genérica, pois os indivíduos de baixa aptidão podem conter algum
componente que possivelmente contribuirá para uma solução mais poderosa
futuramente, além de manter uma maior diversidade genética.
      Outro fator importantíssimo para a abordagem emergente é a escolha dos
operadores genéticos. Os operadores genéticos podem ser interpretados como
equivalentes à parte da maquina de inferência da abordagem simbólica pois são
responsáveis pela engrenagem que representaria o raciocínio, cognição e
aprendizagem nestes sistemas.
      Existem vários operadores genéticos capazes de produzir descendentes que
resgatam características de seus pais, como a transposição e inversão. Dentre eles, o
mais comum é o operador de recombinação (crossover). A recombinação parte de
dois indivíduos previamente selecionados (probabilisticamente ou não) e os divide,
trocando seus componentes internos para produzir dois novos candidatos.
Considerando dois indivíduos cujo genótipo está representado em um byte (8 bits), o
operador divide as cadeias no meio e forma dois filhos cujo segmento inicial se
origina  de um pai e cujo segmento final vem do outro pai, conforme ilustra a figura
18.


Figura 18: Aplicação do operador de recombinação entre dois indivíduos previamente
selecionados A e B para geração de dois filhos A e B (do autor).
      Vale ressaltar que a decisão de divisão das cadeias ao meio é arbitrária e
poderia se dar em qualquer ponto da representação, definido de forma aleatória ou
adaptado de acordo com a evolução  do processo.
      O operador de mutação também exerce um papel essencial na evolução da
população como um todo, além de garantir uma diversidade genética. A mutação
toma um único individuo e troca aleatoriamente alguns de seus aspectos. A mutação
pode, por exemplo, alterar o valor de um alelo de 0 para 1 e vice-versa. Este processo
é importante pois na população inicial gerada aleatoriamente podemos deixar algum
componente essencial para a solução, sobretudo se relação entre o tamanho do
genótipo e a quantidade de indivíduos é pequena, ou seja, quanto maior o genótipo e
menor o tamanho populacional, menor a quantidade de possíveis combinações
representadas.
      Considere que os indivíduos de uma população tem seu genótipo representado
por um byte e que na população inicial gerada aleatoriamente não há nenhum
individuo com um bit 1 na primeira posição. O operador de recombinação não seria
capaz de gerar qualquer descendente que possua esta característica uma vez que
nenhum dos possíveis pais a exibe. O operador de mutação não garante que haverá
uma mutação específica para este alelo mas torna possível a possibilidade de que isso
aconteça de forma emergente.
      De acordo com o algoritmo genérico citado anteriormente, toda a operação
genética de recombinação, por exemplo, se baseia na função de adaptação. Esta
função normalmente determina o grau de adaptação dos indivíduos através da
mensuração da distância atual entre o individuo e o objetivo: quanto mais próximo do
objetivo proposto, maior o grau de adaptação. Neste sentido, as funções retornam
valores numéricos fixos para as mesmas configurações de genótipos. Assim, dois
indivíduos que possuem os mesmos genes na mesma ordem em qualquer tempo de
execução do algoritmo.
      Uma vez que a aprendizagem em um sistema dessa natureza se dá através da
adaptação dos indivíduos e esta adaptação por sua vez é guiada por uma função fixa,
podemos pensar que há pouco ou nenhum espaço para o surgimento de características
emergentes. Isto é real se considerarmos apenas a ultima geração de indivíduos, que
tendem à convergir cada vez mais para um conjunto de possíveis soluções que
maximizam a resposta da função de adaptação.
       O ponto crítico desta abordagem padrão é justamente a linearidade evolutiva
no sentido de maximizar sempre a função de adaptação. Assim, a reponsabilidade do
designer ao desenvolver um sistema adaptativo é grande pois a função escolhida pode
determinar completamente o conjunto final de indivíduos assim como o curso
evolutivo. Interessante perceber que a utilização de funções de adaptação “duras”
geram, ainda assim, durante o curso evolutivo potenciais resultados emergentes
discutidos na próxima seção.
      Para explorar as possibilidades de aplicação deste método neste trabalho, foi
desenvolvido um sistema de computação evolutiva para a Geração de Acordes cuja
implementação será detalhada abaixo e cujos resultados serão analisados no item 2.4
      Para este sistema, foi levado com conta que a musica é um domínio
relativamente bem definido cuja notação é modelável computacionalmente. Há
intervalos bem definidos entre frequências para denotar as notas musicais chamadas
escalas. Por sua vez, um conjunto de notas distintas pode formar um acorde. Pode-se,
então, definir um acorde como sendo um conjunto de notas ou frequências distintas
que seguem um determinado padrão de intervalos.
      Há inúmeros padrões e regras que definem diversas categorias de acordes
como os maiores, menores, diminutos ou aumentados. Estes padrões são normalmente
representados através de intervalos entre as notas, expressos em semitons. Na escala
temperada, um semitom é a distância entre duas teclas subsequentes do piano. A nota
que serve como referência para as demais é denominada tônica. A partir desta nota,
pode-se construir vários diferentes acordes utilizando outras notas em intervalos
específicos,  formando uma tríade (3 notas) ou tétrade (4 notas), conforme a tabela 2.

Semitons
Intervalo
0
Uníssono
1
2ª Menor
2
2ª Maior
3
3ª Menor
4
3ª Maior
5
4ª Justa
6
5ª Diminuta ou 4ª Aumentada
7
5ª Maior
8
6ª Menor
9
6ª Maior
10
7ª Menor
11
7ª Maior
12
8º Justa
Tabela 2: Identificação e nomenclatura das notas definidas pelos intervalos em semitons a partir
de uma nota tônica qualquer.

      A formação de um acorde perfeito maior, considerado como objetivo neste
estudo, por exemplo, se dá através da tétrade: Tônica + 3ª Maior + 5ª Maior + 8ª
Justa.  Dada esta estruturação do problema, a formação de tétrades para a formação de
acordes implicou na modelagem de um cromossomo contendo 4 alelos. Cada alelo
representando uma das quatro notas que compõem uma tétrade, sendo o primeiro
alelo o da tônica, o segundo a 3ª Maior, o terceiro a 5ª Maior e o último representando
a 8ª Justa. O valor armazenado por cada alelo varia entre 0 e 12, representando todas
as notas possíveis dentro de uma oitava, conforme a tabela 3.

Posição
0
1
2
3
Conteúdo
Tônica (0-12)
3ª Maior (0-12)
5ª Maior (0-12)
8ª Justa (0-12)
Tabela 3: Codificação da informação necessária para a formação de acordes em 4 alelos.
      A função de fitness avalia os cromossomos considerando distância entre as
notas contidas nos alelos em relação às notas necessárias para formação de um acorde
maior, classificando os indivíduos com valores interpolados entre 0 e 100 (fitness
máximo), de um em um. Além disso , foi progressivamente acrescido um peso à cada
nota da tétrade, conforme a tabela 4 que denota a importância da formação das notas
na formação do acorde a partir de uma tônica.

Distância
3ª Maior
5ª Maior
8ª Justa
Peso
4
3
2
Tabela 4: Peso das distâncias para cada nota.

      Uma vez gerada randomicamente uma população inicial contendo 10
indivíduos foram executadas 100 rodadas utilizando dois operadores genéticos com
taxas de probabilidade diferentes sobre esta população: crossover de um ponto e
mutação. Para o operador de crossover foi aplicada uma taxa percentual 80%
enquanto ao operador de mutação foi utilizada uma taxa de 2%. Esta taxa simboliza o
percentual de indivíduos afetados pelos operadores a cada rodada.
      Ao serem criados novos indivíduos através da aplicação destes operadores foi
mantido o número de indivíduos constante através da técnica de substituição de
geração com elitismo. Nesta abordagem são gerados os todos os novos filhos
baseados na aplicação dos operadores e é mantido o número necessário de indivíduos
da geração anterior, ordenados de acordo com o seu grau de adaptação (medida
fitness).
      É importante perceber que no exemplo do Gerador de Acordes, cada indivíduo
pode ser entendido como uma possível solução para o sistema. A codificação do
cromossomo, enquanto representação não-explícita do conhecimento, e a escolha da
forma como os operadores genéticos são aplicados para permitir a aprendizagem
(evolução) desenvolvem um papel essencial na forma como este sistema irá evoluir.
As arquiteturas neurais e os algoritmos genéticos fornecem, portanto, um modelo
natural para o problema do processamento em paralelo, porque cada neurônio ou
indivíduo é uma unidade independente (LUGER, 2004). influência.
1.6.	Relevância da Inteligência Artificial e Suas Abordagens
      O comportamento inteligente modelado computacionalmente é primordial no
desenvolvimento desta pesquisa. Se considerarmos que o principal objetivo de um
sistema Artelligent é exibir um comportamento que possa ser compreendido como
criativo, devemos entender que a base, alicerce, que possibilita este tipo de
manifestação encontra-se exatamente nas técnicas de IA.
      Nesta seção foi realizado um recorte teórico e técnico da IA visando ressaltar
os três primeiros princípios que compõem um sistema considerado Artelligent: (a)
utilização de um agente ou conjunto de agentes inteligentes e seu ambiente de tarefa,
(b) a utilização de alguma técnica de IA para descrever o agente e (c) a sua respectiva
representação de conhecimento.
      As técnicas e exemplos de IA apresentados aqui, além de ilustrar a enorme
gama de aplicações possíveis, compõem a base teórica-técnica sobre a qual a pesquisa
se desenvolve. A utilização de agentes inteligentes ou sistemas de agentes inteligentes
em um determinado ambiente de tarefa pressupõe certamente a utilização destes
algoritmos de IA, além de considerar também a representação de conhecimento.
      Os ambientes de tarefa podem ser definidos através de diversas dimensões
como a discretização do tempo, a capacidade de observação parcial ou total do agente
sobre o ambiente, a discretização das ações do agente e a previsibilidade das
mudanças do ambiente. Neste sentido, compreender  e descrever o ambiente torna-se
uma tarefa tão importante quanto escolher corretamente o modelo de agente à ser
implementado. Considerando o framework que está sendo apresentado, propõe-se que
a identificação do ambiente seja considerada a fim de possibilitar a melhor escolha
quanto às definições do próprio agente.
      Uma vez definido o ambiente, devemos considerar durante o planejamento do
nosso modelo de agente (a) as técnicas de desenvolvimento de agente e (b) a
representação do conhecimento, cognição e aprendizagem. Muito embora os dois
pontos estejam fortemente interligados, podemos em um nível mais avançado propor
estruturas hibridas como sistemas multi-agentes de redes neurais ou algoritmos
genéticos de redes neurais, por exemplo. No entanto, o nível de complexidade
aumenta consideravelmente quando tratamos de estruturas hibridas, podendo a
complexidade organizacional gerada pela arquitetura confundir-se com os resultados
emergentes oriundos do(s) agente(s). Dentre as técnicas de IA, foram destacadas aqui
as abordagens alternativas à GOFAI e a representação lógica/simbólica: Algoritmos
Genéticos, Redes Neurais Artificiais e Sistemas Multiagentes.
      Por fim, o agente deve ter capacidade de compreender o ambiente onde esta
inserido e raciocinar sobre ele a fim de definir quais ações deve desempenhar. Neste
framework propomos que a representação do conhecimento, cognição e aprendizagem
devem ser consideradas sob três perspectivas distintas: simbólica, conexionista e
social/emergente. O raciocínio lógico ocupa um lugar predominante no campo da IA
considerada como GOFAI (Good Old-Fashined Artificial Intelligence) onde persiste a
abordagem simbólica, que muito embora não pareça a mais adequada para facilitar o
surgimento de resultados emergentes pode dotar o agente de algum nível de
criatividade (discutidos na seção 2) quando se introduz a capacidade de aprendizagem
e adaptação e/ou cria-se um sistema multiagentes modelados utilizando a abordagem
simbólica onde o resultado das interações entre os agentes é emergente.
      As abordagens conexionista e social proveem uma maior liberdade na
representação do conhecimento, uma vez que não estão necessariamente associadas à
abordagem lógica. Nestas duas abordagens a representação do conhecimento não é
necessariamente explicita reduzindo o papel do designer de agentes na sua construção
cognitiva. Na abordagem conexionista, o agente realiza a aprendizagem através da
interação com o ambiente não sendo obrigatório algum treinamento prévio. Por outro
lado, a abordagem social pode utilizar algum tipo de representação explicita que pode
estar distribuída entre os agentes e normalmente será modificada e adaptada através
da interação entre eles.



SEÇÃO II | Arte, Inteligência e Criatividade Computacionais
      Para um sistema Artelligent o ambiente tem papel fundamental na construção
cognitiva do agente. Neste sentido aqui são descritos alguns princípios de design de
agentes oriundos do campo de pesquisa da IA cognitiva e corporificada que
corroboram esta tese. Esta relação também está bem descrita nas abordagens sobre a
criatividade oriundas da psicologia. Além disso, vale à pena ressaltar que a pesquisa
realizada sobre a criatividade nos fornece poderosos frameworks para compreender,
categorizar e classificar os diferentes níveis e manifestações da criatividade.
      Estas abordagens da criatividade são de suma importância, ainda que em
alguns pontos sejam contrastantes entre si, pois fornecem uma base sobre a qual
podemos analisar a criatividade computacional, componente dos sistemas
Artelligentes. Tanto as pesquisas do campo da IA quanto da psicologia, no que tange
a questão da criatividade, apontam para um comportamento eminentemente
autopoiético e emergente. Estes dois conceitos também são abordados no que
concerne o comportamento criativo emergindo tanto do resultado de mecanismos
cognitivos internos quanto da interação social entre indivíduos.
2.1.	Princípios para Design de Agentes
      Há algum esforço dentro do campo da IA, mais especificamente na área da IA
cognitiva, para tornar os princípios de design de agentes mais explícitos. A discussão
sobre estes princípios inicialmente propostos por Rolf Pfeifer na década de 1990 tem
sido trabalhadas por diversos outros autores (eg. PFEIFER, 1996; PFEIFER, IIDA e
BONGARD, 2005; PFEIFER e GOMEZ, 2005; PFEIFER e BONGARD, 2007),
culminando em uma aprofundada análise em Froese e Ziemke (2009).
Apresentaremos, portanto, alguns destes princípios, tabela 5, que servirão não apenas
como motivação e embasamento mas também como fundamentos norteadores na
tentativa de delinear as intersecções entre os conceitos apresentados nesta pesquisa.


Tabela 5: Sumário dos princípios de design da IA corporificada (adaptado de Pfeifer, Iida &
Bongard, 2005).
      Os princípios estão divididos em duas subcategorias: (i) “design procedure
principles” ou princípios de design dos procedimentos, identificados como (P-X) e
(ii) “agent design principles” ou princípios de design do agente, identificado como
(A-X). Enquanto o primeiro grupo trata da filosofia geral ligada à abordagem
escolhida, o segundo trata mais especificamente da metodologia utilizada para o
desenvolvimento de agentes autônomos (PFEIFER, IIDA e BONGARD, 2005).
Pfeifer e Bogard (2007, pp. 357-358) apresentam como estes princípios básicos
podem ser estendidos para incluir insights específicos para cada área e problema
ligados à sistemas adaptativos, evolução artificial e sistemas distribuídos.
      O primeiro princípio de design de procedimentos (P-1) torna explícito que a
metodologia aplicada pela IA deve ser primariamente científica em detrimento do
esforço ligado à simples engenharia, muito embora estes dois objetivos possam
coexistir em harmonia e não são mutuamente excludentes (PFEIFER e GOMEZ,
2005). Para Froese e Ziemke (2009), a questão principal é que devemos construir tais
sistemas inteligentes de tal forma que eles possam nos ajudar a compreender melhor o
fenômeno natural da vida e da mente.
      O princípio da emergência (P-2) é fundamental nesta pesquisa pois demonstra
a convergência das teorias abordadas no sentido da utilização da emergência como
uma forma de heurística para o desenvolvimento de sistemas inteligentes que
demonstrem um comportamento “natural”. Este princípio é compartilhado por muitas
abordagens computacionais da IA no sentido mínimo de que o comportamento
sempre deve emergir das interações de um agente e seu ambiente.
      Este segundo princípio deixa claro que se desejamos desenvolver sistemas
adaptativos, devemos buscar a emergência. O termo emergência, por si só é um pouco
controverso mas, aqui, o utilizamos no sentido mais pragmático: no sentido de não ter
sido programado ou previsto. Ao desenvolver objetivando a emergência, a estrutura
final do agente será o resultado do histórico das suas interações com o ambiente.
      Para Pfeifer e Gomez (2005), a relação entre comportamento e emergência vai
além das interações entre agente e ambiente. Assim, de uma forma estrita, o
comportamento é sempre emergente posto que ele não pode ser reduzido à apenas um
mecanismo interno: ele é sempre o resultado da interação sistema-ambiente. Neste
sentido, Pfeifer, Iida e Bongard (2005) apontam que a emergência cessa de ser um
fenômeno com características discretas (ou é emergente ou não é) e passa a ser tratado
como uma questão de “nível de emergência”: quanto menos influência as escolhas do
designer tiverem sobre o comportamento atual do agente, maior será o nível de
emergência do mesmo.
      Os sistemas desenvolvidos para demonstrar um comportamento emergente
normalmente são mais robustos e adaptativos. Por exemplo, um sistema que
especifique condições iniciais e mecanismos de desenvolvimento irão
automaticamente explorar o ambiente para modelar a estrutura final do agente, como
nos algoritmos genéticos (PFEIFER, IIDA e BONGARD, 2005).
      O quarto princípio (P-4) apenas realça o fato de que organismos (naturais ou
artificiais) estão imersos em três escalas temporais: o estado orientado (tempo
presente), aprendizagem e desenvolvimento (ontogenia) e mudança evolutiva
(filogenia). Portanto, qualquer explicação completa do comportamento de um
organismo deve incorporar estas três perspectivas (FROESE e ZIEMKE, 2009).
      O primeiro princípio do design de agentes (A-1) destaca a importância de que
qualquer sistema autônomo nunca deve ser pensado de forma isolada. Froese e
Ziemke (2009) apontam que devemos considerar três componentes do sistema que
estão correlacionados: (i) o campo de atuação ou ambiente, (ii) o objetivo e
comportamento desejado e (iii) o agente propriamente dito.
      Como um complemento para (A-1), o princípio (A-2) também se faz
importante nesta pesquisa pois denota uma clara intersecção com o conceito de
autopoiese. (A-2) propõe que para melhor compreender o fenômeno da inteligência
nós devemos pesquisar agentes completos em detrimento do estudo dos componentes
internos do agente de forma isolada. Claro que isso não invalida o desenvolvimento
dos componentes isoladamente, mas para Froese e Ziemke, se quisermos um maior
ganho científico na compreensão da inteligência devemos investigar como o
comportamento adaptativo emerge da dinâmica holística cérebro-corpo-mundo. Ainda
sobre (A-2), Pfeifer e Gomez (2005) apontam ainda que os agentes de interesse
devem ser autônomos, autossuficientes, corporificados e situados em um determinado
contexto.
      Uma quebra de paradigma é oferecida pelo princípio (A-3) que propõe, em
contraste à muitas das abordagens computacionais da IA, que a cognição não é
integrada apenas em um controlador central mas, possivelmente, baseia-se em um
largo número de processos assíncronos, paralelos e fracamente acoplados (PFEIFER e
GOMEZ, 2005; FROESE e ZIEMKE, 2009).
2.2.	Criatividade Artística e Computacional
      Uma breve recapitulação sobre a historia da pesquisa sobre criatividade sugere
que varias ideias ligadas ao tema são discutidas literalmente há centenas de anos. A
principio, pode-se facilmente listar entre estas ideias relacionadas à criatividade: a
imaginação, a originalidade, a genialidade, o talento, a liberdade e a individualidade
(ENGELL, 1981) (MARTINDALE, 2007) (SINGER, 1981).
      A conceitualização da criatividade nasce da dialética sobre a própria natureza
humana. A pesquisa sobre criatividade tem florescido nos últimos anos sobretudo
graças ao avanço das pesquisas cientificas nas psicologia cognitivas e neurociências.
De acordo com Mark Runco e Robert Albert (2010), entre os anos 1920 e 1950 dentre
os 121.000 títulos listados na Psychological Abstracts  apenas 186 abordavam o tema
da criatividade, o que significa aproximadamente 0,002% das publicações disponíveis
no acervo. De 1960 a 1997 o número de artigos subiu para aproximadamente 9.000
titulos, representando 0,1% do total. Apesar de parecer insignificante a quantidade de
títulos publicados, podemos perceber claramente a aceleração do número de
pesquisadores envolvidos e investigadores da área. Praticamente todos os maiores
psicólogos do século XXI (Freud, Piaget, Rogers, Skinner) debruçaram-se seriamente,
em algum momento, sobre a questão da criatividade humana.
      Assim como as pesquisas em IA, o campo da criatividade computacional
baseia-se notadamente nos estudos realizados sobre a criatividade humana e, em
alguns aspectos, tenta mimetizar os aspectos biológicos da criatividade ou utiliza da
categorização e definição da criatividade para definir em termos computacionais os
seus algoritmos e sistemas. Portanto, nesta pesquisa, será traçado um panorama das
visões sobre criatividade que podem ser computacionalmente aplicadas, sobretudo no
que concerne a definição e categorização dos níveis de criatividade.
2.2.1.	Pesquisa em Criatividade(s)
      De um ponto de vista histórico, muito antes da visão cristã de criatividade
emergir, o conceito de gênio normalmente associado com poderes místicos e uma
certa proteção ou influência divina já era utilizado. Este conceito nos influenciou (e
influencia) durante séculos.  A capacidade criativa ocupava e denotava, portanto, um
valor social. Ná época aristotélica, junto com o desenvolvimento dos princípios
lógicos que embasam a GOFAI, o conceito de criatividade passou a ser associado
com a “louca” e frenética inspiração. A visão romana de inspiração tem duas
características adicionais: era vista como uma função primariamente masculina e
poderia ser passada hereditariamente.
      A visão ocidental de criatividade tem origem bíblica na Genesis, seguida pela
ideia do “artesão executando trabalhos divinos”. Esta crença se diferencia da visão
oriental sobretudo no que diz respeito aos objetivos da criatividade e do papel do
participante no processo criativo. Para os taoístas e budistas a criatividade era no
máximo uma forma de descoberta e mimetismo, uma vez que a ideia de criação de
algo completamente novo, ex nihilo, seria impossível em um universo completo
(BOORSTIN, 1992). Platão corroborava esta teoria ao considerar que nada novo seria
possível e que a arte à seu tempo fazia um esforço para mimetizar formas ideais
(RUNCO e ALBERT, 2010). Originalidade, que tornou-se um marco critico
contemporâneo da criatividade, não era portanto uma atributo considerado nestas
discussões.
      Estas premissas não foram seriamente discutidas ou questionadas por
aproximadamente 1.200 anos até que na idade média, uma nova perspectiva surgiu
como um talento especial ou incomum era manifestado por um individuo
(normalmente homem). Durante a renascença uma mudança significativa de
paradigma aconteceu e o atributo divino dos grandes artistas passou a ser reconhecido
e enfatizado como habilidades e perspectivas próprias dos indivíduos (RUNCO e
ALBERT, 2010).
      Apesar da interessante mudança paradigmática no entendimento da
criatividade, os avanços foram lentos e pouco significativos. Através dos anos,
mesmo havendo algumas discussões filosóficas sobre o assunto, apenas no século
XXVIII houve duas mudanças significativas sobre a perspectiva intelectual que
moldou o pensamento ocidental: o movimento iluminista se tornou um movimento
identificável com uma filosofia intelectualmente coerente, o rompimento com a
indesejada autoridade que emanava de uma gama de dogmáticas fontes não
cientificas.
      Ao passo que o iluminismo atingia uma massa critica, as ciências naturais
enquanto filosofia e metodologia institucionalizadas tomava forma. Com o
surgimento da ciência e invenção da pesquisa, juntamente com o movimento
iluminista, foi possível iniciar um processo que utilizasse algum tipo de metodologia a
fim de lançar luz sobre o processo criativo humano.
      Vários desdobramentos intelectuais aconteceram antes que um conceito de
criatividade fosse, de fato, desenvolvido. Um destes desdobramentos aconteceu
durante a primeira metade do século XVIII quando a premissa cientifica do direito
natural  foi largamente aceita (STRAUSS, 1968). Runco e Albert (2010) apontam que
ainda que tangencialmente acelerou-se o processo de debates de onde surgiram quatro
principais distinções: (a) genialidade foi dissociada do sobrenatural, (b) genialidade,
apesar de excepcional, era um potencial em cada individuo, (c) talento e genialidade
eram distinguíveis um do outro e (d) o potencial e o exercício dependem da atmosfera
politica da época.
      Posteriormente, em 1961, Melvin Rhodes desenvolveu um estudo que buscava
identificar a multifacetada criação construtiva utilizada nesta pesquisa. De acordo
com  Rhodes, existem quatro Ps: Person (Pessoa), Process (Processo), Product
(Produto) e Press/Place (Ambiente/Local). Pessoa inclui as habilidades cognitivas,
traços biológicos e de personalidade. Processos descrevem os processos cognitivos
que operam ao criar ideias, que incluem a etapas de preparação, incubação,
iluminação e verificação. Produto inclui as ideias expressas em forma de linguagem,
objeto ou outro resultado final. Ambiente inclui a relação entre o criativo e o seu
ambiente (RHODES, 1961).
      Neste sentido, resultados criativos são o resultado de processos criativos
engajados por pessoas criativas, que são suportadas por um ambiente criativo.Mais
recentemente,  versões estendidas deste framework surgiram com mais dois Ps:
Persuasion (Persuação) e Potential (Potencial) (KOZBELT, BEGHETTO e RUNCO,
2010)
      Assim como para a Inteligência Artificial, também não há um consenso
quanto à definição de criatividade até os dias de hoje. No entanto há algumas
definições que convergem no sentido de que há no processo criativo alguma função
emergente de ressignificar ou criar algo novo, implicando na reconstrução do passado
ou reinterpretação do presente (BAHIA, 2008) (HENNESSEY e AMABILE, 2010)
(RUNCO e ALBERT, 2010) (KOZBELT, BEGHETTO e RUNCO, 2010).
      Apesar de ficar claro que há diversos níveis de criatividade, Mihaly
Csikszentmihalyi (apud  KOZBELT, BEGHETTO e RUNCO, 2010) propõe uma
categorização dos níveis de criatividade de little-C (pequeno-C) à Big-C (Grande-C).
Este tipo de categorização nos permite avaliar os diferentes algoritmos de acordo com
a potencial geração de resultados emergentes e criativos. Ao comparar concepções
teóricas se faz necessária esta distinção quantitativa entre o pequeno e o Grande-C. O
Grande-C se refere  a exemplos não ambíguos de expressão criativa, como o Jazz de
Miles Davis ou a pintura de Picasso. Em contraponto, o pequeno-C foca na
criatividade diária cotidiana, por exemplo quando uma pessoa desenvolve uma nova
forma de cozinhar quando não há um ingrediente necessário e posteriormente recebe
elogios pela comida feita.
      Assim como a maior parte das dicotomias, no entanto, falta à esta abordagem
uma certa suavidade para os casos que se encontram nos níveis intermediários.
Paradoxalmente esta abordagem pode parecer excessivamente inclusiva para alguns
casos e não-inclusiva para outros. Por exemplo, se compararmos três pessoas: (a) um
artista não eminente que trabalha profissionalmente com o ensino e venda de
aquarelas, (b) um pintor de aquarelas amador que utiliza o seu tempo livre para pintar
e (c) um estudante secundarista que gosta de pintar esporadicamente. Cada um dos
casos exibe qualitativamente níveis diferentes de criatividade, muito embora nenhum
deles possa ser caracterizado como Grande-C (se comparamos com Cézanne ou
Kandinsky, por exemplo).
      Neste sentido, será que deve-se incluir os três casos citados acima na mesma
categoria? Ao fazer este agrupamento podemos obscurecer potenciais diferenças entre
subcategorias. Uma forma de resolver este tipo de limitação é criar categorias mais
restritivas com cortes mais precisos seguindo exemplos “claros” de criatividade.
Porém, ao realizar cortes mais ríspidos há o risco  já mencionado de excluir potenciais
manifestações criativas de natureza mais subjetiva.
      No intuito de atenuar esta limitação na tradicional dicotomia, podemos
considerar duas novas categorias: mini-C e Pro-C (KAUFMAN e BEGHETTO,
2009). A categoria mini-c ajuda a diferenciaras formas subjetivas e objetivas de
criatividade que se encaixariam na categoria pequeno-C, abrindo espaço para formas
mais subjetivas, pessoais, internas, mentais ou emocionais de criatividade
(KOZBELT, BEGHETTO e RUNCO, 2010). A categoria Pro-C ajuda a distinguir a
área nebulosa que existe entre pequeno-C e Grande-C. Pro-C abre espaço para
criadores “profissionais” (como os artistas profissionais) que ainda nao atingiram (e
podem nunca atingir) o status eminente, mas ainda assim estão bem além dos
criadores pequeno-C (como os hobbistas, por exemplo).
      Por mais que ainda hajam lacunas e a quantidade de categorias não seja
suficiente para descrever todos os níveis de criatividade possíveis, serão utilizadas
nesta pesquisa as 4 categorias para ajudar a descrever o potencial criativo dos
algoritmos.
      Além da categorização dos níveis de criatividade, Kozbelt propõe uma
categorização das teorias sobre criatividade, organizadas em 10 categorias das quais
destacam-se pela convergência com os conceitos apresentados no desenvolvimento
deste pesquisa: Developmental (desenvolvimentista ), Stage & Componential
Processes (estágios e processos componentes), Evolutionary (evolutiva) e Systems
(sistêmica) (KOZBELT, BEGHETTO e RUNCO, 2010). Um resumo das categorias é
apresentado nas figuras 19 e 20.

Figura 19: Sumário da categorização das teorias da criatividade (KOZBELT, BEGHETTO e
RUNCO, 2010)

Figura 20: Continuação do sumário da categorização das teorias da criatividade (KOZBELT,
BEGHETTO e RUNCO, 2010).
2.2.1.1.	Abordagem Desenvolvimentista
      As teorias ligadas à abordagem desenvolvimentista (tradução livre do termo
original) são interessantes para o objeto desta pesquisa pois facilitam a compreensão
de como planejar ambientes propícios para que o potencial criativo seja atingido.
Neste sentido, esta abordagem enfatiza os aspectos criativos Pessoa, Ambiente e
Potencial e seus resultados podem variar de mini-C a Pro-C. Apesar dos Produtos não
ocupem um lugar de destaque nesta abordagem, eles desenvolvem um papel
importante, porém tácito. Esta  participação implícita do Produto acontece pois esta
teoria considera que há uma trajetória temporal que inicia-se com formas mais
subjetivas de criatividade (mini-C) e evolui para formas mais tangíveis e maduras de
expressão criativa (KOZBELT, BEGHETTO e RUNCO, 2010).
      Esta relação temporal é de significante importância no contexto desta pesquisa
pois se analisarmos uma geração inicial de algoritmos genéticos aleatórios, não
podemos afirmar (ainda) que algum processo criativo se expressa. Porém, após
algumas gerações podemos começar a visualizar algum avanço no que diz respeito
aos processos criativos emergentes. Outro aspecto relevante é a questão da interação
entre o individuo e o meio (ambiente/lugar) onde esta interação é principal
responsável pela evolução do processo criativo do individuo. Através desta interação
é que se desenvolve cognitivamente o individuo criativo, portanto a criatividade seria
também um resultado co-evolutivo do ambiente e individuo.
2.2.1.2.	Estágios e Processos Componentes
      A tentativa de compreender a estrutura e natureza dos processos criativos em
termos de estágios e processos sequenciais ou recursivos, que estão envolvidos nos
processos cognitivos individuais, pode gerar modelos e critérios interessantes que
ajudam a compreender as características da criatividade além de permitirem a
implementação de algoritmos e sistemas que exibam comportamento criativo.
Claramente esta abordagem enfatiza o Processo sobre os outros Ps e em termos de
magnitude criativa pode variar entre mini-C a Big-C.
      As teorias que encaixam-se nesta abordagem baseiam-se nos quatro processos
descritos por Graham Wallas em 1926. O estágio inicial é o de preparação (a) onde o
individuo reúne informações acerca do ambiente e define um problema ou objetivo
que deve ser solucionado. Posteriormente, há o processo de incubação (b) que
envolve uma certa dedicação temporal para distanciar-se do problema e dedicar-se ao
processo cognitivo de compreendê-lo. Caso o segundo estagio seja efetivo, temos
então o que Wallas considera como iluminação (c). Neste terceiro estágio uma
solução ou ideia se apresenta ao individuo (ou é descoberta por ele). Por fim, o ultimo
estágio é o de verificação (d) onde o individuo de fato aplica a solução, executa a
ideia e verifica as possíveis implicações (WALLAS, 1926).
      No entanto, o modelo proposto por Wallas sugere uma linearidade que
dificilmente pode ser verificada mas propõe um modelo de processos que se aplicados
recursivamente em ciclos iterativos e incrementais executados diversas vezes pode
ajudar a refinar uma ideia potencial a fim de torná-la cada vez mais adaptada.
      É importante ressaltar que os quatro processos descritos do ponto de vista
cognitivo humano encontram perfeita equivalência nos estágios vistos nos agentes
inteligentes simples, figura 21. Para um agente, o primeiro estágio (a) é o de aquisição
de conhecimento, onde ele deve perceber o ambiente através de seus sensores por
exemplo. No segundo estágio (b), o agente utiliza essa representação de mundo para
desenvolver algum tipo de raciocínio. No terceiro estágio (c), o agente toma uma
decisão de qual é a “melhor” ação à ser executada. Por fim, o agente executa esta ação
(d) através de seus atuadores e volta ao primeiro estágio para dar sequencia de forma
iterativa ao processo como um todo (RUSSEL e NORVIG, 2003).

Figura 21: Modelo e algoritmo de agente reflexivo simples destacados os processos de (a)
preparação, (b) incubação, (c) iluminação e (d) verificação adaptado de Russel e Norvig (2003).
      A possibilidade de criação de um algoritmo, baseado nestes processos,  abre as
portas para que sistemas computacionais possam incrementar a sua magnitude criativa
através de repetição interativa e recursiva de tais processos. No entanto, por mais que
esta equivalência possa ser demonstrada em um agente reativo simples, este tipo de
evolução criativa demanda que o agente ou sistema desenvolvido dê suporte ao
aprendizado, pois fica claro que nesta abordagem o desenvolvimento criativo é
incremental e sobrepõe-se ao que foi criado em iteração anterior de forma evolutiva.
Na ausência de mecanismos de aprendizagem, dadas as mesmas condições do
ambiente o agente executará sempre o mesmo conjunto de ações deixando pouco ou
nenhum espaço para o surgimento de resultados emergentes.
2.2.1.3.	Abordagem Evolutiva (Darwinista)
      Segundo Kozbelt, Beghetto e Runco (2010), vários pesquisadores como
Lumsden, Simonton e Johnson-Laird, propuseram diversas teorias da criatividade
baseando-se nas ideias da evolução biológica, Darwinista e/ou Lamarckista. Dentre
estas teorias destaca-se como forte candidata à mais compreensiva, genericamente
falando, a teoria baseada no modelo Darwinista de Dean Keith Simonton (1984, 1988,
1991, 2003, 2004).
      Em diferentes graus, Simonton aborda todos os Ps da criatividade: Pessoa e
Potencial na identificação de idiossincrasias disposicionais  e desenvolvimentistas
normalmente associadas à capacidade de concretizar um determinado potencial
criativo inicial; Processo, ao determinar um modelo com duas etapas – idealização e
elaboração – onde a aleatoriedade das combinações entre ideias desenvolve um papel
central pois gera uma complexidade difícil de ser controlada ou limitada; Produto, ao
apontar o embate entre avaliações iniciais de curto prazo e julgamentos estáveis de
longo-prazo no que diz respeito aos artefatos criativos;  Ambiente, ao identificar
fatores sociais que levam à comportamentos criativos; Persuasão, ao enfatizar como
as relações sociais dinâmicas podem estabelecer o veredito sobre uma expressão
criativa (SIMONTON, 2003).
      A base do modelo de Simonton é, de fato, o processo de dois estágios
descritos por Campbell (apud. KOZBELT, BEGHETTO e RUNCO, 2010),
envolvendo a geração “cega” de um conjunto de ideias/hipoteses e a retenção e
elaboração seletiva sobre este conjunto. Sob este ponto de vista, as ideias são
combinadas de forma aleatória (SIMONTON, 2004), abaixo do nível de consciência.
As combinações mais interessantes são então conscientemente elaboradas a fim de
elaborar produtos criativos que por sua vez são julgados por outros indivíduos.
      O argumento de Campbell de um sofisticado modelo quantitativo de como a
produtividade criativa se desenvolve durante o processo ontogênico de um individuo
se assemelha aos algoritmos descritos na computação evolutiva e tem importante
impacto na compreensão da natureza eminente dos processos e ambientes criativos.
Este modelo assume que existem diferenças iniciais no que tange o potencial criativo
entre diferentes indivíduos. Neste sentido, ao passar do tempo, um determinado
individuo é capaz de expandir o seu potencial criativo através, principalmente, do
exercício do processo de criação e da aprendizagem oriunda deste processo
(SIMONTON, 1997). Esta visão, de certa forma, vai ao encontro do descrito na
abordagem desenvolvimentista.
2.2.1.4.	Abordagem Sistêmica
      Uma forma mais ampla e ambiciosa de se abordar a criatividade é caracterizá-
la como sendo um resultado emergente de um sistema complexo que contém vários
subcomponentes interagindo uns com os outros. Cada um destes subcomponentes
deve ser considerado a fim de propor uma compreensão mais rica e significativa dos
processos e resultados criativos. A abordagem sistêmica propõe uma visão mais
qualitativa e contextualizada da criatividade, quase que se contraponto à visão
quantitativa evolutiva. No entanto, assim como a abordagem evolutiva, o ponto de
vista sistêmico aborda todos os Ps, porém com diferente ênfase de acordo com o
teórico como Csikszentmihalyi, Gruber, Wallace e Dasgupta (KOZBELT,
BEGHETTO e RUNCO, 2010).
      A teoria sistêmica de Csikszentmihalyi (1988) tem um foco menos acentuado
no individuo criativo se comparado à abordagem evolutiva, mas ambas as teorias
convergem no sentido de que consideram a criatividade como sendo uma
característica multifatorial e deve ser observada a partir de um olhar mais amplo.
Neste sentido, enfatiza-se o papel ubíquo do ambiente sobre os demais Ps,
especialmente quando se trata do surgimento de resultados criativos de magnitude
Grande-C (CSIKSZENTMIHALYI, 1988). Além disso, deve-se considerar a natureza
do individuo criativo através do detalhamento de como os indivíduos (além do
criador) contribuem para a emergência da criatividade.
      Csikszentmihalyi introduziu a sua visão sistêmica ao reinterpretar a questão “o
que é criatividade?” como “onde está a criatividade?”. Ao invés de observar a
criatividade como um atributo intrínseco de alguns artefatos, ele propôs que a analise
da criatividade emergia através de três componentes: (a) o domínio ou corpo de
conhecimento que existe em uma determinada disciplina em um determinado recorte
temporal; (b)  o individuo que adquire conhecimento sobre um determinado domínio
e produz alguma variação do domínio em questão; (c) o campo, compreendendo
também os especialistas e membros de determinada disciplina, que decidem quais
novidades produzidas são validas para uma nova geração (KOZBELT, BEGHETTO e
RUNCO, 2010).
      Esta visão enfatiza uma criatividade mais colaborativa e dependente das
condições sociais em detrimento de uma visão de processos intrapsíquicos e
contribuições individuais. Neste sentido de criatividade colaborativa, podemos
considerar que sistemas e populações de indivíduos com “menor” poder cognitivo
podem também demonstrar uma característica criativa que emerge da interação entre
esses indivíduos. Portanto, se considerarmos um sistema multiagentes, capazes de
desenvolver processos cognitivos além de agir de forma autônoma sobre o ambiente
torna-se possível modelar também algoritmos que de certa forma demonstrem um
comportamento criativo emergente.
2.3.	Autopoiesis: auto-organização e autonomia
      A noção de autopoiese, enquanto organização do vivo, originou-se nos
trabalhos dos biólogos chilenos Humberto Maturana e Francisco Varela na década de
1970 (MATURANA e VARELA, 1997). Mesmo o conceito de autopoiese tendo sido
desenvolvido no contexto da biologia teórica foi, também, desde o seu início
associado com a simulação artificial muito antes do termo “vida artificial” ter sido
introduzido no final dos anos 1980 (LANGTON, 1989).
      Hoje em dia o conceito de autopoiese continua tendo um impacto significativo
no campo da vida artificial computacional. Pier Luisi (2003) apresenta uma boa
revisão do conceito. Além disso, houve também um esforço para integrar a noção de
autopoiese ao campo das ciências cognitivas.
      Após a criação do termo autopoiese, o conceito associado evoluiu nos
trabalhos de ambos: Maturana e Varela. Para o propósito desta pesquisa será utilizada
a definição mais extensamente utilizada por Varela em uma série de publicações nos
anos 1990 e que tem sido resgatada em trabalhos recentes como Froese, Virgo &
Izquierdo (2007). De acordo com esta definição de Francisco Varela, um sistema
autopoiético – a organização mínima do vivo – é aquele que continuamente produz os
componentes que o especificam ao mesmo tempo em que percebe o sistema como
uma unidade concreta no espaço-tempo, o que faz com que a rede de produção dos
componentes seja possível (apud. FROESE e ZIEMKE, 2009).
      Para ser mais preciso, um sistema autopoiético, definido enquanto unidade, é
organizado como uma rede de processos de produção (síntese e destruição) de
componentes de tal forma que estes componentes: (i) continuamente regeneram-se
formando uma rede que os produz e (ii) constituem o sistema como uma unidade
distinguível no domínio no qual ele existe. Além dos dois critérios explícitos para a
autopoiese, podemos acrescentar um outro ponto importante, a saber, que a auto-
constituição de uma identidade implica a constituição de um domínio relacional entre
o sistema e o seu ambiente (FROESE e ZIEMKE, 2009).
      A forma deste domínio não é pré-determinada mas, possivelmente, co-
determinada pela organização do sistema e do ambiente. Assim, qualquer sistema que
cumpra os critérios para autopoiese gera também o seu próprio domínio de interações
no mesmo momento em que emerge a sua identidade.
      Autopoiesis pode ser considerada como uma tentativa de resposta para a
questão sobre como podemos determinar quando um agente é um ser vivente ou não
baseado no tipo de sistema e não em como ele se comporta ou de onde ele veio.
Assim, esta proposta contrasta com a abordagem puramente funcional ou histórica.
      Um organismo unicelular é o perfeito exemplo paradigmático de um sistema
autopoiético e pode ser utilizado para ilustrar a circularidade que é inerente à esta
autoprodução (figura 22). Neste caso do ser unicelular esta relação circular é expressa
na co-dependência entre os limites determinados pela membrana (externa) e a rede
metabólica (interna). Esta rede metabólica se constrói distinguindo-se do ambiente
como um sistema unificado e isto só é possível graças ao sistema externo
(membrana), que evita que o sistema se disperse no ambiente. Por sua vez, o sistema
externo só é construído pois existe uma rede metabólica funcional, capaz de produzir
os componentes que o compõem. Este sistema como um todo é denominado
autopoiético e pode ser reproduzido artificialmente através da utilização das técnicas
de IA apresentadas nesta pesquisa: algoritmos genéticos e redes neurais.

Figura 22: O ciclo de auto-regulação de uma célula enquanto unidade autopoiética (LUISI,
2003).

      Para Evan Thompson (2007), muito embora haja casos na literatura em que
organismos multicelulares são considerados como autopoiéticos, esta é uma discussão
profunda que se distancia do trivial e permanece controversa. Ainda assim, nós
intuitivamente dizemos que tais organismos preenchem os requerimentos para que
sejam declarados como autônomos. Isto faz com que este organismo multicelular seja
distintamente diferente de uma entidade autopoiética mínima no que tange a sua
identidade mas seja similar no que demarca uma entidade autônoma em relação ao
seu ambiente (FROESE e ZIEMKE, 2009).
      Para Maturana e Varela (1997), tanto a criação da teoria autopoiética quanto a
sua aplicação aos sistemas sociais representou uma revolução epistemológica. Essa
proposta de mutação no foco epistemológico propiciou uma melhor observação do
meio e suas características. Anteriormente, o processo de observação científica de um
dado objeto pressupunha a análise estrutural de todos os seus elementos constitutivos
isoladamente. Conhecer algo significava poder determinar quais são as partes que
determinam o todo desse objeto. Não se avaliava as relações entre os elementos, mas
apenas sua condição/colocação no todo.
      Máquinas, segundo Maturana e Varela (1997), são consideradas comumente
como sistemas materiais definidos pela natureza de seus componentes e pelo objetivo
que cumprem em seu operar como artefatos de fabricação humana. No entanto esta
maneira de vê-las é obviamente ingênua já que nada diz sobre como são construídas.
      As máquinas são unidades, formadas por componentes, caracterizados por
propriedades capazes de satisfazer determinadas relações que determinam na unidade
as interações e transformações destes mesmos componentes. Portanto, qualquer
máquina é um sistema que pode materializar-se diante de muitas estruturas diferentes
e cuja organização definitória não depende das propriedades dos componentes mas da
relação entre eles (MATURANA e VARELA, 1997).
      O fato de que os seres vivos são máquinas não pode demonstrar-se apelando
aos seus componentes. Deve-se mostrar a sua organização mecanicista de forma tal
que seja óbvio o modo como todas as suas propriedades emergem dela mesma. Para
fazer isto, Maturana e Varela (1997) descrevem primeiro a classe de máquinas que
são os sistemas viventes e em seguida indicam como as propriedades particulares que
as caracterizam podem surgir como consequência da organização desta classe de
máquina.
      Para Newell e Simon em sua leitura durante o Turing Award (1976), a
atividade inteligente seja natural ou artificial pode ser alcançada por uma máquina
através do desenvolvimento de padrões simbólicos (coleções de padrões e processos)
capazes de representar as características proeminentes de um determinado domínio de
problema.
      Para a máquina, este padrão simbólico deve ser suficientemente consistente a
fim de gerar uma abstração do domínio onde ela está inserida que a permita realizar
operações sobre estes padrões para gerar soluções potenciais dos problemas. Este
conjunto de padrões simbólicos, por sua vez, pode alterar a sua coleção de padrões,
que consistem na base de conhecimento do agente, através dos processos internos,
autopoiéticamente. Isto significa dizer que a organização interna deste conjunto de
padrões simbólicos só pode ser alterada pelos processos internos, autocontidos na
máquina autopoiética (MATURANA e VARELA, 1997).
      Podemos dizer, apoiando-se em Maturana e Varela, portanto, que a máquina
autopoiética é um sistema auto-homeostático que tem a sua própria organização como
variável que mantém constante. A organização autopoiética significa que os processos
concatenados de uma maneira específica tal que estes processos produzem os
componentes que constituem o sistema e o especificam enquanto uma unidade.
      Kenneth Craik (1943) especificou três passos fundamentais para a definição
de um agente baseado no conhecimento: (i) o estímulo deve ser traduzido em uma
representação interna; (ii) a representação é manipulada por processos cognitivos para
derivar novas representações internas; (iii) essas representações internas são
traduzidas em estímulo. Segundo Craik, a justificativa da modelagem do agente desta
forma pois:
Se o organismo transporta um ‘modelo em escala reduzida’ da
realidade externa e de suas próprias ações possíveis dentro de
sua cabeça, ele é capaz de experimentar várias alternativas,
concluir qual a melhor delas, reagir a situações futuras antes
que elas surjam, utilizar o conhecimento de eventos passados
para lidar com o presente e o futuro e, em todos os sentidos,
reagir de maneira muito mais completa, segura e competente
às emergências que enfrenta (CRAIK, 1943).

      Entre as máquinas, aquelas que mantem algumas de suas variáveis constantes
ou dentro de um intervalo limitado de valores e devem expressar-se de tal modo que o
processo se defina como verificado integralmente dentro dos limites que a própria
organização da máquina especifica, ou seja, ela se autocontém (MATURANA e
VARELA, 1997).  Esta definição traz pontos em comum com a especificação de
agente baseado em conhecimento de Kenneth Craik pois tanto o sistema autopoiético
quanto o agente definem-se pela característica de modificarem a si mesmo.
      As máquinas autopoiéticas, definidas em Maturana e Varela, são autônomas.
Isso quer dizer que subordinam todas as suas mudanças à conservação de sua própria
organização, independente de quão profundas sejam as outras transformações que
possa sofrer no processo. Estas possuem individualidade, ou seja, por meio da
manutenção invariável da sua organização conservam ativamente uma identidade que
não depende necessariamente de suas interações com o observador.
      As máquinas autopoiéticas são, ainda, definidas como unidade por, e apenas
por, sua organização autopoiética: suas operações estabelecem seus próprios limites
no processo de autopoiese. Além disso, não possuem nem entradas, nem saídas mas
podem ser perturbadas por fatos externos e experimentar mudanças internas que
compensam estas perturbações. Assim, muitos sistemas autopoiéticos demonstram um
comportamento dinâmico e não-linear, devido ao fato de que estas mudanças no
ambiente causam uma reorganização da estrutura interna do sistema, que por sua vez
causa novas perturbações no ambiente, caracterizando um movimento emergente.
      Aqui foram dadas várias evidências que podem contribuir para esclarecer
porque o conceito de autopoiese pode ser bastante interessante para os artistas.
Artistas computacionais, sobretudo, podem encontrar neste conceito inúmeros
desafios tecnológicos capazes de os inspirar a produzir trabalhos artísticos. Artistas
que focam no contexto social também podem ser atraídos para as diversas aplicações
sociais inerentes ao conceito. Os teóricos podem achar fascinante e inspiradora a
ontologia por trás do que foi apresentado. Os trabalhos que lidam com interatividade
podem ser enriquecidos ao utilizarem todos os aspectos da autopoiese.
      Além dos princípios de design aqui apresentados e das diversas formas de
emergência e classes de autopoiese que o artista deve considerar há outros fatores que
são importantes que se tenha em mente como quais perturbações o agente poderá
perceber. Na maioria dos casos das obras interativas estas perturbações serão oriundas
do público, do interagente. O artista deverá, portanto, determinar como o sistema
deverá perceber, reagir e adaptar-se à tais mudanças sem influência as escolhas do
mesmo. Para tal, é necessário que o artista dote o agente das ferramentas necessárias
para a realização das inferências.
      As perturbações percebidas pelo sistema podem reforçar ou inibir um
determinado comportamento. No entanto quanto menos influência o projetista tiver
sobre qual a influência da perturbação em qualquer um dos casos, maior será o nível
de emergência do comportamento resultante. Esta perturbação, mesmo quando inibe
um determinado comportamento, reforça uma estrutura organizacional interna cujos
elementos podem favorecer o presente comportamento ou mover a organização
interna para outra direção. Por outro lado o sistema pode causar a perturbação que, no
caso da obra interativa, pode ser infligida ao público ou no ambiente em que está
inserido. Uma vez que o artista toma conscientemente estas decisões ao projetar o
sistema autopoiético ele o dota de um autocontrole que pode, inclusive, torná-lo
independente de seu criador, atingindo um nível máximo de autopoiese e emergência.
      No âmbito da autopoiese há duas obras que valem ser destacadas. A primeira
chama-se Autopoiesis, do artista Kenneth Rinaldo (2000), figura 23. Segundo o
artista, Autopoiesis é uma vida robótica artificial composta por uma série de quinze
braços sonoros mecânicos que interagem com o público e modificam o seu
comportamento. Estas mudanças comportamentais do sistema baseiam-se no feedback
de sensores infravermelhos presentes nos braços, da presença de participantes na
instalação e da comunicação entre os distintos braços.
      Esta série de esculturas robóticas comunicam-se através de uma rede de
computadores e tons telefônicos audíveis, sendo este último meio de comunicação
uma espécie de linguagem musical do grupo. A interatividade, nesta obra, engaja os
interagentes que, por sua vez, interferem diretamente na evolução emergente do
sistema. Isto faz com que se crie um caminho evolutivo único não apenas para cada
um dos braços que compõem a instalação como também para a obra como um todo,
denotando uma emergência local-para-global. Por outro lado o comportamento da
obra como um todo influencia o comportamento de cada um dos braços
individualmente, da forma global-para-local.

Figura 23: Em destaque, um dos 15 braços robóticos da instalação Autopoiesis, do artista Ken
Rinaldo (2000)

      Autopoiesis é, portanto, um ambiente em constante evolução que parece
(re)criar a si mesmo como um sistema vivo. Nenhum ambiente ou elemento virtual
está aparentemente ligado ao projeto, mas é o ambiente físico que reage através dos
princípios do comportamento inteligente (PAUL, 2008).
      A segunda obra, chama-se Audible Ecosystems de Agostino Di Scipio e
encontra-se profundamente descrita em seu artigo “Sound is the Interface” (2003).
Nesta, Di Scipio transforma uma sala em um sistema autopoiético capaz de responder
tanto às perturbações causadas pela presença do interagente quanto ao movimento
auto-regulatório constante do sistema.
      Ao optar por inserir o ecossistema num ambiente que tem o barulho, noise,
como única fonte sonora, Di Scipio estabelece um ecossistema coeso capaz de
interagir com a sala, escolhendo certas frequências e descartando outras. Isto permite
que o sistema autopoiético “escolha” quais frequências devem ser reforçadas e quais
devem ser amenizadas. Esta interação pode ser melhor compreendida através da
determinação das relações entre as partes, ilustrada na figura 24.

Figura 24: Esquema básico da interface da obra Audible Ecosystems (DI SCIPIO, 2003).

      Para Di Scipio (2003), o sistema autônomo faz com que os processos de
interação se reflitam na sua estrutura interna. Assim,  isolar sistema do meio é “mata-
lo”. Portanto, o papel do som é fundamental aqui posto que o barulho é o meio no
qual o sistema sonoro está situado, ou seja, é o ambiente do ecossistema. Além disso,
o som é a fonte de energia que permite que o sistema autopoiético possa se manter e
desenvolver. Porém, paradoxalmente, nenhum sistema autônomo pode existir sem que
haja um acesso direto ao que é externo ao próprio sistema. De fato, uma dialética
complexa se impõe entre autonomia e a heteronomia proposta por Kant em qualquer
sistema vivo.
      Neste ponto, é importante esclarecer a relação entre (a) emergência através da
auto-organização, (b) autonomia através de fechamento organizacional e (c)
autopoiese através da autoprodução. Para Pfeifer e Bongard (2007), uma forma
coerente de se observar estas relações é na forma de classes de inclusão, da
emergência à autopoiese. Ao visualizar estas relações enquanto classes listamos,
respectivamente, da mais inclusiva para a menos inclusiva: da emergência à
autopoiese. Em outras palavras, (a), (b) e (c) podem ser caracterizadas como
emergência através da auto-organização (PFEIFER e BONGARD, 2007).
      O conceito de auto-organização pode ser interpretado de várias formas
distintas, mas do ponto de vista autopoiético é digno de ser apresentado por dois
aspectos: (a) a determinação local-para-global, de tal forma que o processo emergente
tem sua identidade global constituída e restringida como um resultado das interações
locais e (b) determinação global-para-local onde a identidade global e sua interação
contextual em curso restringem as interações locais (PFEIFER e BONGARD, 2007).
      No segundo caso de autonomia (b), o tipo de emergência apresentado pode ser
denominado co-emergência dinâmica, figura 25. Nesta relação, o sistema autônomo
não se caracteriza apenas pela emergência através da auto-organização mas também
pela autoprodução pois o todo é constituído pelas relações entre as partes e as partes
são constituídas pelas relações que mantém com as demais.


Figura 25: Representação gráfica do processo de co-emergência, segundo Luisi (2003).

      Finalmente, os sistemas autopoiéticos (iii) são também sistemas autônomos
posto que são caracterizados por tal co-emergência dinâmica mas são especificados
dentro de um domínio específico. Perceba que a noção de fechamento organizacional
vai além do conceito de causa-efeito.
      Vale à pena ressaltar que a noção de autonomia utilizada na abordagem
enativa é fundamentalmente diferente da utilizada no campo da robótica e IA.
Enquanto este último campo geralmente se interessa por uma forma de autonomia
comportamental, o primeiro se interessa pela autonomia constitutiva, determinada
pela autoconstrução de uma identidade sob condições precárias (FROESE, VIRGO e
IZQUIERDO, 2007).
      Perceba que isto não significa que a abordagem enativa, cujo foco é a
autonomia constitutiva ignora os aspectos comportamentais. Froese e Ziemke (2009)
apontam que a autonomia constitutiva, de fato, segue uma autonomia comportamental
uma vez que (a) a autonomia constitutiva é fundamentalmente um processo de
construção de identidade e (b) esta identidade emergente dota, logica e
mecanicamente, o ponto de referencia para um domínio de interações (MATURANA
e VARELA, 1997). É, portanto, uma questão importante até que ponto a separação
entre o domínio constitutivo e comportamental de um sistema autônomo pode ser
justificada do ponto de vista da emergência e autopoiese.
2.4.	Emergência, heurística para criatividade
      O conceito de emergência é definido segundo Peter Cariani (2009) como o
surgimento de novas entidades que, em um sentido ou em outro, não poderiam ter
sido previstas com base naquilo que as precedeu. Pode-se ainda entender a
emergência como o surgimento de macropadrões decorrentes de microprocessos.
      Encontramos na natureza vários exemplos de emergência. Segundo Cariani
(2009), as principais mudanças emergentes na história do universo incluem a
formação de partículas, átomos e moléculas na microescala e a formação de estrelas,
galáxias, e buracos negros na macroescala. Pode-se inclusive questionar se as leis da
física e mesmo o tempo em si são aspectos emergentes oriundos dos primórdios do
universo.
      De uma forma geral, emergência designa o comportamento que não foi
explicitamente programado em um sistema ou agente. Segundo Pfeifer e Bongard
(2007), pode-se distinguir entre três tipos de emergência: (a) fenômeno global
surgindo de um comportamento coletivo, (b) comportamento individual como
resultado de uma interação do agente com o ambiente e (c) emergência
comportamental de uma escala de tempo para outra. A formação de uma trilha de
formigas é um exemplo do primeiro tipo. As formigas, por si sós, não tem consciência
sobre o fato de que estão formando uma trilha que irá determinar o menor caminho
até a fonte de alimento (PFEIFER e BONGARD, 2007).  Para Laurentiz (2007), ao se
trabalhar com uma população de indivíduos, não podemos focar apenas em um dos
indivíduos, mas devemos nos interessar também nas propriedades emergentes dos
processos dinâmicos destas populações.
      Guilherme Kujawski (2009) cita um formigueiro como representação de uma
sociedade centralizada sobre a égide da rainha, detentora do poder absoluto, capaz de
guiar operárias e soldados através de estímulos químicos. Para ele, este mito é
desfeito sob a luz da ciência que determina que o todo não é simplesmente a soma das
partes constituintes e sim algo mais complexo. Kujawski questiona como seria
possível uma inteligência centralizada, neste caso, determinar que o cemitério ficasse
o mais longe possível da colônia mas não tão longe quanto o depósito de lixo. Este
comportamento se torna possível por meio de uma inteligência distribuída, ou o que
ele se refere como sinédoque biológica. Neste caso, a simples interação das partes
individuais faz emergir o todo complexo do sistema.
      O neurocientista Valentino Braitenberg (apud. PFEIFER e BONGARD, 2007)
apresentou em 1984 uma série de veículos-robôs cujo grau de complexidade é
incrementado, começando com os mais simples cujos “cérebros” são formados por
apenas alguns fios. Um dos veículos de Braitenberg, ilustrado na figura 26, demonstra
o comportamento de seguir a luz ou afastar-se dela. Neste tipo de veículo, este
comportamento emerge pois o robô tem dois fios que conectam os seus sensores aos
motores de uma forma particular e há uma fonte de luz no ambiente. Este é um
exemplo de comportamento emergente resultante da interação com o ambiente.

Figura 26: Veículos simples de Braitenberg: (a) veículo que busca a luz e (b) veículo que evita a
luz (PFEIFER e BONGARD, 2007).
      O terceiro tipo de emergência diz respeito às escalas de tempo que devem ser
incorporadas sob três perspectivas: (i) estado orientado, o “aqui e agora” que diz
respeito ao estado atual do mecanismo, (ii) aprendizado e desenvolvimento, sob o
ponto de vista ontogenético e (iii) evolutivo, perspectiva filogenética, ver figura 27.
      A perspectiva temporal do “aqui e agora” trata do que está acontecendo no
presente momento, a perspectiva ontogenética se estende pela vída inteira do
indivíduo enquanto a perspectiva evolutiva pode se estender por várias gerações de
uma população de indivíduos. Por exemplo, acertar o seu dedão com um martelo pode
o ensiná-lo como manejar melhor a ferramenta, assim o “aqui e agora” afeta o
desenvolvimento em uma outra escala de tempo. O aprendizado influenciará suas
decisões futuras, assim o desenvolvimento afetará o “aqui e agora”. Por fim, a
evolução da morfologia da mão altera o que um organismo pode fazer com ela, assim
a evolução afeta o “aqui e agora” (PFEIFER e BONGARD, 2007).

Figura 27: Escalas de tempo e emergência: “aqui e agora”, ontogenético e filogenético (PFEIFER
e BONGARD, 2007).

      Portanto, as três escalas de tempo – “aqui e agora”, ontogenética e filogenética
– devem ser consideradas a fim de determinar se o sistema como um todo é capaz de
gerar resultado ou comportamento emergentes em qualquer uma destas escalas. Se
pode-se demonstrar que um mecanismo pode ser observado em uma escala de tempo
torna-se mais fácil a compreensão do mecanismo observado. Compreende-se por
mecanismo qualquer função ou parte do sistema que participe ativamente da evolução
do agente ou tenha papel relevante na construção emergente do mesmo.
      A observação sobre como os processos ou características de um agente
emergem de um processo de ação ao longo de uma escala de tempo nos permite
aprender não apenas sobre o processo em si mas também como e em que situações
isto ocorre. Por exemplo, ao utilizar algorítmos evolutivos para projetar de forma
automatizada agentes capazes de locomover-se, podemos estudar qual morfologia e
sistemas neurais desenvolvem-se dependendo do ambiente, como água ou terra
(PFEIFER e BONGARD, 2007). Obviamente que esta observação não se limita aos
ambientes físicos posto que um agente que trafega no ciberespaço também deve lidar
com características do ambiente.
      No entanto, a emergência é algo mais amplo do que o simples surgimento de
novas estruturas e novos padrões. Inclui também a formação fundamentalmente nova
de organizações de matéria, processos informativos e o aparecimento de um novo
aspecto de mundo (CARIANI, 2009). Num contexto natural fica claro que as
transições emergentes possam envolver uma ou mais destas formações
fundamentalmente novas porém isto não se aplica necessariamente aos modelos
computacionais dado o diferente contexto e ambiente em que as relações se
constroem: o ciberespaço. No âmbito computacional as relações se dão através da
criação de novas conexões e manifestam-se na forma de bits, sendo necessária uma
outra abordagem para a definição dos fenômenos emergentes neste contexto,
podendo-se questionar sobre a possibilidade de obtenção de resultados emergentes em
um ambiente virtual.
       Kujawski afirma que é possível algo novo, imprevisível, emergir de uma
máquina limitada como a de Turing desde que fique clara a diferença entre regras e
leis. As primeiras são procedimentos formais bem definidos, as segundas são
condições universais (KUJAWSKI, 2009). Existem “algoritmos” ou um conjunto de
regras por trás de qualquer fenômeno emergente, independente da sua natureza. Um
bom exemplo de emergência em um sistema de regras simples é o Game of Life, jogo
criado por John Conway na década de 1950. Neste jogo, o nascimento, a
sobrevivência ou a morte de indivíduos é uma função de seu próprio estado e do
estado dos seus vizinhos. Apesar da sua simplicidade, os experimentos do jogo da
vida mostram que ele é capaz de evoluir estruturas de complexidade e habilidades
extraordinárias, incluindo “organismos” multicelulares autorreplicáveis (LUGER,
2004).
      No jogo da vida, cada célula sobrevive ou morre segundo regras simples: (a)
cada célula com dois ou três vizinhos sobreviverá até a próxima geração, (b) a célula
que tiver mais de quatro vizinhos é removida (perece) por superpopulação e cada
célula com menos de dois vizinhos morre de isolamento e (c) cada célula vazia
adjacente a exatamente três vizinhos – nem mais, nem menos – deve tornar-se viva.
Iniciando-se com um padrão simples e aplicando-se estas regras surgem padrões mais
complexos, conforme ilustrado na figura 28 (GARDNER, 1970).

Figura 28: Sequência de passos da esquerda para a direita considerando as regras do jogo da
vida, segundo John Conway (apud. GARDNER, 1970).
      As redes neurais recorrentes, apresentadas anteriormente, proveem um bom
espaço para a experimentação visando os resultados emergentes, pois baseiam-se em
regras simples e, dependendo da forma como se estrutura a topologia da rede, ela
pode tornar-se instável, oscilando e exibindo às vezes um comportamento caótico.
Este comportamento abre espaço para a ocorrência do resultado emergente.
      Um nível mais profundo de emergência denominado emergência epistêmica
envolve, naturalmente, o surgimento de novas visões de mundo e novas perspectivas
intrinsecamente ligadas às alterações sensoriais. O aprimoramento ou
desenvolvimento de novos órgãos sensoriais permite que um organismo evolua para
uma outra linhagem, surgindo concomitantemente uma nova visão de mundo. Este
desenvolvimento também ocorre na evolução tecnológica na medida em que
construímos artefatos como termômetros, relógios, e telescópios que ampliam os
nossos sentidos ou proveem um aumento das funções biológicas.
      De acordo com o que se permite à um sistema escolher os seus próprios
sensores, este tem um poder decisório de como será o seu olhar para o mundo,
atingindo um grau limitado de autonomia epistêmica, liberando-se das limitações
impostas pelos sensores com os quais contava inicialmente. Um outro fechamento
organizacional é atingido quando um organismo se torna capaz de construir seus
próprios sensores. Enquanto o fechamento organizacional da autopoiese permitiu aos
organismos que controlassem suas estruturas internas, esse outro fechamento
organizacional permite que os organismos controlem seu curso epistêmico
(CARIANI, 2009).
      Através da identificação das primitivas que compõem um sistema podemos
criar um novo sistema complexo ou modificar um pré-existente através da
recombinação destas. A recombinação ou alteração das partes ou das relações entre
elas é capaz, por si só, de constituir novos sistemas com potencial para gerar
resultados emergentes. Estas primitivas dependem do sistemas, podendo ser “átomos”
materiais ou estruturais, símbolos, estados, funcionalidades, operações, hipóteses de
uma teoria, sensações ou ideias. Para que uma entidade seja considerada como uma
primitiva ela não deve poder ser construída a partir das combinações dos outros, ou
seja, suas propriedades não podem ser logicamente deduzidas das propriedades  das
outras entidades. Neste sentido, as combinações de objetos de um nível inferior não
geram, necessariamente, primitivas de um nível superior posto que estes sistemas de
nível mais abstrato podem ser decompostos em átomos de um nível ainda mais baixo.
      Assim, torna-se possível prever os processos criativos que simplesmente
recombinam as primitivas fixas existentes versus aquelas que, de alguma forma,
geram novas primitivas. Para Cariani (2009), a geração de novidade emergente pode
ocorrer de dois modos: emergência combinatória e emergência criativa, conforme
figura 29.
      Os algoritmos genéticos são claros exemplos de novidade combinatória. Nesta
abordagem uma codificação especifica em linguagem genética as primitivas de um
sistema que deverão ser combinadas para formar produções complexas enquanto um
processo seletivo heurístico orienta a geração destas novas combinações de primitivas
(CARIANI, 2009).

Figura 29: Emergência combinatória versus emergência criativa (Cariani, 2009).
      Retomando o exemplo do Gerador de Acordes, foram realizados vários testes
com as taxas variando entre 60 e 80% para o operador de crossover e 0.8% a 2% para
mutação, conforme ilustrado na figura 30. Ficou claro que o uso de operador de
crossover não representa uma evolução posto que as notas que formam um acorde
maior para determinada tônica, não formam acordes maiores para nenhuma outra
nota. O que significa dizer que ao se aplicar o crossover sobre dois indivíduos com
tônicas diferentes, cujas notas já encontram-se harmônicas, podemos gerar dois
indivíduos completamente mal adaptados de acordo com a função de fitness
(BARRETTO, 2012).

Figura 30: Resumo dos parâmetros utilizados na implementação do Gerador de Acordes.
      O operador de mutação quando aplicado a taxas mais altas provocou
indivíduos com fitness máximo em menos rodadas, ou seja, gerou acordes maiores
com um número menor de interações. Isto se explica pois se uma nota não compõe
um acorde considerando a tônica e as demais, ou seja, se apenas uma das notas está
fora do intervalo, é muito mais provável que ela possa ser evoluída individualmente
através de uma mutação do que em conjunto com a nota subsequente (ou anterior)
através de recombinação.
      A cada rodada executada o indivíduo com maior fitness tinha o conteúdo do
seu cromossomo enviado para síntese sonora. Esta evolução mapeada dos
cromossomos permitiu perceber que, enquanto não se atingia um acorde maior, vários
outros tipos de acorde emergiram do sistema.
      No entanto, a aplicação do operador de recombinação e mutação permitiu o
surgimento de acordes não previstos como os menores, diminutos e aumentados, por
exemplo, sugerem um comportamento emergente do sistema. Os dados oriundos deste
sistema ainda serão estudados para identificar que tipos de acordes emergem além de
definir com que frequência isto acontece e o que favorece tal comportamento.
      Neste exemplo a emergência combinatória foi capaz de gerar acordes que não
haviam sido previstos como os dissonantes, por exemplo, porém o conjunto de
possibilidades possíveis dentro deste domínio é finito. Essa estratégia para a geração
de novas variedades a partir de combinações de conjuntos de primitivas relativamente
pequenos é poderosa, formando a base da sistemática da linguagem humana e
computacional.
      Para Cariani (2009), os computadores são adequados para gerar e avaliar as
combinações de primitivas de símbolos e as operações lógicas sobre estes. Assim é
possível detectar as propriedades formais úteis, interessantes e imprevistas oriundas
do sistema. Analogamente, nas máquinas adaptativas e treináveis, como os algoritmos
genéticos e as redes neurais, a busca direcionada otimiza as combinações de
características. Estas máquinas se distinguem formalmente pelas regras que dirigem
os processos de busca e pelo fato de suas estruturas dos respectivos espaços de
combinação serem transversais.
      A novidade combinatória se mostra como uma estratégia dinâmica e criativa
na medida em que leva constantemente à formação de novas cominações de
elementos. No entanto, vale à pena ressaltar que o uso de conjuntos fixos de
elementos primitivos significa que o conjunto de combinações possíveis é fechado.
Por outro lado, os sistemas que criam suas próprias primitivas são abertos em relação
ao seu conjunto atual de primitivas.
      Esta abertura dos sistemas capazes de criar novas primitivas se dá pois o
processo de geração de novas primitivas é indefinido. Pode-se demonstrar esta
diferença ao se verificar a características dos domínios fechados e abertos. Por
exemplo, um conjunto de todas as sequencias de 3 dígitos utilizando números de 0 a 9
é extremamente bem definido e contém quase 20 mil elementos, que podem ser
enumerados.
      No entanto, um conjunto de todas as possibilidades de combinação de 3
objetos definidos arbitrariamente é mal definido pois a quantidade de objetos
possíveis é indefinida, podendo-se alterar as combinações possíveis ao se incluir
qualquer novo objeto no conjunto de primitivas.
      Assim, no primeiro caso, as primitivas são passíveis de descrição exaustiva
através de seus tipos simbólicos. Consequentemente o conjunto é bem definido e
fechado. No segundo caso, o espaço das próprias primitivas não está bem definido e,
portanto, o conjunto de possibilidades é mal definido e aberto. Os organismos
biológicos ou artefatos tecnológicos, como as redes neurais, enquadram-se na
categoria dos conjuntos abertos posto que são capazes de desenvolver novas
primitivas baseados na sua interação com o mundo e com eles mesmos.
      Cariani (2009) define basicamente dois modos da arte produzir emergência
criativa. O primeiro modo de emergência criativa diz respeito à possibilidade da obra
provocar novas ideias, significados e perspectivas em seu público. Já o segundo
modo, mais interessante para esta pesquisa, diz respeito à arte que cria objetos
autônomos e que por si sós, independentemente, desenvolvem novas primitivas. Para
Cariani (2009), essa emergência criativa pode ou não estar explicitamente tensionada
ou declarada pelo artista.
      A instalação Bacterial Orchestra (2006), de Martin Lübke e Olle Cornéer é
um bom exemplo desta emergência criativa não declarada pelos artistas, expressa
através de objetos/artefatos autônomos. Esta consiste em uma orquestra formada por
várias células capazes de ouvir e reproduzir os sons do ambiente. O material sonoro é
oriundo do som ambiente onde as células estão inseridas, como pessoas conversando,
som dos passos ou os sons que outras células reproduzem (CORNÉER e LÜBCKE,
2006). Assim, em conjunto, comportam-se como um organismo mais complexo
trabalhando sobre um domínio mal definido e, portanto, aberto.
      Cada unidade desta ecologia é um sistema simples dotado de um microfone e
uma caixa de som. A célula é inicializada randomicamente com um conjunto de
parâmetros codificados em seu cromossomo que determinará como ela irá responder
aos estímulos sonoros. O gatilho que ativa as células é um pico na amplitude do som:
no primeiro pico a célula começa a registrar o som e no segundo pico ela para e passa
a reproduzir o que acabou de gravar. Simultaneamente cada célula é capaz de
“lembrar” as frases melódicas que registrou anteriormente e utiliza esta informação ao
reproduzir as novas frases registradas.
      Quando uma célula é instanciada com um DNA que a impede de ser ativada
ela é eliminada por inanição. Por outro lado, se ela recebe muito estímulo estará
fadada à perecer por superestimulação. Apenas as células que respondem bem ao
meio ambiente são capazes de continuar vivendo.
      Da interação simples entre as células resulta uma espécie de microfonia que
possibilita novas evoluções sonoras. Estas evoluções são capazes de gerar padrões
sonoros que não foram previstos, complexificando o espaço sônico da instalação ao
passar do tempo à medida em que os sons reverberavam através de cada uma das
células em uma espécie de retroalimentação, dificultando a definição de um escopo de
possibilidades apresentadas pela instalação, figura 31.


Figura 31: The Bacterial Orchestra no encontro de novas mídias em Norrkoping (2006).

      O conjunto de células que compõem a instalação caracterizam o que Lübcke e
Cornéer chamam de “organismo musical evolutivo auto-organizado” e podem ser
comparadas às redes neurais. A topologia desta rede auto-organizada consideraria
cada célula como um neurônio capaz de receber e emitir estímulos. Estes estímulos
são sonoros e através deles a rede se autorregulará assim como qualquer outro
organismo vivo.
      Essa manifestação da emergência em organismos vivos instiga a pesquisa no
sentido de que a corporificação dos agentes, seja de forma física ou virtual, pode ser
um dos primeiros passos para que esta transição determinista-emergente, em agentes,
seja possível. Esta motivação encontra-se presente também na obra The Flock, do
artista americano Kenneth Rinaldo em parceria com Mark Grossman (1994), figura
32.
      The Flock é um grupo de esculturas cibernéticas sonoras que exibem um
comportamento análogo ao de bando, encontrado em grupos naturais como os
pássaros, peixes e morcegos. Este comportamento de bando assemelha-se ao
demonstrado na obra EvoTwitter, detalhada mais adiante na seção 3.1. O
comportamento de bando demonstra características de supra-organização, quando
uma série de indivíduos independentes entre si agem como um sistema complexo
unificado e vai de encontro ao primeiro tipo de emergência destacado por Pfeifer e
Bongard (2007): um fenômeno global surgindo de um comportamento coletivo.
      Para Rinaldo (apud. WILSON, 2002), o conceito chave por trás desta
obra/instalação é a emergência do comportamento de vários sistemas independentes
que não possuem um controle centralizado para guiar o comportamento. O
comportamento global evolui naturalmente das interações locais entre os sistemas.
Quando as necessidades básicas do sistema são satisfeitas, como a autopreservação,
outras funções mais abstratas tendem a emergir, como o comportamento de bando. Os
resultados são caóticos, não-lineares e normalmente não se repetem.

Figura 32: Três braços robóticos da obra The Flock que demonstram comportamento emergente
de bando (Rinaldo, 1994).

      As concepções de emergência oferecem para o campo da arte da tecnologia
uma heurística para a criatividade. Se a emergência pode ser definida como novidade
pura, então compreender os processos que levam a acontecimentos, estruturas,
funções e perspectivas emergentes pode ser pertinente para a construção de artefatos
que percebam ou utilizem estes processos para criar novidades puras. Neste sentido
torna-se possível o desenvolvimento e aplicação de algoritmos baseados em processos
emergentes naturais para expandir a criatividade humana ou construção de sistemas
artificiais autônomos ou semiautônomos capazes de ser criativos por si sós.
2.5.	Criatividade, Autopoiesis e Emergência em Sistemas
      Quando se aborda a questão da emergência enquanto heurística para
criatividade (BARRETTO, 2012), pode parecer um caminho direto e natural para
qualquer tipo de emergência atingida. No entanto, dados os vários níveis de
emergência aqui apresentados, podemos perceber claramente que nem todo resultado
emergente é necessariamente criativo. No entanto, é possível modelar um agente com
esse objetivo utilizando parte do framework teórico apresentado nesta seção.
      Ainda na primeira seção, quando discutimos as técnicas e abordagens para
desenvolvimento de agentes inteligentes, pudemos perceber a grande variedade de
modelos de agentes que poderiam ser implementados. Neste sentido, torna-se
evidente que o papel do designer de agentes tem uma influência direta no resultado
exibido pelo sistema. Portanto, é interessante que o designer de agentes tenha à
disposição uma série de informações para que possa minimizar sua influência.
      Neste sentido são descritos, no inicio desta seção, alguns princípios de design
de agentes oriundos do campo de pesquisa da IA cognitiva e corporificada que
corroboram a ideia de que quanto menos o agente depender do seu designer, maior
será o grau de emergência. Esta relação também está bem descrita nas abordagens
sobre a criatividade oriundas da psicologia. Além disso, vale à pena ressaltar que a
pesquisa realizada sobre a criatividade nos fornece poderosos frameworks para
compreender, categorizar e classificar os diferentes níveis e manifestações da
criatividade.
      Do ponto de vista da psicologia, deve-se considerar o histórico das pesquisas
em criatividade para que se possa compreender dois pontos importantes: (a) ainda há
muito à se pesquisar dada a recente origem da área, (b) há muitas abordagens e
notações possíveis dentre as quais destacamos algumas cuja descrição se familiariza
mais com o objeto desta tese e que mantém algum tipo de correlação entre si (e com
as técnicas apresentadas na seção 1).
      As abordagens para criatividade (listadas no item 2.1) consideram que uma
relação temporal é significante na construção/evolução criativa. Para estas
abordagens, essa importância se aplica na evolução criativa ao longo do tempo,
através de sucessivas interações entre o individuo e o meio. A abordagem baseada em
processos e componentes, por exemplo, considera que através da repetição de uma
série de processos aplicados recursivamente, durante um certo período de tempo, é
possível refinar cada vez mais um determinado resultado, tornando-o cada vez mais
adaptado.
      Da mesma maneira, a abordagem evolutiva considera que ao passar do tempo,
um determinado individuo é capaz de expandir o seu potencial criativo através,
principalmente, do exercício do processo de criação e da aprendizagem oriunda deste
processo (SIMONTON, 1997). Por fim, a abordagem sistêmica também considera,
além da relação temporal, a criatividade como sendo um resultado emergente de um
sistema complexo que contém vários subcomponentes (como agentes, por exemplo)
interagindo entre si.
      Estas abordagens da criatividade são de suma importância, ainda que em
alguns pontos sejam contrastantes entre si, pois fornecem uma base sobre a qual
podemos analisar a criatividade computacional, componente dos sistemas
Artelligentes. Tanto as pesquisas do campo da IA quanto da psicologia, no que tange
a questão da criatividade, apontam para um comportamento eminentemente
autopoiético e emergente. Estes dois conceitos também são abordados no que
concerne o comportamento criativo emergindo tanto do resultado de mecanismos
cognitivos internos quanto da interação social entre indivíduos.
      Estas abordagens da criatividade são de suma importância, ainda que em
alguns pontos sejam contrastantes entre si, pois fornecem uma base sobre a qual
podemos analisar a criatividade computacional, componente dos sistemas
Artelligentes. Tanto as pesquisas do campo da IA quanto da psicologia, no que tange
a questão da criatividade, apontam para um comportamento eminentemente
autopoiético e emergente. Estes dois conceitos também são abordados no que
concerne o comportamento criativo emergindo tanto do resultado de mecanismos
cognitivos internos quanto da interação social entre indivíduos.


SEÇÃO III | Zer0: um sistema Artelligent
      Além dos outros trabalhos apresentados anteriormente, do autor ou de outros
artistas/cientistas, foi desenvolvido de forma concomitante à abordagem teórica um
gameart que demonstra todas as características elencadas aqui para que um sistema
seja considerado Artelligent.
      Inicialmente serão abordadas as questões poéticas que norteiam o
desenvolvimento do gameart (item 3.1). Além disso, discutiremos o papel do designer
de agentes no que tange a escolha da técnica de IA utilizada, o ambiente, a
representação de conhecimento e construção cognitiva dos agentes utilizando os
conceitos discutidos na primeira seção. Posteriormente, apresentamos também quais
implicações as escolhas do designer e suas implicações sob a ótica das abordagens
para a criatividade, considerando a autopoiese e a emergência, discutidas na segunda
seção.
3.1.	Do zero ao um
      É interessante perceber que as pessoas, normalmente, vivem em um ambiente
competitivo onde há sempre a necessidade de maximizar a sua performance na
realização das tarefas. Nos jogos “tradicionais” normalmente são confrontantes os
objetivos dos jogadores, desencadeando uma concorrência onde o mais forte
normalmente vence. Neste sentido, no fluxo de execução de um jogo comercial,
normalmente, os jogadores tendem à acumular artefatos que incrementem o seu
poder. Na contramão desta tendência, Zer0  propõe um universo colaborativo onde
pode-se estabelecer uma série de relações mutuamente benéficas.
      O jogo baseia-se na teoria flow desenvolvida por Mihaly Csikszentmihalyi em
1990. De acordo com Jen Chenova, as pessoas normalmente associam vários
sentimentos com o conceito de fun (diversão), como a sensação de atemporalidade,
alegria, foco e imediatismo (CHENOVA, 2006). Existe praticamente um consenso
universal que a ausência de equilíbrio entre o desafio de uma determinada atividade e
a habilidade necessária para superar o desafio descaracterizaria completamente a
sensação de divertimento, aniquilando a experiência de diversão (BARRETTO e
VENTURELLI, 2015).
      De acordo com a pesquisa de Csikszentmihalyi, a fenomenologia do flow tem
oito principais componentes:
1.	Um desafio que requer algum tipo de habilidade;
2.	A confluência entre ação e consciência;
3.	Objetivos claros;
4.	Feedback direto;
5.	Concentração na tarefa sendo desempenhada;
6.	A sensação de controle;
7.	A “perda” de autoconsciência;
8.	A transformação da experiência temporal.
      A fim de proporcionar a experiência descrita por Csikszentmihalyi, o jogo
convida o jogador à apreciar um passeio em um universo dominado por formas
geométricas, figura 33, onde o jogador também desempenha o papel de uma das
formas geométricas.

Figura 33: Interagente realizando uma incursão no universo do jogo Zer0 (do autor).
      Neste sistema multiagentes,  cada agente é visualmente representado por uma
forma geométrica capaz de emitir pulsos. Cada indivíduo possui um relógio interno
próprio que regula os seus próprios pulsos. Cada vez que um pulso intersecta outro,
um evento sonoro é gerado, gerando a trilha sonora do jogo.
      Outro ponto que constitui a poética do jogo, aborda de forma sutil e lúdica a
questão da discretização da informação através da representação binária. Neste tipo de
representação, qualquer informação é representada através de uma cadeia de bits
composta por 0s e 1s. O processo de transição discreta se contrapõe à representação
contínua (figura 34), onde há um infinito numero de estados possíveis entre os valores
0 e 1.

Figura 34: Transição discreta e contínua: (a) sinal degrau unitário, (b) sinal continuo unitário
(do autor).
      O percurso natural, portanto, é a transição entre 0 e 1, independentemente do
sistema de representação continua ou discreta, tendo em ambos os casos o mesmo
ponto de partida e chegada. No entanto, o percurso difere enormemente em ambas as
transições. Na discreta temos uma transição direta, imediata e íngreme, que causa uma
ruptura no estado atual. Por outro lado, na transição contínua, temos um conjunto
infinito de estados intermediários que podem ser ocupados.
      A fim de inverter esta lógica de forma lúdica, o agente, controlado pelo
usuário, também representado por uma forma geométrica, busca evoluir e ganhar mais
lados através da interação com os outros agentes. Sua forma inicial é a de uma reta,
forma geométrica com o menor numero de lados possível neste contexto, visualmente
representando o numero 1 (figura 35).

Figura 35: Evolução das formas geométricas através do acumulo de lados.
      À medida em que o agente evolui e ganha lados, passa a ser representado
progressivamente por forma geométricas mais complexas como triângulo, quadrado,
pentágono, assim por diante. O agente desenvolve, portanto, uma série de transições
discretas entre formas geométricas a partir da linha e a cada transição aproxima-se
cada vez mais de tornar-se um círculo.
      Paradoxalmente, ainda que ganhe lados e essa transição entre formas seja
discreta o agente ocupa infinitos estados intermediários na jornada inversa entre o 1 e
0. Neste sentido, o agente jamais tornar-se-á um 0, posto que como acrescenta lados à
sua figura geométrica, poderá ter infinitos lados e visualmente se assemelhará à um
circulo porém sem jamais sê-lo. Assim, a transição mista discreta/contínua que o
agente desenvolve durante a sua interação no jogo não pode ser completa.
      De um ponto de vista pragmático, a figura geométrica é desenhada por um
método que recebe como parâmetro a quantidade de lados que um determinado
individuo possui. Esta quantidade de lados pode ser interpretada como a “resolução”
de um circulo posto que quanto mais lados houver maior será a semelhança com um
circulo perfeito. No entanto, a partir deste método, independente da quantidade de
lados teremos sempre um polígono regular e nunca um círculo.
      Neste sentido, contrapondo-se também à questão da concorrência entre
jogadores e maximização de recursos nos jogos “tradicionais”, temos aqui o caminho
exatamente inverso. A quantidade de lados aproxima o jogador do zero e, portanto, da
própria anulação. Neste contexto onde o acumulo de recursos, neste caso o número de
lados, produz um efeito inverso ao esperado busca-se questionar a natureza desta
expectativa de maximização
3.2.	Ecologia de um Sistema Artelligent
      Ao se desenvolver um sistema Artelligent, devemos inicialmente considerar
que utilizaremos alguma (ou uma hibridização) das técnicas listadas na primeira
seção. Assim, sugerimos que abstraindo a implementação seja delimitada
primeiramente a questão poética ou o contexto onde o sistema estará inserido. É
importante ter em mente os possíveis pontos de partida, interações e objetivos que
podem ser alcançados a fim de possibilitar que seja escolhida uma técnica adequada.
3.2.1.	Descrevendo o Ambiente de Tarefa
      Antes de iniciar a modelagem do agente, propriamente dita, devemos analisar,
descrever e especificar o ambiente, tabela 6. Para que possamos compreender melhor
o ambiente onde o agente estará inserido, podemos descrevê-lo utilizando as
dimensões apontadas no item 1.3.2.
      No caso do Zer0, podemos dizer que sob o ponto de vista do agente, o
ambiente é parcialmente observável uma vez que o agente é capaz de perceber o
mundo apenas dentro de um espaço restrito, limitado ao raio dos seus pulsos. Apesar
disso, o ambiente pode mostrar-se algumas vezes completamente inobservável se
dentro da área de influência do agente não houver outros agentes, por exemplo. Neste
caso específico, o agente pode decidir ficar parado ou mover-se para uma direção
aleatória que não necessariamente maximizará a sua probabilidade de interação com
outros agentes.
      Uma vez que teremos vários agentes dentro de um mesmo ambiente, capazes
de interagir entre si, podemos ainda classificar este contexto como multiagentes. A
interação entre os agentes não é de natureza competitiva, mas de natureza
colaborativa. Uma vez que há uma interação entre dois agentes, ambos beneficiam-se
do resultado da ação ao estender as suas expectativas de vida.
      As mudanças que acontecem neste contexto não são previsíveis para os
agentes, não sendo possível para eles deduzir o próximo estado do ambiente ou
determinar precisamente quais serão as consequências das suas ações. Se
considerarmos que a mudança de estado do ambiente depende das ações tomadas
pelos agentes que estão no ambiente e que estas ações são fruto de um mecanismo
interno de inferência, não observável externamente, podemos deduzir que para o
agente o ambiente apresenta-se incerto ou estocástico, dada a natureza parcialmente
observável da ação dos demais agentes.
      Além disso, a experiência do agente dentro deste ambiente possui uma
natureza episódica uma vez que não é representada internamente no agente uma
narrativa temporal que correlacione um histórico de percepções e ações realizadas
pelo agente. Esta representação episódica, neste caso, dispensa o agente de possuir
algum tipo de memória de estados, que implicaria em maior necessidade de recursos,
reduzindo a complexidade computacional da modelagem do agente.
      No entanto, apesar de não haver necessidade de registrar um histórico de ações
realizadas pelo agente, cada individuo deve utilizar algum tipo de mecanismo de
inferência para avaliar as informações obtidas através de seus sensores. Durante este
espaço de tempo em que o agente desenvolve um raciocínio a fim de definir uma ação
adequada, o ambiente continua a se modificar. A principal implicação disto é que, por
exemplo, enquanto um agente decide se deve tentar comunicar-se e executa esta ação
o outro agente com o qual este se comunicaria já pode ter se distanciado, frustrando a
comunicação. É importante perceber que neste tipo de ambiente dinâmico pode-se
notar uma necessidade de definir uma forma de representação temporal: contínua ou
discreta. No caso dos eventos deste ambiente, foi definido que estes acontecem de
forma concorrente ao invés de realizadas em turnos, assemelhando-se à uma
representação temporal contínua (porém naturalmente discretizada dada a sua
representação computacional).

Dimensões
Ambiente de Tarefa – Zer0
Capacidade de Observação
Parcialmente Observável
Previsibilidade
Estocástico
Experiência do Agente
Episódico
Mudanças no Ambiente
Dinâmico
Representação Temporal
Contínuo
Agentes
Multiagentes
Tabela 6: Descrição resumida do ambiente de tarefa do Zer0.
3.2.2.	Modelando os Agentes
      Considerando o arcabouço poético descrito no item 3.1 e o ambiente descrito
no item 3.2.1 como ponto de partida, podemos identificar a necessidade de realizar o
design e implementação de pelo menos um agente. Através de um mecanismo de
abstração, exercitamos a capacidade de extrairmos as características relevantes de um
determinado contexto enquanto ignoramos características menos importantes ou
acidentais. A capacidade de abstrair nos permite pensar de forma macro em como o
sistema deve comportar-se, além de nos permitir identificar claramente os pontos
relevantes que serão considerados ao desenharmos o agente em seu nível de
implementação.
      Durante o processo de abstração, baseado na descrição macro do sistema,
podemos extrair algumas características que deverão ser incluídas pelos agentes,
como possuir uma determinada quantidade de lados, uma cor específica e uma
descrição dos seus pulsos internos. Dentre as características elencadas, devemos
selecionar algumas que serão representadas internamente, como fazendo parte da
construção cognitiva do atente, enquanto outras serão exibidas externamente como
compondo o fenótipo do agente.
      Uma boa metodologia para representar estas informações, a fim de facilitar a
identificação das características e sua respectiva modelagem, é a representação
através do PEAS (discutida no item 1.3.2). Para o Zer0, existem basicamente dois
tipos semelhantes de agentes: controlado pelo usuário e autônomo. O segundo é
descrito a seguir, enquanto o primeiro é apenas uma versão levemente modificada da
versão autônoma a fim de permitir que o movimento da forma seja controlado pelo
interagente. Os agentes autônomos tem suas principais características descritas
através do seguinte PEAS (Performance, Environment, Actuators, Sensors):
•	Performance:
o	Aumentar a expectativa de vida (interagir);
o	Mover-se;
•	Environment:
o	Espaço 2D infinito;
o	Multiagente.
o
•	Actuators:
o	Pulsar;
o	Mover-se;
o	Permanecer na mesma posição;
•	Sensors:
o	Posição;
o	Outros agentes;
o	Expectativa de vida (idade atual);
      Cada agente tem codificado internamente uma expectativa de vida que é
inicializada aleatoriamente. Uma vez que esta expectativa decai, os indivíduos tentam
maximizá-la através da interação com outras formas geométricas. Cada vez que dois
pulsos colidem, ambos os agentes tem sua expectativa de vida incrementada e ganham
uma certa quantidade de pontos. Quanto mais pontos um agente tem, mais lados ele
possui visualmente.
      O agente controlado pelo interagente é iniciado com um lado único (uma
linha), então desenvolve-se lado a lado: triangulo, quadrado, pentágono e assim por
diante. Conforme podemos ver na figura 36, a percepção do agente se baseia no
componente de percepção.

Figura 36: Arquitetura interna do agente autônomo (BARRETTO e VENTURELLI, 2015).
      Este componente é o responsável por informar ao agente como está o mundo
em um determinado momento, considerando a sua esfera de influência, tornando
visíveis para o agente as informações acerca da sua posição atual e o tempo de vida (a
expectativa está internamente codificada, portanto o agente não tem como prever
quando irá perecer). Além disso, o agente é capaz de perceber se há outros agentes
dentro daquele determinado espaço, delimitado pelo alcance dos seus pulsos, figura
37.

Figura 37: Representação do ambiente onde o agente está inserido, com agentes tentando se
comunicar (do autor).
      Baseado neste conjunto de informações, o agente atualiza a sua representação
interna do mundo (conhecimento), incluindo a existência ou não de outros agentes
próximos, sua posição atual e sua expectativa de vida. Após atualizar a sua
representação interna, a máquina de inferência desenvolve um raciocínio a fim de
determinar quais ações podem ser mais adequadas.
      A máquina de inferências neste caso é representada através de uma arquitetura
de regras de condição-ação. Por exemplo, se não é o momento de gerar um pulso (de
acordo com o relógio interno do agente) e não há agentes dentro da esfera de
influência o agente pode decidir por mover-se.
3.2.3.	Interação Multiagentes
      Conforme dito anteriormente, os agentes são capazes de estabelecer
comunicação através dos pulsos que emitem e cada um destes pulsos é um sinal. Cada
vez que os pulsos interagem, estendem a sua expectativa de vida. Além disso, cada
uma das interações gera um evento sonoro, assim, compondo em tempo real a trilha
sonora do jogo. Estas possíveis interações estão brevemente representadas na figura
38, de acordo com a metodologia TROPOS, utilizada para o desenvolvimento de
software baseado em agentes (CASTRO, KOLP e MYLOPOULOS, 2000).

Figura 38: Interações entre dois agentes autônomos e um agente humano, descritos de acordo
com a metodologia TROPOS.
      O ambiente no qual estão inseridos os agentes pode ser descrito como
parcialmente observável, pois cada agente tem apenas uma visão restrita e limitada do
ambiente. Considerando que o estado seguinte do ambiente depende de outros fatores
além das próprias ações de cada agente, podemos dizer que o ambiente é estocástico.
Além disso, enquanto o agente raciocina o ambiente continua em constante evolução
não havendo divisão temporal em intervalos. Estas ultimas duas características
impõem algumas limitações de tempo forçando o agente à responder rapidamente.
      É importante ressaltar que a modificação e atualização do estado interno do
agente não pode ser feita através de imposição externa, mas apenas através dos seus
próprios mecanismos internos. O sistema de mecanismos internos ao passo em que
define os limites entre o agente e o ambiente também é o responsável por manter e
atualizar a si mesmo. Assim, define-se o agente autopoiéticamente dotando-o da
capacidade e responsabilidade de manter a si próprio.
      Esta abordagem vai de encontro aos princípios de design de agentes
destacados no item 2.1 onde foi discutida a necessidade de minimizar o impacto do
designer sobre a representação de mundo do agente e a sua capacidade de tomada de
decisão. Assim, os agentes tornam-se mais facilmente adaptáveis à mudanças o
ambiente, reagindo e adaptando suas ações à medida em que co-evoluem com o
ambiente.
      Neste sentido, a co-evolução dinâmica entre agente e ambiente, discutida no
item 2.5, torna-se essencial tanto para a construção cognitiva dos agentes quanto para
a definição da configuração do ambiente no que tange a forma como ele interfere nas
ações do agente. Assim, a disposição dos agentes e a respectiva comunicação entre
eles é emergente como um resultado da sua representação autopoiética interna e
também das propriedades dinâmicas do ambiente.
3.3.	Criatividade e Emergência(s)
      Dentre os requisitos listados para definir um sistema Artelligent, mostram-se
dentre os mais relevantes a necessidade de considerar pelo menos uma abordagem
para criatividade além da capacidade de exibir um comportamento emergente.
Considerar uma das abordagens para criatividade listadas anteriormente não restringe
necessariamente a utilização de uma outra abordagem posto que, conforme discutido
no item 2.1, a área de pesquisa em criatividade está constantemente evoluindo. A
necessidade de pensar e buscar uma forma de correlacionar ao menos uma abordagem
sobre criatividade com a modelagem de agentes tem como principal motivo fazer com
que o processo de design de agentes considere premissas ligadas à estas abordagens.
      Se consideramos do ponto de vista computacional as premissas oriundas das
pesquisas em criatividade humana, podemos aplicar os frameworks que também
descendem destas abordagens como a categorização dos resultados criativos
utilizando a escala C e a decomposição do processo criativo em componentes
utilizando e metodologia dos Ps. Neste sentido, a “escolha” de uma abordagem leva à
utilização da categorização C e decomposição em Ps, ajudando o designer de agentes
à considerar também estes aspectos, tão relevantes quanto a descrição correta do
agente e do ambiente, por exemplo.
	No caso do Zer0, todo o sistema pode ser visto sob a ótica da abordagem
sistêmica da criatividade, muito embora exiba algumas características descritas na
abordagem evolutiva. Do ponto de vista evolutivo, o resultado criativo é modificado à
medida em que o tempo passa gerando estados diferentes de forma ininterrupta
através da constante evolução da interação entre os indivíduos. A visão sistêmica
pode ser adotada pois, neste caso, a expressão criativa é caracterizada como sendo um
resultado emergente de um sistema complexo que contém vários subcomponentes
interagindo uns com os outros. Cada um destes subcomponentes é considerado a fim
de propor uma compreensão mais rica e significativa dos processos e resultados
criativos.
      Nesta abordagem, o foco está principalmente no Place (Ambiente) e em um
menor grau nos outros Ps: Person (Pessoa), Product (Produto) e Process (Processo).
O ambiente desempenha um papel fundamental posto que é o meio de interação entra
os diversos agentes, influenciando também diretamente na decisão individual de cada
agente mas, sobretudo, no contexto geral de interações. O ambiente e o produto se
intersectam uma vez que a composição musical que emerge da interação entre os
agentes é uma representação sonora do ambiente e, ao mesmo tempo, a expressão
criativa do sistema. A relativamente simples construção cognitiva do agente faz com
que os processos que operam também sejam simples.
      No entanto, devemos nos questionar sobre o nível ideal de complexidade que
propomos no sistema. À primeira vista podemos supor que quanto maior for a
complexidade da construção cognitiva do agente maior será a probabilidade de
encontrarmos algum comportamento emergente. No entanto, existe uma linha tênue
que divide o comportamento emergente do aleatório conforme apontado em estudos
anteriores (SILVA, 2013) (GALANTER, 2003). À medida em que partimos de um
sistema simples, completamente determinista, incapaz de exibir algum
comportamento emergente e adicionamos alguma “desordem” à ele, aumentamos a
sua capacidade de produzir tal comportamento.
      Conforme a definição de emergência adotada aqui, como sendo o surgimento
de algo que não poderia ser previsto com base no que o antecedeu, podemos dizer que
o comportamento emergente demanda necessariamente algum nível de
imprevisibilidade. Isto significa que ao incrementarmos o nível de complexidade
adicionamos também alguma instância de aleatoriedade. No entanto, em determinado
ponto o nível de complexidade passa a ser traduzido progressivamente em desordem
ou pura aleatoriedade. A principal diferença, neste caso, é que muito embora o
comportamento emergente não pudesse ser previsto ele pode ser explicado, enquanto
na aleatoriedade o desenvolvimento desse raciocínio explicativo posterior ao evento
torna-se praticamente impossível.

Figura 39: Relação entre complexidade e ordem x desordem (GALANTER, 2003).
      A fim de gerenciar o nível de complexidade (figura 39), mantendo-o abaixo do
nível de aleatoriedade, foi proposto para este sistema um modelo de agente
relativamente pouco complexo. Os resultados emergentes listados à seguir, oriundos
da interação entre os agentes, representados na forma da trilha sonora e composição
visual do gameart podem ser considerados como mini-C (KAUFMAN e
BEGHETTO, 2009) podendo ser estendido até pequeno-C (KOZBELT, BEGHETTO
e RUNCO, 2010).
      3.3.1.	Cor e Opacidade
      O modelo de agente desenvolvido utiliza-se do principio de fenótipo oriundo
dos algoritmos genéticos para exibir visual e sonoramente uma expressão da sua
configuração interna. Tal qual nos algoritmos genéticos, os agentes do jogo Zer0 tem
codificados uma estrutura baseada no genótipo (DNA) dos algoritmos genéticos que
contém informações relevantes tais como a sua cor e o seu ritmo (clock) interno de
pulsos.
      Muito embora não haja uma função de avaliação (fitness) explicita, os
indivíduos que não interagem com outros agentes por decisão deliberada ou por
encontrarem-se isolados em alguma parte do universo 2D, gradativamente são
eliminados. Um operador de recombinação é aplicado utilizando uma distribuição
normal de probabilidade, que teoricamente provê uma equânime escolha entre os
indivíduos que irão reproduzir-se. Portanto, partindo de uma população aleatória, a
ecologia tende a favorecer naturalmente os indivíduos que (a) possuem uma maior
tendência de comunicação ou (b) possuem um clock mais acelerado.
      Apesar da cor estar, ainda que fracamente, relacionada dentro da
representação do genótipo com os atributos listados acima, esperava-se que houvesse
um agrupamento de indivíduos com cores semelhantes. No entanto, o que se verificou
foi que a variedade de cores se manteve equilibrada. Esta característica emergente do
sistema de manutenção do equilíbrio entre as diferentes cores suscita que o próprio
sistema, enquanto composto por outros sistemas, pode funcionar de forma
autopoiética ao manter internamente as estruturas que o definem enquanto sistema.
      Por fim, a opacidade está relacionada à expectativa de vida e ao tempo de vida
restante de um individuo. Quanto mais próximo do fim do seu tempo de vida, mais
translucido fica o individuo. É interessante perceber que, de fato, os indivíduos
tendem a tentar comunicar-se mais à medida em que vão perdendo sua opacidade. Isto
demonstra que o mecanismo interno de inferência adaptou-se de forma autopoiética e
emergente ao avaliar uma nova condição do individuo.
      3.3.2	Movimentação, Pulsos e Composição Sonora
      O caráter comunicativo dos agentes tem papel fundamental na construção
poética do Zer0. A comunicação entre os agentes é a maneira disponível à eles de
estender a sua expectativa de vida. No entanto, esta capacidade de comunicação
depende necessariamente de uma propriedade interna dos indivíduos: emitir pulsos.
      Esta representação interna é feita através de uma matriz unidimensional com 8
posições que contém o intervalo em milissegundos entre um pulso emitido e o
próximo. O exemplo extraído de um agente do sistema (tabela 7), ilustra que entre o
pulso 0 e o pulso 1 deve-se aguardar 50 milissegundos e entre o pulso 7 e o pulso 0
deve-se esperar 121 milissegundos, por exemplo. Assim, pode-se verificar que os
indivíduos que possuem os menores valores armazenados internamente tendem a
comunicar-se de forma mais ágil.

      Enquanto um próximo pulso não pode ser emitido, os agentes podem
movimentar-se ou permanecer onde estão. A decisão depende da análise do agente
sobre a possibilidade de maximizar a sua expectativa de vida. Para tal, um peso foi
associado à cada uma das possibilidades e é adaptado cada vez que o agente toma
uma decisão e ele atinge ou não o seu macro-objetivo. O mecanismo de inferência
considera também a quantidade de agentes visíveis dentro da sua esfera de influência
ao tomar uma decisão.
      A visão do agente da sua área de influência se dá através da intersecção dos
seus pulsos com outros emitidos por agentes vizinhos. Ao acontecer a interseção entre
os pulsos emitidos, cada agente emite o seu evento sonoro. Logo, a composição
sonora depende de ambos aspectos aqui mencionados: movimentação e pulsação dos
agentes. Assim, uma vez que o movimento dos agentes influencia na posição relativa
entre eles e, por sua vez, a distância entre os agentes influencia na comunicação entre
eles podemos concluir que a composição musical resulta de um processo de co-
emergência dinâmica entre agentes e meio-ambiente.
3.4.	Zer0, Artelligent
      Ao descrever o processo de implementação do gameart Zer0, foram
destacados ao longo dos tópicos como as premissas e requisitos elencados para definir
um sistema Artelligent foram atendidos:
a.	Utilizar um agente inteligente ou um conjunto/sistema de agentes
inteligentes e seu ambiente de tarefa (itens 3.2.1 e 3.2.2);
b.	Utilizar alguma técnica de IA, para descrever e implementar o item
acima, que facilite o surgimento de um comportamento emergente
(item 3.2.2 e 3.2.3);
c.	Represente o conhecimento de tal forma que ele seja extensível ou
emergente (item 3.2.2);
d.	Utilize os princípios para design de agentes, minimizando o papel do
designer na construção do conhecimento agente ao passo em que se
maximiza o papel da aprendizagem e/ou adaptação (item 3.2.3);
e.	Considere pelo menos dois Ps: Person (Pessoa), Process (Processo),
Product (Produto) e Press/Place (Ambiente/Local) (item 3.3);
f.	Seja capaz de gerar Produto(s) considerados no mínimo mini-C (item
3.3);
g.	Considere pelo menos uma das abordagens para criatividade:
desenvolvimentista, processual, evolutiva ou sistêmica (3.3);
h.	Demonstre um comportamento autopoiético no que diz respeito ao
gerenciamento das suas estruturas internas de conhecimento e/ou
cognição (3.2.3);
i.	Seja capaz de exibir comportamento emergente (combinatória ou
epistêmica) ou demonstre algum tipo de co-emergência dinâmica com
o ambiente (item 3.3).
      Considerando a descrição proposta para o conceito de Artelligent nesta tese,
pode-se afirmar que o sistema implementado apresenta as características necessárias.
Muito embora fossem possíveis diferentes implementações e variações da modelagem
aqui proposta para o sistema, a escolha da implementação utilizando algoritmos
relativamente menos complexos tende a facilitar a compreensão de como podem ser
aplicados os princípios levantados.





CONCLUSÃO
      Em um primeiro momento, se faz necessário reafirmar a que um sistema
Artelligent pode ser definido como um sistema autopoiético que através da utilização
de técnicas especificas de inteligência artificial, representa o conhecimento de forma
extensível e, considerando os princípios que regem o processo criativo humano,  é
capaz de exibir resultados de caráter reconhecidamente emergentes em um
determinado ambiente.
      Para um sistema Artelligent, é notável a importância do campo da IA no
sentido de que sem as técnicas oriundas deste campo, provavelmente, seria
extremamente difícil propor um sistema capaz de representar conhecimento, aprender
e evoluir. Em um sistema Artelligent a representação de conhecimento desempenha
um papel fundamental pois, sobre ela desenrolam-se os processos cognitivos ou
evolutivos. Muito embora não se possa afirmar enquanto regra, podemos perceber que
notadamente um sistema cuja representação de conhecimento está mais aberta à
modificações e preparada para um processo adaptativo já denota, por si só, o
surgimento emergente de novo conhecimento.
      Fica muito clara, na definição de Artelligent, que o objetivo de um sistema
Artelligent é alcançar resultados emergentes. Para que isto seja possível, deve-se
utilizar em todos os aspectos possíveis os princípios e técnicas que favoreçam e
facilitem o surgimento deste comportamento. Neste sentido, o papel do designer deve
ser de criar uma plataforma sobre a qual o agente ou sistema deve ser capaz de
construir através de um processo iterativo o seu próprio conhecimento.
      Dentre as técnicas disponíveis atualmente, destacam-se as que transitam
próximas aos modelos biológicos como os algoritmos genéticos, as redes neurais
artificiais e os sistemas multiagentes. As três abordagens já exibem naturalmente um
comportamento emergente intrínseco. No caso do algoritmo genético, o próprio
percurso evolutivo é emergente no sentido de que pode-se determinar qual o objetivo
evolutivo através de uma função de adaptação mas não o processo evolutivo que se dá
entre sucessivas gerações de indivíduos, como se pode perceber no exemplo do
gerador de acordes.
      O comportamento emergente exibido pelos algoritmos genéticos é semelhante
ao observado nas redes neurais artificiais. Para as RNAs a própria representação do
conhecimento é emergente, uma vez que ela se faz através do ajuste dos pesos
sinápticos entre as conexões neurais. Por mais que inicializemos duas redes neurais
iguais, com os mesmos pesos e quantidade de neurônios, e as utilizemos sobre um
mesmo ambiente não podemos afirmar que o conhecimento será representado de
forma idêntica em ambas.
      Os sistemas multiagentes são capazes de exibir uma auto-organização
emergente e baseiam-se no fato de que cada agente é autônomo e pode definir as suas
próprias metas e objetivos a fim de determinar quais ações deve realizar. Neste tipo de
abordagem, podemos desenvolver agentes cuja capacidade cognitiva é baixa pois o
caráter emergente se encontra na interação entre os diversos indivíduos que compõem
uma determinada população ou sociedade. Este caráter social dos SMAs é emergente
e, notadamente, facilita o surgimento de resultados emergentes.
      No que diz respeito à representação de conhecimento, deve-se levar em conta
que a representação do conhecimento não é determinante para o surgimento de
resultados emergentes mas deve ser considerado seriamente dependendo da
abordagem. Ao tratar-se de uma abordagem multiagentes verifica-se que o foco passa
a ser a construção coletiva de um comportamento. Para que seja possível um
comportamento emergente oriundo de uma interação entre diversos agentes é
necessária que haja algum tipo de comunicação explicita ou implícita, mediada ou
direta. Neste sentido, podemos considerar que a comunicação e a sua forma fazem
parte da representação de conhecimento para os agentes.
      Ao pensar em um agente que fará parte de um sistema Artelligent, devemos
considerá-lo autopoiético no sentido de que ele deve ser autossuficiente e autônomo
para gerenciar seus próprios mecanismos internos, conforme descrito na seção 2.
Devemos considerar os princípios de agente e considerar não apenas representação
interna do agente mas também levar em consideração o ambiente e as relações que ele
estabelecerá com ele. Sob este ponto de vista, o designer tem um papel fundamental
ao escolher corretamente como representar o ambiente de maneira suficiente para que
o agente consiga desempenhar uma interação com o ambiente mas ao mesmo tempo
considerando que deve haver espaço suficiente para que a construção cognitiva ou a
auto-organização social dos agentes aconteça de forma emergente através de um
processo autopoiético.
      Nesta pesquisa, portanto, mostram-se evidências que podem ajudar a
esclarecer por que o conceito autopoiese pode ser bastante interessante para os artistas
e cientistas. Artistas computacionais, especialmente, pode encontrar neste conceito
vários desafios tecnológicos que possam inspirá-los a produzir arte. Teóricos da IA
pode achar fascinante e inspiradora a ontologia por trás do que foi apresentado. Os
artigos que tratam de interatividade, autonomia e criatividade podem ser enriquecidos
quando consideram todos os aspectos de autopoiese e emergência.
      Muito embora pareça claro que o conceito de emergência oferece para o
campo da arte e tecnologia uma heurística para a criatividade, se fazia necessária a
criação de um framework que abarcasse uma série de princípios que pudessem
facilitar o surgimento desta emergência com um grau de complexidade controlado. Se
a emergência pode ser definida como o surgimento de novidade pura, então a
compreensão dos processos que levam a esses eventos, estruturas, funções e
perspectivas emergentes podem ser relevantes para a construção de artefatos que
utilizam esses processos para criar novidade. Neste sentido, é possível conceber e
implementar algoritmos baseados em processos emergentes naturais a fim de expandir
a criatividade humana ou construir sistemas artificiais capazes de demonstrar
criatividade autônoma.
      Para concluir, seria interessante para listar algumas possíveis desafios para a
investigação futura. A compreensão teórica do comportamento inteligente e criativo
seria um deles uma vez que apesar de mais de meio século de pesquisa em AI, ele
ainda carece de uma compreensão profunda dos mecanismos que controlam, facilita
ou permite o comportamento inteligente.  Além disso, seria interessante também
desenvolver a compreensão nos termos da neurociência, que pode nos prover modelos
matemáticos mais precisos a fim de desenvolver outros algoritmos baseados no
comportamento biológico do cérebro humano. A implementação destes algoritmos
utilizando diferentes técnicas computacionais pode, por fim, ajudar a compreender o
próprio funcionamento do cérebro através da análise dos resultados obtidos.
      Como uma extensão do gerador de acordes com uma função de fitness fixa, foi
proposto que a função de fitness utilizada poderia ser variável e relacionada
esteticamente utilizando sinais de EEG (Eletroencefalograma) (BARRETTO, REGO
e VENTURELLI, 2013). Nesta abordagem, a ERAN (Early Right Anterior
Negativity) aparece como um sinal EEG possível de ser utilizado porém ainda pouco
estudado em processamento em tempo real. O ERAN é um marcador de quebra de
expectativa no estimulo auditivo e esta expectativa esta baseada no conhecimento
prévio de um determinado campo harmônico. Pode-se verificar a descrição de um
algoritmo para detectar o ERAN e analisá-lo no que diz respeito à sua utilização como
função de fitness para avaliar os indivíduos a fim de orientar a evolução do algoritmo
genético baseado na experiência musical e estética do individuo. Esta pesquisa está
em andamento em parceria com o Laboratório de Neurociência e Cognição da
Universidade Presbiteriana Mackenzie.
      Por fim, no Zer0, a representação visual e a trilha sonora do jogo emergem de
um ambiente complexo definido com regras simples. Os agentes evoluem junto com o
meio ambiente, criando uma espécie de auto-identidade, necessária para atingir um
nível autopoiético. Seria interessante evoluir esses agentes ao utilizar representações
de conhecimento mais complexas. Por exemplo, um modelo BDI poderia provocar
grandes mudanças na partitura musical.
